<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[redis详解、]]></title>
    <url>%2F2019%2F10%2F01%2Fredis%E8%AF%A6%E8%A7%A3%E3%80%81%2F</url>
    <content type="text"><![CDATA[要掌握的很好的，就是redis架构 安装一个虚拟机集群 安装redis集群 配置持久化 RDB 快照手动设置检查点（etc/redis/redis.conf 里面可以修改save） 高并发，高可用，海量数据，备份，随时可以恢复， redis架构，每秒钟几十万的访问量QPS，99.99%的高可用性，TB级的海量数据，备份和恢复，缓存架构就成功了一半。最最简单的模式，无非就是存取redis，存数据，取数据。解决各种各样的高并发下缓存面临的难题，缓存架构中不断引入各种解决方案和技术，解决高并发的问题。 搭建redis集群，从0开始，一步一步搭建一个4个结点的Centos集群。安装4台虚拟机 在hosts文件下,配置好所有的机器的ip地址到hostname的映射关系 使用ssh配置每台机器之间免密登录 make install 要把redis作为一个系统的daemon进程去运行，每次系统启动，redis进程一起启动 如果没有持久化的话，redis遇到灾难性故障的时候，就会丢失所有的数据 如果通过持久化将数据搞一份儿在磁盘上去，然后定期比如说同步和备份到一些云存储服务上去，那么就可以保证数据不丢失全部，还是可以恢复一部分数据回来的。redis如果单单把数据放在内存中，是没有任何方法应对一些灾难性的工作的，redis 在启动会自动从磁盘中恢复数据到内存中。 redis 持久化RDB,AOF利弊比较比如你redis整个挂了，然后redis就不可用了，你要做的事情是让redis变得可用，尽快变得可用 重启redis，尽快让它对外提供服务，但是就像上一讲说，如果你没做数据备份，这个时候redis启动了，也不可用啊，数据都没了 很可能说，大量的请求过来，缓存全部无法命中，在redis里根本找不到数据，这个时候就死定了，缓存雪崩问题，所有请求，没有在redis命中，就会去mysql数据库这种数据源头中去找，一下子mysql承接高并发，然后就挂了 mysql挂掉，你都没法去找数据恢复到redis里面去，redis的数据从哪儿来？从mysql来。。。 具体的完整的缓存雪崩的场景，还有企业级的解决方案，到后面讲 如果你把redis的持久化做好，备份和恢复方案做到企业级的程度，那么即使你的redis故障了，也可以通过备份数据，快速恢复，一旦恢复立即对外提供服务 redis的持久化，跟高可用，是有关系的，企业级redis架构中去讲解 redis持久化：RDB，AOF 1、RDB和AOF两种持久化机制的介绍 RDB持久化机制，对redis中的数据执行周期性的持久化 AOF机制对每条写入命令作为日志，以append-only的模式写入一个日志文件中，在redis重启的时候，可以通过回放AOF日志中的写入指令来重新构建整个数据集 如果我们想要redis仅仅作为纯内存的缓存来用，那么可以禁止RDB和AOF所有的持久化机制 通过RDB或AOF，都可以将redis内存中的数据给持久化到磁盘上面来，然后可以将这些数据备份到别的地方去，比如说阿里云，云服务 如果redis挂了，服务器上的内存和磁盘上的数据都丢了，可以从云服务上拷贝回来之前的数据，放到指定的目录中，然后重新启动redis，redis就会自动根据持久化数据文件中的数据，去恢复内存中的数据，继续对外提供服务 如果同时使用RDB和AOF两种持久化机制，那么在redis重启的时候，会使用AOF来重新构建数据，因为AOF中的数据更加完整 2、RDB持久化机制的优点 （1）RDB会生成多个数据文件，每个数据文件都代表了某一个时刻中redis的数据，这种多个数据文件的方式，非常适合做冷备，可以将这种完整的数据文件发送到一些远程的安全存储上去，比如说Amazon的S3云服务上去，在国内可以是阿里云的ODPS分布式存储上，以预定好的备份策略来定期备份redis中的数据 （2）RDB对redis对外提供的读写服务，影响非常小，可以让redis保持高性能，因为redis主进程只需要fork一个子进程，让子进程执行磁盘IO操作来进行RDB持久化即可 （3）相对于AOF持久化机制来说，直接基于RDB数据文件来重启和恢复redis进程，更加快速 3、RDB持久化机制的缺点 （1）如果想要在redis故障时，尽可能少的丢失数据，那么RDB没有AOF好。一般来说，RDB数据快照文件，都是每隔5分钟，或者更长时间生成一次，这个时候就得接受一旦redis进程宕机，那么会丢失最近5分钟的数据 （2）RDB每次在fork子进程来执行RDB快照数据文件生成的时候，如果数据文件特别大，可能会导致对客户端提供的服务暂停数毫秒，或者甚至数秒 4、AOF持久化机制的优点 （1）AOF可以更好的保护数据不丢失，一般AOF会每隔1秒，通过一个后台线程执行一次fsync操作，最多丢失1秒钟的数据 （2）AOF日志文件以append-only模式写入，所以没有任何磁盘寻址的开销，写入性能非常高，而且文件不容易破损，即使文件尾部破损，也很容易修复 （3）AOF日志文件即使过大的时候，出现后台重写操作，也不会影响客户端的读写。因为在rewrite log的时候，会对其中的指导进行压缩，创建出一份需要恢复数据的最小日志出来。再创建新日志文件的时候，老的日志文件还是照常写入。当新的merge后的日志文件ready的时候，再交换新老日志文件即可。 （4）AOF日志文件的命令通过非常可读的方式进行记录，这个特性非常适合做灾难性的误删除的紧急恢复。比如某人不小心用flushall命令清空了所有数据，只要这个时候后台rewrite还没有发生，那么就可以立即拷贝AOF文件，将最后一条flushall命令给删了，然后再将该AOF文件放回去，就可以通过恢复机制，自动恢复所有数据 5、AOF持久化机制的缺点 （1）对于同一份数据来说，AOF日志文件通常比RDB数据快照文件更大 （2）AOF开启后，支持的写QPS会比RDB支持的写QPS低，因为AOF一般会配置成每秒fsync一次日志文件，当然，每秒一次fsync，性能也还是很高的 （3）以前AOF发生过bug，就是通过AOF记录的日志，进行数据恢复的时候，没有恢复一模一样的数据出来。所以说，类似AOF这种较为复杂的基于命令日志/merge/回放的方式，比基于RDB每次持久化一份完整的数据快照文件的方式，更加脆弱一些，容易有bug。不过AOF就是为了避免rewrite过程导致的bug，因此每次rewrite并不是基于旧的指令日志进行merge的，而是基于当时内存中的数据进行指令的重新构建，这样健壮性会好很多。 6、RDB和AOF到底该如何选择 （1）不要仅仅使用RDB，因为那样会导致你丢失很多数据 （2）也不要仅仅使用AOF，因为那样有两个问题，第一，你通过AOF做冷备，没有RDB做冷备，来的恢复速度更快; 第二，RDB每次简单粗暴生成数据快照，更加健壮，可以避免AOF这种复杂的备份和恢复机制的bug （3）综合使用AOF和RDB两种持久化机制，用AOF来保证数据不丢失，作为数据恢复的第一选择; 用RDB来做不同程度的冷备，在AOF文件都丢失或损坏不可用的时候，还可以使用RDB来进行快速的数据恢复 RDB持久化详解1、如何配置RDB持久化机制2、RDB持久化机制的工作流程3、基于RDB持久化机制的数据恢复实验 1、如何配置RDB持久化机制 redis.conf文件，也就是/etc/redis/6379.conf，去配置持久化 save 60 1000 每隔60s，如果有超过1000个key发生了变更，那么就生成一个新的dump.rdb文件，就是当前redis内存中完整的数据快照，这个操作也被称之为snapshotting，快照 也可以手动调用save或者bgsave命令，同步或异步执行rdb快照生成 save可以设置多个，就是多个snapshotting检查点，每到一个检查点，就会去check一下，是否有指定的key数量发生了变更，如果有，就生成一个新的dump.rdb文件 2、RDB持久化机制的工作流程 （1）redis根据配置自己尝试去生成rdb快照文件（2）fork一个子进程出来（3）子进程尝试将数据dump到临时的rdb快照文件中（4）完成rdb快照文件的生成之后，就替换之前的旧的快照文件 dump.rdb，每次生成一个新的快照，都会覆盖之前的老快照 3、基于RDB持久化机制的数据恢复实验 （1）在redis中保存几条数据，立即停掉redis进程，然后重启redis，看看刚才插入的数据还在不在 数据还在，为什么？ 带出来一个知识点，通过redis-cli SHUTDOWN这种方式去停掉redis，其实是一种安全退出的模式，redis在退出的时候会将内存中的数据立即生成一份完整的rdb快照 /var/redis/6379/dump.rdb （2）在redis中再保存几条新的数据，用kill -9粗暴杀死redis进程，模拟redis故障异常退出，导致内存数据丢失的场景 这次就发现，redis进程异常被杀掉，数据没有进dump文件，几条最新的数据就丢失了 （2）手动设置一个save检查点，save 5 1（3）写入几条数据，等待5秒钟，会发现自动进行了一次dump rdb快照，在dump.rdb中发现了数据（4）异常停掉redis进程，再重新启动redis，看刚才插入的数据还在 rdb的手动配置检查点，以及rdb快照的生成，包括数据的丢失和恢复，全都演示过了 AOF持久化详解1、AOF持久化的配置2、AOF持久化的数据恢复实验3、AOF rewrite4、AOF破损文件的修复5、AOF和RDB同时工作 1、AOF持久化的配置 AOF持久化，默认是关闭的，默认是打开RDB持久化 appendonly yes，可以打开AOF持久化机制，在生产环境里面，一般来说AOF都是要打开的，除非你说随便丢个几分钟的数据也无所谓。如果开启AOF，就算没有AOF文件，redis在重启时，也会创建一个新的空的AOF文件恢复数据，在这个时候就需要先关闭AOF，拷贝dump.rdb恢复redis先，接着应该直接在命令行热修改redis配置，打开AOF。此时磁盘上的配置文件还是no（关闭）的，还需要在磁盘上将AOF打开。 AOF append-only ，顺序写入，如果AOF文件破损，那么用redis-check-aof fix修复文件打开AOF持久化机制之后，redis每次接收到一条写命令，就会写入日志文件中，当然是先写入os cache的，然后每隔一定时间再fsync一下 而且即使AOF和RDB都开启了，redis重启的时候，也是优先通过AOF进行数据恢复的，因为aof数据比较完整 可以配置AOF的fsync策略，有三种策略可以选择，一种是每次写入一条数据就执行一次fsync; 一种是每隔一秒执行一次fsync; 一种是不主动执行fsync always: 每次写入一条数据，立即将这个数据对应的写日志fsync到磁盘上去，性能非常非常差，吞吐量很低; 确保说redis里的数据一条都不丢，那就只能这样了 mysql -&gt; 内存策略，大量磁盘，QPS到多少，一两k。QPS，每秒钟的请求数量redis -&gt; 内存，磁盘持久化，QPS到多少，单机，一般来说，上万QPS没问题 everysec: 每秒将os cache中的数据fsync到磁盘，这个最常用的，生产环境一般都这么配置，性能很高，QPS还是可以上万的 no: 仅仅redis负责将数据写入os cache就撒手不管了，然后后面os自己会时不时有自己的策略将数据刷入磁盘，不可控了 2、AOF持久化的数据恢复实验 （1）先仅仅打开RDB，写入一些数据，然后kill -9杀掉redis进程，接着重启redis，发现数据没了，因为RDB快照还没生成（2）打开AOF的开关，启用AOF持久化（3）写入一些数据，观察AOF文件中的日志内容 其实你在appendonly.aof文件中，可以看到刚写的日志，它们其实就是先写入os cache的，然后1秒后才fsync到磁盘中，只有fsync到磁盘中了，才是安全的，要不然光是在os cache中，机器只要重启，就什么都没了 （4）kill -9杀掉redis进程，重新启动redis进程，发现数据被恢复回来了，就是从AOF文件中恢复回来的 redis进程启动的时候，直接就会从appendonly.aof中加载所有的日志，把内存中的数据恢复回来 3、AOF rewrite redis中的数据其实有限的，很多数据可能会自动过期，可能会被用户删除，可能会被redis用缓存清除的算法清理掉 redis中的数据会不断淘汰掉旧的，就一部分常用的数据会被自动保留在redis内存中 所以可能很多之前的已经被清理掉的数据，对应的写日志还停留在AOF中，AOF日志文件就一个，会不断的膨胀，到很大很大 所以AOF会自动在后台每隔一定时间做rewrite操作，比如日志里已经存放了针对100w数据的写日志了; redis内存只剩下10万; 基于内存中当前的10万数据构建一套最新的日志，到AOF中; 覆盖之前的老日志; 确保AOF日志文件不会过大，保持跟redis内存数据量一致 redis 2.4之前，还需要手动，开发一些脚本，crontab，通过BGREWRITEAOF命令去执行AOF rewrite，但是redis 2.4之后，会自动进行rewrite操作 在redis.conf中，可以配置rewrite策略 auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb 比如说上一次AOF rewrite之后，是128mb 然后就会接着128mb继续写AOF的日志，如果发现增长的比例，超过了之前的100%，256mb，就可能会去触发一次rewrite 但是此时还要去跟min-size，64mb去比较，256mb &gt; 64mb，才会去触发rewrite （1）redis fork一个子进程（2）子进程基于当前内存中的数据，构建日志，开始往一个新的临时的AOF文件中写入日志（3）redis主进程，接收到client新的写操作之后，在内存中写入日志，同时新的日志也继续写入旧的AOF文件（4）子进程写完新的日志文件之后，redis主进程将内存中的新日志再次追加到新的AOF文件中（5）用新的日志文件替换掉旧的日志文件 4、AOF破损文件的修复 如果redis在append数据到AOF文件时，机器宕机了，可能会导致AOF文件破损 用redis-check-aof –fix命令来修复破损的AOF文件 5、AOF和RDB同时工作 （1）如果RDB在执行snapshotting操作，那么redis不会执行AOF rewrite; 如果redis再执行AOF rewrite，那么就不会执行RDB snapshotting（2）如果RDB在执行snapshotting，此时用户执行BGREWRITEAOF命令，那么等RDB快照生成之后，才会去执行AOF rewrite（3）同时有RDB snapshot文件和AOF日志文件，那么redis重启的时候，会优先使用AOF进行数据恢复，因为其中的日志更完整 6、最后一个小实验，让大家对redis的数据恢复有更加深刻的体会 （1）在有rdb的dump和aof的appendonly的同时，rdb里也有部分数据，aof里也有部分数据，这个时候其实会发现，rdb的数据不会恢复到内存中（2）我们模拟让aof破损，然后fix，有一条数据会被fix删除（3）再次用fix得aof文件去重启redis，发现数据只剩下一条了 数据恢复完全是依赖于底层的磁盘的持久化的，主要rdb和aof上都没有数据，那就没了 9.29redis在企业级数据备份方案以及数据恢复负灾演练到这里为止，其实还是停留在简单学习知识的程度，学会了redis的持久化的原理和操作，但是在企业中，持久化到底是怎么去用得呢？ 企业级的数据备份和各种灾难下的数据恢复，是怎么做得呢？ 1、企业级的持久化的配置策略 在企业中，RDB的生成策略，用默认的也差不多 save 60 10000：如果你希望尽可能确保说，RDB最多丢1分钟的数据，那么尽量就是每隔1分钟都生成一个快照，低峰期，数据量很少，也没必要 10000-&gt;生成RDB，1000-&gt;RDB，这个根据你自己的应用和业务的数据量，你自己去决定 AOF一定要打开，fsync，everysec auto-aof-rewrite-percentage 100: 就是当前AOF大小膨胀到超过上次100%，上次的两倍auto-aof-rewrite-min-size 64mb: 根据你的数据量来定，16mb，32mb 2、企业级的数据备份方案 RDB非常适合做冷备，每次生成之后，就不会再有修改了 数据备份方案 （1）写crontab定时调度脚本去做数据备份（2）每小时都copy一份rdb的备份，到一个目录中去，仅仅保留最近48小时的备份（3）每天都保留一份当日的rdb的备份，到一个目录中去，仅仅保留最近1个月的备份（4）每次copy备份的时候，都把太旧的备份给删了（5）每天晚上将当前服务器上所有的数据备份，发送一份到远程的云服务上去 /usr/local/redis 每小时copy一次备份，删除48小时前的数据 crontab -e 0 sh /usr/local/redis/copy/redis_rdb_copy_hourly.sh redis_rdb_copy_hourly.sh #!/bin/sh cur_date=date +%Y%m%d%krm -rf /usr/local/redis/snapshotting/$cur_datemkdir /usr/local/redis/snapshotting/$cur_datecp /var/redis/6379/dump.rdb /usr/local/redis/snapshotting/$cur_date del_date=date -d -48hour +%Y%m%d%krm -rf /usr/local/redis/snapshotting/$del_date 每天copy一次备份 crontab -e 0 0 * sh /usr/local/redis/copy/redis_rdb_copy_daily.sh redis_rdb_copy_daily.sh #!/bin/sh cur_date=date +%Y%m%drm -rf /usr/local/redis/snapshotting/$cur_datemkdir /usr/local/redis/snapshotting/$cur_datecp /var/redis/6379/dump.rdb /usr/local/redis/snapshotting/$cur_date del_date=date -d -1month +%Y%m%drm -rf /usr/local/redis/snapshotting/$del_date 每天一次将所有数据上传一次到远程的云服务器上去 3、数据恢复方案 （1）如果是redis进程挂掉，那么重启redis进程即可，直接基于AOF日志文件恢复数据 不演示了，在AOF数据恢复那一块，演示了，fsync everysec，最多就丢一秒的数 （2）如果是redis进程所在机器挂掉，那么重启机器后，尝试重启redis进程，尝试直接基于AOF日志文件进行数据恢复 AOF没有破损，也是可以直接基于AOF恢复的 AOF append-only，顺序写入，如果AOF文件破损，那么用redis-check-aof fix （3）如果redis当前最新的AOF和RDB文件出现了丢失/损坏，那么可以尝试基于该机器上当前的某个最新的RDB数据副本进行数据恢复 当前最新的AOF和RDB文件都出现了丢失/损坏到无法恢复，一般不是机器的故障，人为 大数据系统，hadoop，有人不小心就把hadoop中存储的大量的数据文件对应的目录，rm -rf一下，我朋友的一个小公司，运维不太靠谱，权限也弄的不太好 /var/redis/6379下的文件给删除了 找到RDB最新的一份备份，小时级的备份可以了，小时级的肯定是最新的，copy到redis里面去，就可以恢复到某一个小时的数据 容灾演练 我跟大家解释一下，我其实上课，为什么大量的讲师可能讲课就是纯PPT，或者是各种复制粘贴，都不是现场讲解和写代码演示的 很容易出错，为了避免出错，一般就会那样玩儿 吐槽，念PPT，效果很差 真实的，备课，讲课不可避免，会出现一些问题，但是我觉得还好，真实 appendonly.aof + dump.rdb，优先用appendonly.aof去恢复数据，但是我们发现redis自动生成的appendonly.aof是没有数据的 然后我们自己的dump.rdb是有数据的，但是明显没用我们的数据 redis启动的时候，自动重新基于内存的数据，生成了一份最新的rdb快照，直接用空的数据，覆盖掉了我们有数据的，拷贝过去的那份dump.rdb 你停止redis之后，其实应该先删除appendonly.aof，然后将我们的dump.rdb拷贝过去，然后再重启redis 很简单，就是虽然你删除了appendonly.aof，但是因为打开了aof持久化，redis就一定会优先基于aof去恢复，即使文件不在，那就创建一个新的空的aof文件 停止redis，暂时在配置中关闭aof，然后拷贝一份rdb过来，再重启redis，数据能不能恢复过来，可以恢复过来 脑子一热，再关掉redis，手动修改配置文件，打开aof，再重启redis，数据又没了，空的aof文件，所有数据又没了 在数据安全丢失的情况下，基于rdb冷备，如何完美的恢复数据，同时还保持aof和rdb的双开 停止redis，关闭aof，拷贝rdb备份，重启redis，确认数据恢复，直接在命令行热修改redis配置，打开aof，这个redis就会将内存中的数据对应的日志，写入aof文件中 此时aof和rdb两份数据文件的数据就同步了 redis config set热修改配置参数，可能配置文件中的实际的参数没有被持久化的修改，再次停止redis，手动修改配置文件，打开aof的命令，再次重启redis （4）如果当前机器上的所有RDB文件全部损坏，那么从远程的云服务上拉取最新的RDB快照回来恢复数据 （5）如果是发现有重大的数据错误，比如某个小时上线的程序一下子将数据全部污染了，数据全错了，那么可以选择某个更早的时间点，对数据进行恢复 举个例子，12点上线了代码，发现代码有bug，导致代码生成的所有的缓存数据，写入redis，全部错了 找到一份11点的rdb的冷备，然后按照上面的步骤，去恢复到11点的数据，不就可以了吗 redis通过主从架构实现读写分离，完成10万+QPS1、redis高并发跟整个系统的高并发之间的关系 redis，你要搞高并发的话，不可避免，要把底层的缓存搞得很好 mysql，高并发，做到了，那么也是通过一系列复杂的分库分表，订单系统，事务要求的，QPS到几万，比较高了 要做一些电商的商品详情页，真正的超高并发，QPS上十万，甚至是百万，一秒钟百万的请求量 光是redis是不够的，但是redis是整个大型的缓存架构中，支撑高并发的架构里面，非常重要的一个环节 首先，你的底层的缓存中间件，缓存系统，必须能够支撑的起我们说的那种高并发，其次，再经过良好的整体的缓存架构的设计（多级缓存架构、热点缓存），支撑真正的上十万，甚至上百万的高并发 2、redis不能支撑高并发的瓶颈在哪里？ 单机 3、如果redis要支撑超过10万+的并发，那应该怎么做？ 单机的redis几乎不太可能说QPS超过10万+，除非一些特殊情况，比如你的机器性能特别好，配置特别高，物理机，维护做的特别好，而且你的整体的操作不是太复杂 单机在几万 读写分离，一般来说，对缓存，一般都是用来支撑读高并发的，写的请求是比较少的，可能写请求也就一秒钟几千，一两千 大量的请求都是读，一秒钟二十万次读 读写分离 主从架构 -&gt; 读写分离 -&gt; 支撑10万+读QPS的架构 4、接下来要讲解的一个topic redis replication redis主从架构 -&gt; 读写分离架构 -&gt; 可支持水平扩展的读高并发架构 redis replication（主从架构）基本原理课程大纲 1、图解redis replication基本原理2、redis replication的核心机制3、master持久化对于主从架构的安全保障的意义 redis replication -&gt; 主从架构 -&gt; 读写分离 -&gt; 水平扩容支撑读高并发 redis replication的最最基本的原理，铺垫 1、图解redis replication基本原理 2、redis replication的核心机制 （1）redis采用异步方式复制数据到slave节点，不过redis 2.8开始，slave node会周期性地确认自己每次复制的数据量（2）一个master node是可以配置多个slave node的（3）slave node也可以连接其他的slave node（4）slave node做复制的时候，是不会block master node的正常工作的（5）slave node在做复制的时候，也不会block对自己的查询操作，它会用旧的数据集来提供服务; 但是复制完成的时候，需要删除旧数据集，加载新数据集，这个时候就会暂停对外服务了（6）slave node主要用来进行横向扩容，做读写分离，扩容的slave node可以提高读的吞吐量 slave，高可用性，有很大的关系 3、master持久化对于主从架构的安全保障的意义 如果采用了主从架构，那么建议必须开启master node的持久化！ 不建议用slave node作为master node的数据热备，因为那样的话，如果你关掉master的持久化，可能在master宕机重启的时候数据是空的，然后可能一经过复制，salve node数据也丢了 master -&gt; RDB和AOF都关闭了 -&gt; 全部在内存中 master宕机，重启，是没有本地数据可以恢复的，然后就会直接认为自己IDE数据是空的 master就会将空的数据集同步到slave上去，所有slave的数据全部清空 100%的数据丢失 master节点，必须要使用持久化机制 第二个，master的各种备份方案，要不要做，万一说本地的所有文件丢失了; 从备份中挑选一份rdb去恢复master; 这样才能确保master启动的时候，是有数据的 即使采用了后续讲解的高可用机制，slave node可以自动接管master node，但是也可能sentinal还没有检测到master failure，master node就自动重启了，还是可能导致上面的所有slave node数据清空故障 9.30redis主从复制原理细讲1、复制的完整流程 （1）slave node启动，仅仅保存master node的信息，包括master node的host和ip，但是复制流程没开始 master host和ip是从哪儿来的，redis.conf里面的slaveof配置的 （2）slave node内部有个定时任务，每秒检查是否有新的master node要连接和复制，如果发现，就跟master node建立socket网络连接（3）slave node发送ping命令给master node（4）口令认证，如果master设置了requirepass，那么salve node必须发送masterauth的口令过去进行认证（5）master node第一次执行全量复制，将所有数据发给slave node（6）master node后续持续将写命令，异步复制给slave node 2、数据同步相关的核心机制 指的就是第一次slave连接msater的时候，执行的全量复制，那个过程里面你的一些细节的机制 （1）master和slave都会维护一个offset master会在自身不断累加offset，slave也会在自身不断累加offsetslave每秒都会上报自己的offset给master，同时master也会保存每个slave的offset 这个倒不是说特定就用在全量复制的，主要是master和slave都要知道各自的数据的offset，才能知道互相之间的数据不一致的情况 （2）backlog master node有一个backlog，默认是1MB大小master node给slave node复制数据时，也会将数据在backlog中同步写一份backlog主要是用来做全量复制中断候的增量复制的 （3）master run id info server，可以看到master run id如果根据host+ip定位master node，是不靠谱的，如果master node重启或者数据出现了变化，那么slave node应该根据不同的run id区分，run id不同就做全量复制如果需要不更改run id重启redis，可以使用redis-cli debug reload命令 （4）psync 从节点使用psync从master node进行复制，psync runid offsetmaster node会根据自身的情况返回响应信息，可能是FULLRESYNC runid offset触发全量复制，可能是CONTINUE触发增量复制 3、全量复制 （1）master执行bgsave，在本地生成一份rdb快照文件（2）master node将rdb快照文件发送给salve node，如果rdb复制时间超过60秒（repl-timeout），那么slave node就会认为复制失败，可以适当调节大这个参数（3）对于千兆网卡的机器，一般每秒传输100MB，6G文件，很可能超过60s（4）master node在生成rdb时，会将所有新的写命令缓存在内存中，在salve node保存了rdb之后，再将新的写命令复制给salve node（5）client-output-buffer-limit slave 256MB 64MB 60，如果在复制期间，内存缓冲区持续消耗超过64MB，或者一次性超过256MB，那么停止复制，复制失败（6）slave node接收到rdb之后，清空自己的旧数据，然后重新加载rdb到自己的内存中，同时基于旧的数据版本对外提供服务（7）如果slave node开启了AOF，那么会立即执行BGREWRITEAOF，重写AOF rdb生成、rdb通过网络拷贝、slave旧数据的清理、slave aof rewrite，很耗费时间 如果复制的数据量在4G~6G之间，那么很可能全量复制时间消耗到1分半到2分钟 4、增量复制 （1）如果全量复制过程中，master-slave网络连接断掉，那么salve重新连接master时，会触发增量复制（2）master直接从自己的backlog中获取部分丢失的数据，发送给slave node，默认backlog就是1MB（3）msater就是根据slave发送的psync中的offset来从backlog中获取数据的 5、heartbeat 主从节点互相都会发送heartbeat信息 master默认每隔10秒发送一次heartbeat，salve node每隔1秒发送一个heartbeat 6、异步复制 master每次接收到写命令之后，现在内部写入数据，然后异步发送给slave node 搭建redis主从节点之前几讲都是在铺垫各种redis replication的原理，和知识，主从，读写分离，画图 知道了这些东西，关键是怎么搭建呢？？？ 一主一从，往主节点去写，在从节点去读，可以读到，主从架构就搭建成功了 1、启用复制，部署slave node wget http://downloads.sourceforge.net/tcl/tcl8.6.1-src.tar.gztar -xzvf tcl8.6.1-src.tar.gzcd /usr/local/tcl8.6.1/unix/./configuremake &amp;&amp; make install 使用redis-3.2.8.tar.gz（截止2017年4月的最新稳定版）tar -zxvf redis-3.2.8.tar.gzcd redis-3.2.8make &amp;&amp; make test &amp;&amp; make install （1）redis utils目录下，有个redis_init_script脚本（2）将redis_init_script脚本拷贝到linux的/etc/init.d目录中，将redis_init_script重命名为redis_6379，6379是我们希望这个redis实例监听的端口号（3）修改redis_6379脚本的第6行的REDISPORT，设置为相同的端口号（默认就是6379）（4）创建两个目录：/etc/redis（存放redis的配置文件），/var/redis/6379（存放redis的持久化文件）（5）修改redis配置文件（默认在根目录下，redis.conf），拷贝到/etc/redis目录中，修改名称为6379.conf （6）修改redis.conf中的部分配置为生产环境 daemonize yes 让redis以daemon进程运行pidfile /var/run/redis_6379.pid 设置redis的pid文件位置port 6379 设置redis的监听端口号dir /var/redis/6379 设置持久化文件的存储位置 (设置持久化文件的位置) （7）让redis跟随系统启动自动启动 在redis_6379脚本中，最上面，加入两行注释 chkconfig: 2345 90 10description: Redis is a persistent key-value databasechkconfig redis_6379 on（随着系统启动+） 在slave node上配置（redis.conf文件中只要修改slaveof相关配置）： slaveof 192.168.1.1（主节点） 6379（redis端口号），即可也可以使用slaveof命令 则配置成自己是从节点2、强制读写分离 基于主从复制架构，实现读写分离 redis slave node只读，默认开启，slave-read-only 开启了只读的redis slave node，会拒绝所有的写操作，这样可以强制搭建成读写分离的架构 3、集群安全认证 master上启用安全认证，requirepassmaster连接口令，masterauth 4、读写分离架构的测试 先启动主节点，eshop-cache01上的redis实例再启动从节点，eshop-cache02上的redis实例 刚才我调试了一下，redis slave node一直说没法连接到主节点的6379的端口 在搭建生产环境的集群的时候，不要忘记修改一个配置，bind bind 127.0.0.1 -&gt; 本地的开发调试的模式，就只能127.0.0.1本地才能访问到6379的端口 每个redis.conf中的bind 127.0.0.1 -&gt; bind自己的ip地址在每个节点上都: iptables -A INPUT -ptcp –dport 6379 -j ACCEPT redis-cli -h ipaddrinfo replication 在主上写，在从上读ps -ef | grep redis查看redis进程是否已经启动 哨兵架构基础知识讲解1、哨兵的介绍 sentinal，中文名是哨兵 哨兵是redis集群架构中非常重要的一个组件，主要功能如下 （1）集群监控，负责监控redis master和slave进程是否正常工作（2）消息通知，如果某个redis实例有故障，那么哨兵负责发送消息作为报警通知给管理员（3）故障转移，如果master node挂掉了，会自动转移到slave node上（4）配置中心，如果故障转移发生了，通知client客户端新的master地址 哨兵本身也是分布式的，作为一个哨兵集群去运行，互相协同工作 （1）故障转移时，判断一个master node是宕机了，需要大部分的哨兵都同意才行，涉及到了分布式选举的问题（2）即使部分哨兵节点挂掉了，哨兵集群还是能正常工作的，因为如果一个作为高可用机制重要组成部分的故障转移系统本身是单点的，那就很坑爹了 目前采用的是sentinal 2版本，sentinal 2相对于sentinal 1来说，重写了很多代码，主要是让故障转移的机制和算法变得更加健壮和简单 2、哨兵的核心知识 （1）哨兵至少需要3个实例，来保证自己的健壮性（2）哨兵 + redis主从的部署架构，是不会保证数据零丢失的，只能保证redis集群的高可用性（3）对于哨兵 + redis主从这种复杂的部署架构，尽量在测试环境和生产环境，都进行充足的测试和演练 3、为什么redis哨兵集群只有2个节点无法正常工作？ 哨兵集群必须部署2个以上节点 如果哨兵集群仅仅部署了个2个哨兵实例，quorum=1 +—-+ +—-+| M1 |———| R1 || S1 | | S2 |+—-+ +—-+ Configuration: quorum = 1 master宕机，s1和s2中只要有1个哨兵认为master宕机就可以还行切换，同时s1和s2中会选举出一个哨兵来执行故障转移 同时这个时候，需要majority，也就是大多数哨兵都是运行的，2个哨兵的majority就是2（2的majority=2，3的majority=2，5的majority=3，4的majority=2），2个哨兵都运行着，就可以允许执行故障转移 但是如果整个M1和S1运行的机器宕机了，那么哨兵只有1个了，此时就没有majority来允许执行故障转移，虽然另外一台机器还有一个R1，但是故障转移不会执行 4、经典的3节点哨兵集群 12345+----+| M1 || S1 |+----+ | +—-+ | +—-+| R2 |—-+—-| R3 || S2 | | S3 |+—-+ +—-+ Configuration: quorum = 2，majority = 2 如果M1所在机器宕机了，那么三个哨兵还剩下2个，S2和S3可以一致认为master宕机，然后选举出一个来执行故障转移 同时3个哨兵的majority是2，所以还剩下的2个哨兵运行着，就可以允许执行故障转移 redis哨兵主备切换的数据丢失问题：异步复制，集群脑裂课程大纲 1、两种数据丢失的情况2、解决异步复制和脑裂导致的数据丢失 1、两种数据丢失的情况 主备切换的过程，可能会导致数据丢失 （1）异步复制导致的数据丢失 因为master -&gt; slave的复制是异步的，所以可能有部分数据还没复制到slave，master就宕机了，此时这些部分数据就丢失了 （2）脑裂导致的数据丢失 脑裂，也就是说，某个master所在机器突然脱离了正常的网络，跟其他slave机器不能连接，但是实际上master还运行着 此时哨兵可能就会认为master宕机了，然后开启选举，将其他slave切换成了master 这个时候，集群里就会有两个master，也就是所谓的脑裂 此时虽然某个slave被切换成了master，但是可能client还没来得及切换到新的master，还继续写向旧master的数据可能也丢失了 因此旧master再次恢复的时候，会被作为一个slave挂到新的master上去，自己的数据会清空，重新从新的master复制数据 2、解决异步复制和脑裂导致的数据丢失 min-slaves-to-write 1min-slaves-max-lag 10 要求至少有1个slave，数据复制和同步的延迟不能超过10秒 如果说一旦所有的slave，数据复制和同步的延迟都超过了10秒钟，那么这个时候，master就不会再接收任何请求了 上面两个配置可以减少异步复制和脑裂导致的数据丢失 （1）减少异步复制的数据丢失 有了min-slaves-max-lag这个配置，就可以确保说，一旦slave复制数据和ack延时太长，就认为可能master宕机后损失的数据太多了，那么就拒绝写请求，这样可以把master宕机时由于部分数据未同步到slave导致的数据丢失降低的可控范围内 （2）减少脑裂的数据丢失 如果一个master出现了脑裂，跟其他slave丢了连接，那么上面两个配置可以确保说，如果不能继续给指定数量的slave发送数据，而且slave超过10秒没有给自己ack消息，那么就直接拒绝客户端的写请求 这样脑裂后的旧master就不会接受client的新数据，也就避免了数据丢失 上面的配置就确保了，如果跟任何一个slave丢了连接，在10秒后发现没有slave给自己ack，那么就拒绝新的写请求 因此在脑裂场景下，最多就丢失10秒的数据 redis哨兵的多个核心底层原理的深入解析1、sdown和odown转换机制 sdown和odown两种失败状态 sdown是主观宕机，就一个哨兵如果自己觉得一个master宕机了，那么就是主观宕机 odown是客观宕机，如果quorum数量的哨兵都觉得一个master宕机了，那么就是客观宕机 sdown达成的条件很简单，如果一个哨兵ping一个master，超过了is-master-down-after-milliseconds指定的毫秒数之后，就主观认为master宕机 sdown到odown转换的条件很简单，如果一个哨兵在指定时间内，收到了quorum指定数量的其他哨兵也认为那个master是sdown了，那么就认为是odown了，客观认为master宕机 2、哨兵集群的自动发现机制 哨兵互相之间的发现，是通过redis的pub/sub系统实现的，每个哨兵都会往sentinel:hello这个channel里发送一个消息，这时候所有其他哨兵都可以消费到这个消息，并感知到其他的哨兵的存在 每隔两秒钟，每个哨兵都会往自己监控的某个master+slaves对应的sentinel:hello channel里发送一个消息，内容是自己的host、ip和runid还有对这个master的监控配置 每个哨兵也会去监听自己监控的每个master+slaves对应的sentinel:hello channel，然后去感知到同样在监听这个master+slaves的其他哨兵的存在 每个哨兵还会跟其他哨兵交换对master的监控配置，互相进行监控配置的同步 3、slave配置的自动纠正 哨兵会负责自动纠正slave的一些配置，比如slave如果要成为潜在的master候选人，哨兵会确保slave在复制现有master的数据; 如果slave连接到了一个错误的master上，比如故障转移之后，那么哨兵会确保它们连接到正确的master上 4、slave-&gt;master选举算法 如果一个master被认为odown了，而且majority哨兵都允许了主备切换，那么某个哨兵就会执行主备切换操作，此时首先要选举一个slave来 会考虑slave的一些信息 （1）跟master断开连接的时长（2）slave优先级（3）复制offset（4）run id 如果一个slave跟master断开连接已经超过了down-after-milliseconds的10倍，外加master宕机的时长，那么slave就被认为不适合选举为master (down-after-milliseconds * 10) + milliseconds_since_master_is_in_SDOWN_state 接下来会对slave进行排序 （1）按照slave优先级进行排序，slave priority越低，优先级就越高(slave priority是在redis.conf文件中自己设置的)（2）如果slave priority相同，那么看replica offset，哪个slave复制了越多的数据，offset越靠后，优先级就越高（3）如果上面两个条件都相同，那么选择一个run id比较小的那个slave 5、quorum和majority 每次一个哨兵要做主备切换，首先需要quorum数量的哨兵认为odown，然后选举出一个哨兵来做切换，这个哨兵还得得到majority哨兵的授权，才能正式执行切换 如果quorum &lt; majority，比如5个哨兵，majority就是3，quorum设置为2，那么就3个哨兵授权就可以执行切换 但是如果quorum &gt;= majority，那么必须quorum数量的哨兵都授权，比如5个哨兵，quorum是5，那么必须5个哨兵都同意授权，才能执行切换 6、configuration epoch 哨兵会对一套redis master+slave进行监控，有相应的监控的配置 执行切换的那个哨兵，会从要切换到的新master（salve-&gt;master）那里得到一个configuration epoch，这就是一个version号，每次切换的version号都必须是唯一的 如果第一个选举出的哨兵切换失败了，那么其他哨兵，会等待failover-timeout时间，然后接替继续执行切换，此时会重新获取一个新的configuration epoch，作为新的version号 7、configuraiton传播 哨兵完成切换之后，会在自己本地更新生成最新的master配置，然后同步给其他的哨兵，就是通过之前说的pub/sub消息机制 这里之前的version号就很重要了，因为各种消息都是通过一个channel去发布和监听的，所以一个哨兵完成一次新的切换之后，新的master配置是跟着新的version号的 其他的哨兵都是根据版本号的大小来更新自己的master配置的 redis哨兵集群的实战配置quorum的解释如下： （1）至少多少个哨兵要一致同意，master进程挂掉了，或者slave进程挂掉了，或者要启动一个故障转移操作（2）quorum是用来识别故障的，真正执行故障转移的时候，还是要在哨兵集群执行选举，选举一个哨兵进程出来执行故障转移操作（3）假设有5个哨兵，quorum设置了2，那么如果5个哨兵中的2个都认为master挂掉了; 2个哨兵中的一个就会做一个选举，选举一个哨兵出来，执行故障转移; 如果5个哨兵中有3个哨兵都是运行的，那么故障转移就会被允许执行 动手实操，练习如何操作部署哨兵集群，如何基于哨兵进行故障转移，还有一些企业级的配置方案 1、哨兵的配置文件(存放在redis安装目录下)sentinel.conf 最小的配置 每一个哨兵都可以去监控多个maser-slaves的主从架构 因为可能你的公司里，为不同的项目，部署了多个master-slaves的redis主从集群 相同的一套哨兵集群，就可以去监控不同的多个redis主从集群 你自己给每个redis主从集群分配一个逻辑的名称 sentinel monitor mymaster 127.0.0.1 6379 2sentinel down-after-milliseconds mymaster 60000sentinel failover-timeout mymaster 180000sentinel parallel-syncs mymaster 1 sentinel monitor resque 192.168.1.3 6380 4sentinel down-after-milliseconds resque 10000sentinel failover-timeout resque 180000sentinel parallel-syncs resque 5 sentinel monitor mymaster 127.0.0.1 6379 类似这种配置，来指定对一个master的监控，给监控的master指定的一个名称，因为后面分布式集群架构里会讲解，可以配置多个master做数据拆分 sentinel down-after-milliseconds mymaster 60000sentinel failover-timeout mymaster 180000sentinel parallel-syncs mymaster 1 上面的三个配置，都是针对某个监控的master配置的，给其指定上面分配的名称即可 上面这段配置，就监控了两个master node 这是最小的哨兵配置，如果发生了master-slave故障转移，或者新的哨兵进程加入哨兵集群，那么哨兵会自动更新自己的配置文件 sentinel monitor master-group-name hostname port quorum quorum的解释如下： （1）至少多少个哨兵要一致同意，master进程挂掉了，或者slave进程挂掉了，或者要启动一个故障转移操作（2）quorum是用来识别故障的，真正执行故障转移的时候，还是要在哨兵集群执行选举，选举一个哨兵进程出来执行故障转移操作（3）假设有5个哨兵，quorum设置了2，那么如果5个哨兵中的2个都认为master挂掉了; 2个哨兵中的一个就会做一个选举，选举一个哨兵出来，执行故障转移; 如果5个哨兵中有3个哨兵都是运行的，那么故障转移就会被允许执行 down-after-milliseconds，超过多少毫秒跟一个redis实例断了连接，哨兵就可能认为这个redis实例挂了 parallel-syncs，新的master别切换之后，同时有多少个slave被切换到去连接新master，重新做同步，数字越低，花费的时间越多 假设你的redis是1个master，4个slave 然后master宕机了，4个slave中有1个切换成了master，剩下3个slave就要挂到新的master上面去 这个时候，如果parallel-syncs是1，那么3个slave，一个一个地挂接到新的master上面去，1个挂接完，而且从新的master sync完数据之后，再挂接下一个 如果parallel-syncs是3，那么一次性就会把所有slave挂接到新的master上去 failover-timeout，执行故障转移的timeout超时时长 2、在eshop-cache03上再部署一个redis 只要安装redis就可以了，不需要去部署redis实例的启动 wget http://downloads.sourceforge.net/tcl/tcl8.6.1-src.tar.gztar -xzvf tcl8.6.1-src.tar.gzcd /usr/local/tcl8.6.1/unix/./configuremake &amp;&amp; make install 使用redis-3.2.8.tar.gz（截止2017年4月的最新稳定版）tar -zxvf redis-3.2.8.tar.gzcd redis-3.2.8make &amp;&amp; make testmake install 2、正式的配置 哨兵默认用26379端口，默认不能跟其他机器在指定端口连通，只能在本地访问 mkdir /etc/sentinalmkdir -p /var/sentinal/5000 /etc/sentinel/5000.conf port 5000bind 192.168.31.187dir /var/sentinal/5000sentinel monitor mymaster 192.168.31.187 6379 2sentinel down-after-milliseconds mymaster 30000sentinel failover-timeout mymaster 60000sentinel parallel-syncs mymaster 1 port 5000bind 192.168.31.19dir /var/sentinal/5000sentinel monitor mymaster 192.168.31.187 6379 2sentinel down-after-milliseconds mymaster 30000sentinel failover-timeout mymaster 60000sentinel parallel-syncs mymaster 1 port 5000bind 192.168.31.227dir /var/sentinal/5000sentinel monitor mymaster 192.168.31.187 6379 2sentinel down-after-milliseconds mymaster 30000sentinel failover-timeout mymaster 60000sentinel parallel-syncs mymaster 1 3、启动哨兵进程 在eshop-cache01、eshop-cache02、eshop-cache03三台机器上，分别启动三个哨兵进程，组成一个集群，观察一下日志的输出 redis-sentinel /etc/sentinal/5000.confredis-server /etc/sentinal/5000.conf –sentinel 日志里会显示出来，每个哨兵都能去监控到对应的redis master，并能够自动发现对应的slave 哨兵之间，互相会自动进行发现，用的就是之前说的pub/sub，消息发布和订阅channel消息系统和机制 4、检查哨兵状态 redis-cli -h 192.168.31.187 -p 5000 sentinel master mymasterSENTINEL slaves mymasterSENTINEL sentinels mymaster SENTINEL get-master-addr-by-name mymaster redis cluster横向扩容master1、单机redis在海量数据面前的瓶颈 2、怎么才能够突破单机瓶颈，让redis支撑海量数据？ 3、redis的集群架构 redis cluster 支撑N个redis master node，每个master node都可以挂载多个slave node 读写分离的架构，对于每个master来说，写就写到master，然后读就从mater对应的slave去读 高可用，因为每个master都有salve节点，那么如果mater挂掉，redis cluster这套机制，就会自动将某个slave切换成master redis cluster（多master + 读写分离 + 高可用） 我们只要基于redis cluster去搭建redis集群即可，不需要手工去搭建replication复制+主从架构+读写分离+哨兵集群+高可用 4、redis cluster vs. replication + sentinal 如果你的数据量很少，主要是承载高并发高性能的场景，比如你的缓存一般就几个G，单机足够了 replication，一个mater，多个slave，要几个slave跟你的要求的读吞吐量有关系，然后自己搭建一个sentinal集群，去保证redis主从架构的高可用性，就可以了 redis cluster，主要是针对海量数据+高并发+高可用的场景，海量数据，如果你的数据量很大，那么建议就用redis cluster redis阶段性总结1、讲解redis是为了什么？ topic：高并发、亿级流量、高性能、海量数据的场景，电商网站的商品详情页系统的缓存架构 商品详情页系统，大型电商网站，会有很多部分组成，但是支撑高并发、亿级流量的，主要就是其中的大型的缓存架构 在这个大型的缓存架构中，redis是最最基础的一层 高并发，缓存架构中除了redis，还有其他的组成部分，但是redis至关重要 大量的离散请求，随机请求，各种你未知的用户过来的请求，上千万用户过来访问，每个用户访问10次; 集中式的请求，1个用户过来，一天访问1亿次 支撑商品展示的最重要的，就是redis cluster，去抗住每天上亿的请求流量，支撑高并发的访问 redis cluster在整个缓存架构中，如何跟其他几个部分搭配起来组成一个大型的缓存系统，后面再讲 2、讲解的redis可以实现什么效果？ 我之前一直在redis的各个知识点的讲解之前都强调一下，我们要讲解的每个知识点，要解决的问题是什么？？？ redis：持久化、复制（主从架构）、哨兵（高可用，主备切换）、redis cluster（海量数据+横向扩容+高可用/主备切换） 持久化：高可用的一部分，在发生redis集群灾难的情况下（比如说部分master+slave全部死掉了），如何快速进行数据恢复，快速实现服务可用，才能实现整个系统的高可用 复制：主从架构，master -&gt; slave 复制，读写分离的架构，写master，读slave，横向扩容slave支撑更高的读吞吐，读高并发，10万，20万，30万，上百万，QPS，横向扩容 哨兵：高可用，主从架构，在master故障的时候，快速将slave切换成master，实现快速的灾难恢复，实现高可用性 redis cluster：多master读写，数据分布式的存储，横向扩容，水平扩容，快速支撑高达的数据量+更高的读写QPS，自动进行master -&gt; slave的主备切换，高可用 让底层的缓存系统，redis，实现能够任意水平扩容，支撑海量数据（1T+，几十T，10G * 600 redis = 6T），支撑很高的读写QPS（redis单机在几万QPS，10台，几十万QPS），高可用性（给我们每个redis实例都做好AOF+RDB的备份策略+容灾策略，slave -&gt; master主备切换） 1T+海量数据、10万+读写QPS、99.99%高可用性 3、redis的第一套企业级的架构 如果你的数据量不大，单master就可以容纳，一般来说你的缓存的总量在10G以内就可以，那么建议按照以下架构去部署redis redis持久化+备份方案+容灾方案+replication（主从+读写分离）+sentinal（哨兵集群，3个节点，高可用性） 可以支撑的数据量在10G以内，可以支撑的写QPS在几万左右，可以支撑的读QPS可以上10万以上（随你的需求，水平扩容slave节点就可以），可用性在99.99% 4、redis的第二套企业级架构 如果你的数据量很大，比如我们课程的topic，大型电商网站的商品详情页的架构（对标那些国内排名前三的大电商网站，宝，东，*宁易购），数据量是很大的 海量数据 redis cluster 多master分布式存储数据，水平扩容 支撑更多的数据量，1T+以上没问题，只要扩容master即可 读写QPS分别都达到几十万都没问题，只要扩容master即可，redis cluster，读写分离，支持不太好，readonly才能去slave上读 支撑99.99%可用性，也没问题，slave -&gt; master的主备切换，冗余slave去进一步提升可用性的方案（每个master挂一个slave，但是整个集群再加个3个slave冗余一下） 我们课程里，两套架构都讲解了，后续的业务系统的开发，主要是基于redis cluster去做 5、我们现在课程讲解的项目进展到哪里了？ 我们要做后续的业务系统的开发，redis的架构部署好，是第一件事情，也是非常重要的，也是你作为一个架构师而言，在对系统进行设计的时候，你必须要考虑到底层的redis的并发、性能、能支撑的数据量、可用性 redis：水平扩容，海量数据，上10万的读写QPS，99.99%高可用性 从架构的角度，我们的redis是可以做到的，水平扩容，只要机器足够，到1T数据量，50万读写QPS，99.99% 正式开始做大型电商网站的商品详情页系统，大规模的缓存架构设计]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL注入]]></title>
    <url>%2F2019%2F10%2F01%2FSQL%E6%B3%A8%E5%85%A5%2F</url>
    <content type="text"><![CDATA[SQL注入简介SQL注入是网站存在最多也是最简单的漏洞，主要原因是程序员在开发用户和数据库交互的系统时，没有对用户输入的字符串进行过滤，转义，限制或处理不严谨，导致用户可以通过输入精心构造的字符串去非法获取到数据库中的数据。 SQL注入原理一般用户登录用的SQL语句为：SELECT FROM user WHERE username=’admin’ AND password=’passwd’，此处admin和passwd分别为用户输入的用户名和密码，如果程序员没有对用户输入的用户名和密码做处理，就可以构造万能密码成功绕过登录验证，如用户输入‘or 1#,SQL语句将变为：SELECT FROM user WHERE username=’’or 1#’ AND password=’’，‘’or 1为TRUE，#注释掉后面的内容，所以查询语句可以正确执行。 mybatis是如何防止SQL注入的1、首先看一下下面两个sql语句的区别：123456&lt;select id="selectByNameAndPassword" parameterType="java.util.Map" resultMap="BaseResultMap"&gt;select id, username, password, rolefrom userwhere username = #&#123;username,jdbcType=VARCHAR&#125;and password = #&#123;password,jdbcType=VARCHAR&#125;&lt;/select&gt; 123456&lt;select id="selectByNameAndPassword" parameterType="java.util.Map" resultMap="BaseResultMap"&gt;select id, username, password, rolefrom userwhere username = $&#123;username,jdbcType=VARCHAR&#125;and password = $&#123;password,jdbcType=VARCHAR&#125;&lt;/select&gt; mybatis中的#和$的区别： 1、#将传入的数据都当成一个字符串，会对自动传入的数据加一个双引号。如：where username=#{username}，如果传入的值是111,那么解析成sql时的值为where username=”111”, 如果传入的值是id，则解析成的sql为where username=”id”. 2、$将传入的数据直接显示生成在sql中。如：where username=${username}，如果传入的值是111,那么解析成sql时的值为where username=111；如果传入的值是;drop table user;，则解析成的sql为：select id, username, password, role from user where username=;drop table user;3、#方式能够很大程度防止sql注入，$方式无法防止Sql注入。4、$方式一般用于传入数据库对象，例如传入表名.5、一般能用#的就别用$，若不得不使用“${xxx}”这样的参数，要手工地做好过滤工作，来防止sql注入攻击。6、在MyBatis中，“${xxx}”这样格式的参数会直接参与SQL编译，从而不能避免注入攻击。但涉及到动态表名和列名时，只能使用“${xxx}”这样的参数格式。所以，这样的参数需要我们在代码中手工进行处理来防止注入。【结论】在编写MyBatis的映射语句时，尽量采用“#{xxx}”这样的格式。若不得不使用“${xxx}”这样的参数，要手工地做好过滤工作，来防止SQL注入攻击。 2、什么是sql注入 sql注入解释：是一种代码注入技术，用于攻击数据驱动的应用，恶意的SQL语句被插入到执行的实体字段中（例如，为了转储数据库内容给攻击者） SQL**注入，大家都不陌生，是一种常见的攻击方式。攻击者在界面的表单信息或URL上输入一些奇怪的SQL片段（例如“or ‘1’=’1’”这样的语句），有可能入侵参数检验不足的应用程序。所以，在我们的应用中需要做一些工作，来防备这样的攻击方式。在一些安全性要求很高的应用中（比如银行软件），经常使用将SQL**语句全部替换为存储过程这样的方式，来防止SQL注入。这当然是一种很安全的方式，但我们平时开发中，可能不需要这种死板的方式。 3、mybatis是如何做到防止sql注入的 MyBatis框架作为一款半自动化的持久层框架，其SQL语句都要我们自己手动编写，这个时候当然需要防止SQL注入。其实，MyBatis的SQL是一个具有“输入+输出”的功能，类似于函数的结构，参考上面的两个例子。其中，parameterType表示了输入的参数类型，resultType表示了输出的参数类型。回应上文，如果我们想防止SQL注入，理所当然地要在输入参数上下功夫。上面代码中使用 # 的即输入参数在SQL中拼接的部分，传入参数后，打印出执行的SQL语句，会看到SQL是这样的： 1select id, username, password, role from user where username=? and password=? 不管输入什么参数，打印出的SQL都是这样的。这是因为MyBatis启用了预编译功能，在SQL执行前，会先将上面的SQL发送给数据库进行编译；执行时，直接使用编译好的SQL，替换占位符“?”就可以了。因为SQL注入只能对编译过程起作用，所以这样的方式就很好地避免了SQL注入的问题。 【底层实现原理】MyBatis是如何做到SQL预编译的呢？其实在框架底层，是JDBC中的PreparedStatement类在起作用，PreparedStatement是我们很熟悉的Statement的子类，它的对象包含了编译好的SQL语句。这种“准备好”的方式不仅能提高安全性，而且在多次执行同一个SQL时，能够提高效率。原因是SQL已编译好，再次执行时无需再编译。 1234567//安全的，预编译了的Connection conn = getConn();//获得连接String sql = &quot;select id, username, password, role from user where id=?&quot;; //执行sql前会预编译号该条语句PreparedStatement pstmt = conn.prepareStatement(sql); pstmt.setString(1, id); ResultSet rs=pstmt.executeUpdate(); ...... 12345678910//不安全的，没进行预编译private String getNameByUserId(String userId) &#123; Connection conn = getConn();//获得连接 String sql = &quot;select id,username,password,role from user where id=&quot; + id; //当id参数为&quot;3;drop table user;&quot;时，执行的sql语句如下: //select id,username,password,role from user where id=3; drop table user; PreparedStatement pstmt = conn.prepareStatement(sql); ResultSet rs=pstmt.executeUpdate(); ......&#125; 【 结论：】 #{}：相当于JDBC中的PreparedStatement ${}：是输出变量的值 简单说，#{}是经过预编译的，是安全的；${}是未经过预编译的，仅仅是取变量的值，是非安全的，存在SQL注入。如果我们order by语句后用了${}，那么不做任何处理的时候是存在SQL注入危险的。你说怎么防止，那我只能悲惨的告诉你，你得手动处理过滤一下输入的内容。如判断一下输入的参数的长度是否正常（注入语句一般很长），更精确的过滤则可以查询一下输入的参数是否在预期的参数集合中。 作者：淼淼之森 ###]]></content>
      <tags>
        <tag>查漏补缺</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA锁]]></title>
    <url>%2F2019%2F10%2F01%2FJAVA%E9%94%81%2F</url>
    <content type="text"><![CDATA[JAVA锁Java SE1.6为了减少获得锁和释放锁所带来的性能消耗，引入了“偏向锁”和“轻量级锁”，所以在Java SE1.6里锁一共有四种状态，无锁状态，偏向锁状态，轻量级锁状态和重量级锁状态，它会随着竞争情况逐渐升级。锁可以升级但不能降级，意味着偏向锁升级成轻量级锁后不能降级成偏向锁。这种锁升级却不能降级的策略，目的是为了提高获得锁和释放锁的效率。 悲观锁与乐观锁悲观锁总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁，读锁，写锁等，都是在做操作之前先上锁。JAVA中synchronize和ReentrantLock等独占锁就是悲观锁思想的实现。 乐观锁总是假设最好的情况，每次拿数据都认为别人不会修改，所以不会上锁。但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。 乐观锁适用于多读的应用类型，这样可以提高吞吐量。 java中的4种锁，分别是重量级锁，自旋锁，轻量级锁和偏向锁。重量级锁是悲观锁的一种，自旋锁，轻量级锁和偏向锁属于乐观锁。 乐观锁常见的两种实现方式 乐观锁一般会使用版本号机制或CAS算法实现。 1. 版本号机制一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一。当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值为当前数据库中的version值相等时才更新，否则重试更新操作，直到更新成功。 举一个简单的例子： 假设数据库中帐户信息表中有一个 version 字段，当前值为 1 ；而当前帐户余额字段（ balance ）为 $100 。 操作员 A 此时将其读出（ version=1 ），并从其帐户余额中扣除 $50（ $100-$50 ）。 在操作员 A 操作的过程中，操作员B 也读入此用户信息（ version=1 ），并从其帐户余额中扣除 $20 （ $100-$20 ）。 操作员 A 完成了修改工作，将数据版本号加一（ version=2 ），连同帐户扣除后余额（ balance=$50 ），提交至数据库更新，此时由于提交数据版本大于数据库记录当前版本，数据被更新，数据库记录 version 更新为 2 。 操作员 B 完成了操作，也将版本号加一（ version=2 ）试图向数据库提交数据（ balance=$80 ），但此时比对数据库记录版本时发现，操作员 B 提交的数据版本号为 2 ，数据库记录当前版本也为 2 ，不满足 “ 提交版本必须大于记录当前版本才能执行更新 “ 的乐观锁策略，因此，操作员 B 的提交被驳回。 这样，就避免了操作员 B 用基于 version=1 的旧数据修改的结果覆盖操作员A 的操作结果的可能。 2.简单回顾一下CAS算法CAS算法 即compare and swap（比较与交换），是一种有名的无锁算法。无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）。CAS算法涉及到三个操作数 需要读写的内存值 V 进行比较的值 A 拟写入的新值 B 当且仅当 V 的值等于 A时，CAS通过原子方式用新值B来更新V的值，否则不会执行任何操作（比较和替换是一个原子操作）。一般情况下是一个自旋操作，即不断的重试。 乐观锁的缺点 ABA 问题是乐观锁一个常见的问题 1 ABA 问题如果一个变量V初次读取的时候是A值，并且在准备赋值的时候检查到它仍然是A值，那我们就能说明它的值没有被其他线程修改过了吗？很明显是不能的，因为在这段时间它的值可能被改为其他值，然后又改回A，那CAS操作就会误认为它从来没有被修改过。这个问题被称为CAS操作的 “ABA”问题。 JDK 1.5 以后的 AtomicStampedReference 类就提供了此种能力，其中的 compareAndSet 方法就是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 2 循环时间长开销大自旋CAS（也就是不成功就一直循环执行直到成功）如果长时间不成功，会给CPU带来非常大的执行开销。 如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，pause指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline flush），从而提高CPU的执行效率。 3 只能保证一个共享变量的原子操作CAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效。但是从 JDK 1.5开始，提供了AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作.所以我们可以使用锁或者利用AtomicReference类把多个共享变量合并成一个共享变量来操作。 什么是自旋锁？自旋锁（spinlock）：是指当一个线程在获取锁的时候，如果锁已经被其它线程获取，那么该线程将循环等待，然后不断的判断锁是否能够被成功获取，直到获取到锁才会退出循环。 获取锁的线程一直处于活跃状态，但是并没有执行任何有效的任务，使用这种锁会造成busy-waiting。 它是为实现保护共享资源而提出一种锁机制。其实，自旋锁与互斥锁比较类似，它们都是为了解决对某项资源的互斥使用。无论是互斥锁，还是自旋锁，在任何时刻，最多只能有一个保持者，也就说，在任何时刻最多只能有一个执行单元获得锁。但是两者在调度机制上略有不同。对于互斥锁，如果资源已经被占用，资源申请者只能进入睡眠状态。但是自旋锁不会引起调用者睡眠，如果自旋锁已经被别的执行单元保持，调用者就一直循环在那里看是否该自旋锁的保持者已经释放了锁，”自旋”一词就是因此而得名。 Java如何实现自旋锁？下面是个简单的例子： 1234567891011121314public class SpinLock &#123; private AtomicReference&lt;Thread&gt; cas = new AtomicReference&lt;Thread&gt;(); public void lock() &#123; Thread current = Thread.currentThread(); // 利用CAS while (!cas.compareAndSet(null, current)) &#123; // DO nothing &#125; &#125; public void unlock() &#123; Thread current = Thread.currentThread(); cas.compareAndSet(current, null); &#125;&#125;1234567891011121314 lock（)方法利用的CAS，当第一个线程A获取锁的时候，能够成功获取到，不会进入while循环，如果此时线程A没有释放锁，另一个线程B又来获取锁，此时由于不满足CAS，所以就会进入while循环，不断判断是否满足CAS，直到A线程调用unlock方法释放了该锁。 自旋锁存在的问题 如果某个线程持有锁的时间过长，就会导致其它等待获取锁的线程进入循环等待，消耗CPU。使用不当会造成CPU使用率极高。 上面Java实现的自旋锁不是公平的，即无法满足等待时间最长的线程优先获取锁。不公平的锁就会存在“线程饥饿”问题。 自旋锁的优点 自旋锁不会使线程状态发生切换，一直处于用户态，即线程一直都是active的；不会使线程进入阻塞状态，减少了不必要的上下文切换，执行速度快 非自旋锁在获取不到锁的时候会进入阻塞状态，从而进入内核态，当获取到锁的时候需要从内核态恢复，需要线程上下文切换。 （线程被阻塞后便进入内核（Linux）调度状态，这个会导致系统在用户态与内核态之间来回切换，严重影响锁的性能） TicketLock主要解决的是公平性的问题。思路：每当有线程获取锁的时候，就给该线程分配一个递增的id，我们称之为排队号，同时，锁对应一个服务号，每当有线程释放锁，服务号就会递增，此时如果服务号与某个线程排队号一致，那么该线程就获得锁，由于排队号是递增的，所以就保证了最先请求获取锁的线程可以最先获取到锁，就实现了公平性。 可以想象成银行办理业务排队，排队的每一个顾客都代表一个需要请求锁的线程，而银行服务窗口表示锁，每当有窗口服务完成就把自己的服务号加一，此时在排队的所有顾客中，只有自己的排队号与服务号一致的才可以得到服务。 偏向锁Java偏向锁(Biased Locking)是Java6引入的一项多线程优化。偏向锁，顾名思义，它会偏向于第一个访问锁的线程，如果在运行过程中，同步锁只有一个线程访问，不存在多线程争用的情况，则线程是不需要触发同步的，这种情况下，就会给线程加一个偏向锁。如果在运行过程中，遇到了其他线程抢占锁，则持有偏向锁的线程会被挂起，JVM会消除它身上的偏向锁，将锁恢复到标准的轻量级锁。 它通过消除资源无竞争情况下的同步原语，进一步提高了程序的运行性能。 偏向锁的实现偏向锁获取过程： 访问Mark Word中偏向锁的标识是否设置成1，锁标志位是否为01，确认为可偏向状态。 如果为可偏向状态，则测试线程ID是否指向当前线程，如果是，进入步骤5，否则进入步骤3。 如果线程ID并未指向当前线程，则通过CAS操作竞争锁。如果竞争成功，则将Mark Word中线程ID设置为当前线程ID，然后执行5；如果竞争失败，执行4。 如果CAS获取偏向锁失败，则表示有竞争。当到达全局安全点（safepoint）时获得偏向锁的线程被挂起，偏向锁升级为轻量级锁，然后被阻塞在安全点的线程继续往下执行同步代码。（撤销偏向锁的时候会导致stop the word） 执行同步代码。 注意：第四步中到达安全点safepoint会导致stop the word，时间很短。 偏向锁的释放：偏向锁的撤销在上述第四步骤中有提到。偏向锁只有遇到其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁，线程不会主动去释放偏向锁。偏向锁的撤销，需要等待全局安全点（在这个时间点上没有字节码正在执行），它会首先暂停拥有偏向锁的线程，判断锁对象是否处于被锁定状态，撤销偏向锁后恢复到未锁定（标志位为“01”）或轻量级锁（标志位为“00”）的状态。 偏向锁的适用场景始终只有一个线程在执行同步块，在它没有执行完释放锁之前，没有其它线程去执行同步块，在锁无竞争的情况下使用，一旦有了竞争就升级为轻量级锁，升级为轻量级锁的时候需要撤销偏向锁，撤销偏向锁的时候会导致stop the word操作；在有锁的竞争时，偏向锁会多做很多额外操作，尤其是撤销偏向所的时候会导致进入安全点，安全点会导致stw，导致性能下降，这种情况下应当禁用； 总结自旋锁线程的阻塞和唤醒需要CPU从用户态转为核心态，频繁的阻塞和唤醒对CPU来说是一件负担很重的工作。同时我们可以发现，很多对象锁的锁定状态只会持续很短的一段时间，例如整数的自加操作，在很短的时间内阻塞并唤醒线程显然不值得，为此引入了自旋锁。 所谓“自旋”，就是让线程去执行一个无意义的循环，循环结束后再去重新竞争锁，如果竞争不到继续循环，循环过程中线程会一直处于running状态，但是基于JVM的线程调度，会出让时间片，所以其他线程依旧有申请锁和释放锁的机会。 自旋锁省去了阻塞锁的时间空间（队列的维护等）开销，但是长时间自旋就变成了“忙式等待”，忙式等待显然还不如阻塞锁。所以自旋的次数一般控制在一个范围内，例如10,100等，在超出这个范围后，自旋锁会升级为阻塞锁。（即从轻量级锁转变为重量级锁） 轻量级锁加锁 线程在执行同步块之前，JVM会先在当前线程的栈桢中创建用于存储锁记录的空间，并将对象头中的Mark Word复制到锁记录中，官方称为Displaced Mark Word。然后线程尝试使用CAS将对象头中的Mark Word替换为指向锁记录的指针。如果成功，当前线程获得锁，如果失败，则自旋获取锁，当自旋获取锁仍然失败时，表示存在其他线程竞争锁(两条或两条以上的线程竞争同一个锁)，则轻量级锁会膨胀成重量级锁。 解锁 轻量级解锁时，会使用原子的CAS操作来将Displaced Mark Word替换回到对象头，如果成功，则表示同步过程已完成。如果失败，表示有其他线程尝试过获取该锁，则要在释放锁的同时唤醒被挂起的线程。 重量级锁重量锁在JVM中又叫对象监视器（Monitor），它很像C中的Mutex，除了具备Mutex(0|1)互斥的功能，它还负责实现了Semaphore(信号量)的功能，也就是说它至少包含一个竞争锁的队列，和一个信号阻塞队列（wait队列），前者负责做互斥，后一个用于做线程同步。 如果说轻量级锁是在无竞争的情况下使用CAS操作去消除同步使用的互斥量，那偏向锁就是在无竞争的情况下把整个同步都消除掉，连CAS都不做了。]]></content>
      <tags>
        <tag>查漏补缺</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决缓存数据库双写不一致]]></title>
    <url>%2F2019%2F10%2F01%2F%E8%A7%A3%E5%86%B3%E7%BC%93%E5%AD%98%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8C%E5%86%99%E4%B8%8D%E4%B8%80%E8%87%B4%2F</url>
    <content type="text"><![CDATA[在讲双写不一致的时候，先将为什么会发生双写不一致。数据发生了变更，先删除了缓存，然后要去修改数据库，此时还没修改 一个请求过来，去读缓存，发现缓存空了，去查询数据库，查到了修改前的旧数据，放到了缓存中 数据变更的程序完成了数据库的修改 完了，数据库和缓存中的数据不一样了。。。。 hash路由的算法，跟HashMap中的hash算法是一样的。 通过线程池 +内存队列+通过同个商品的ID路由到同一个队列中，将写请求，和读请求做到一个串行化的效果。只有当写请求完成之后，工作线程才会进行读请求的进行。 更新请求：删除缓存，修改数据库 读请求：读缓存，发现空，进入队列中等待，排到了则读数据库，并将数据写入缓存，返回数据 问题1：当写请求不断积压，读请求等待的时间过长，超过最大等待时间，会直接读数据库，造成双写不一致。 所以在这时候需要添加机器，分散队列的写请求服务。 少量情况下，可能遇到读跟数据更新冲突的情况，如上所述，那么此时更新操作如果先入队列，之后可能会瞬间来了对这个数据大量的读请求，但是因为做了去重的优化，所以也就一个更新缓存的操作跟在它后面 等数据更新完了，读请求触发的缓存更新操作也完成，然后临时等待的读请求全部可以读到缓存中的数据 问题2：大量的读请求在服务上等待。 当有大量的读请求时，此时若缓存为空，那么会通过去重，只让一个读请求进入队列中等待操作。 但可能发生，队列中的读操作还没完成，大量的读请求在服务中等待，造成服务宕机。 12345678进入队列后，读请求会有一个等待的时间，等待同步更新操作完成，这样的话，一个数据变更的操作，先执行，删除缓存，然后再去更新数据库，但是还没完成更新此时如果一个读请求过来，读到了空的缓存，那么可以先将缓存更新的请求发送到队列中，此时会在队列中积压，然后同步等待缓存更新完成这里有一个优化点，一个队列中，其实多个更新缓存请求串在一起是没意义的，因此可以做过滤，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新请求操作进去了，直接等待前面的更新操作请求完成即可待那个队列对应的工作线程完成了上一个操作的数据库的修改之后，才会去执行下一个操作，也就是缓存更新的操作，此时会从数据库中读取最新的值，然后写入缓存中如果请求还在等待时间范围内，不断轮询发现可以取到值了，那么就直接返回; 如果请求等待的时间超过一定时长，那么这一次直接从数据库中读取当前的旧值 高并发下的缓存与+数据库双写不一致问题分析与设计马上开始去开发业务系统 从哪一步开始做，从比较简单的那一块开始做，实时性要求比较高的那块数据的缓存去做 实时性比较高的数据缓存，选择的就是库存的服务 库存可能会修改，每次修改都要去更新这个缓存数据; 每次库存的数据，在缓存中一旦过期，或者是被清理掉了，前端的nginx服务都会发送请求给库存服务，去获取相应的数据 库存这一块，写数据库的时候，直接更新redis缓存 实际上没有这么的简单，这里，其实就涉及到了一个问题，数据库与缓存双写，数据不一致的问题 围绕和结合实时性较高的库存服务，把数据库与缓存双写不一致问题以及其解决方案，给大家讲解一下 数据库与缓存双写不一致，很常见的问题，大型的缓存架构中，第一个解决方案 大型的缓存架构全部讲解完了以后，整套架构是非常复杂，架构可以应对各种各样奇葩和极端的情况 也有一种可能，不是说，来讲课的就是超人，万能的 讲课，就跟写书一样，很可能会写错，也可能有些方案里的一些地方，我没考虑到 也可能说，有些方案只是适合某些场景，在某些场景下，可能需要你进行方案的优化和调整才能适用于你自己的项目 大家觉得对这些方案有什么疑问或者见解，都可以找我，沟通一下 如果的确我觉得是我讲解的不对，或者有些地方考虑不周，那么我可以在视频里补录，更新到网站上面去 多多包涵 1、最初级的缓存不一致问题以及解决方案 问题：先修改数据库，再删除缓存，如果删除缓存失败了，那么会导致数据库中是新数据，缓存中是旧数据，数据出现不一致 解决思路 先删除缓存，再修改数据库，如果删除缓存成功了，如果修改数据库失败了，那么数据库中是旧数据，缓存中是空的，那么数据不会不一致 因为读的时候缓存没有，则读数据库中旧数据，然后更新到缓存中 2、比较复杂的数据不一致问题分析 数据发生了变更，先删除了缓存，然后要去修改数据库，此时还没修改 一个请求过来，去读缓存，发现缓存空了，去查询数据库，查到了修改前的旧数据，放到了缓存中 数据变更的程序完成了数据库的修改 完了，数据库和缓存中的数据不一样了。。。。 3、为什么上亿流量高并发场景下，缓存会出现这个问题？ 只有在对一个数据在并发的进行读写的时候，才可能会出现这种问题 其实如果说你的并发量很低的话，特别是读并发很低，每天访问量就1万次，那么很少的情况下，会出现刚才描述的那种不一致的场景 但是问题是，如果每天的是上亿的流量，每秒并发读是几万，每秒只要有数据更新的请求，就可能会出现上述的数据库+缓存不一致的情况 高并发了以后，问题是很多的 4、数据库与缓存更新与读取操作进行异步串行化 更新数据的时候，根据数据的唯一标识，将操作路由之后，发送到一个jvm内部的队列中 读取数据的时候，如果发现数据不在缓存中，那么将重新读取数据+更新缓存的操作，根据唯一标识路由之后，也发送同一个jvm内部的队列中 一个队列对应一个工作线程 每个工作线程串行拿到对应的操作，然后一条一条的执行 这样的话，一个数据变更的操作，先执行，删除缓存，然后再去更新数据库，但是还没完成更新 此时如果一个读请求过来，读到了空的缓存，那么可以先将缓存更新的请求发送到队列中，此时会在队列中积压，然后同步等待缓存更新完成 这里有一个优化点，一个队列中，其实多个更新缓存请求串在一起是没意义的，因此可以做过滤，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新请求操作进去了，直接等待前面的更新操作请求完成即可 待那个队列对应的工作线程完成了上一个操作的数据库的修改之后，才会去执行下一个操作，也就是缓存更新的操作，此时会从数据库中读取最新的值，然后写入缓存中 如果请求还在等待时间范围内，不断轮询发现可以取到值了，那么就直接返回; 如果请求等待的时间超过一定时长，那么这一次直接从数据库中读取当前的旧值 5、高并发的场景下，该解决方案要注意的问题 （1）读请求长时阻塞 由于读请求进行了非常轻度的异步化，所以一定要注意读超时的问题，每个读请求必须在超时时间范围内返回 该解决方案，最大的风险点在于说，可能数据更新很频繁，导致队列中积压了大量更新操作在里面，然后读请求会发生大量的超时，最后导致大量的请求直接走数据库 务必通过一些模拟真实的测试，看看更新数据的频繁是怎样的 另外一点，因为一个队列中，可能会积压针对多个数据项的更新操作，因此需要根据自己的业务情况进行测试，可能需要部署多个服务，每个服务分摊一些数据的更新操作 如果一个内存队列里居然会挤压100个商品的库存修改操作，每隔库存修改操作要耗费10ms区完成，那么最后一个商品的读请求，可能等待10 * 100 = 1000ms = 1s后，才能得到数据 这个时候就导致读请求的长时阻塞 一定要做根据实际业务系统的运行情况，去进行一些压力测试，和模拟线上环境，去看看最繁忙的时候，内存队列可能会挤压多少更新操作，可能会导致最后一个更新操作对应的读请求，会hang多少时间，如果读请求在200ms返回，如果你计算过后，哪怕是最繁忙的时候，积压10个更新操作，最多等待200ms，那还可以的 如果一个内存队列可能积压的更新操作特别多，那么你就要加机器，让每个机器上部署的服务实例处理更少的数据，那么每个内存队列中积压的更新操作就会越少 其实根据之前的项目经验，一般来说数据的写频率是很低的，因此实际上正常来说，在队列中积压的更新操作应该是很少的 针对读高并发，读缓存架构的项目，一般写请求相对读来说，是非常非常少的，每秒的QPS能到几百就不错了 一秒，500的写操作，5份，每200ms，就100个写操作 单机器，20个内存队列，每个内存队列，可能就积压5个写操作，每个写操作性能测试后，一般在20ms左右就完成 那么针对每个内存队列中的数据的读请求，也就最多hang一会儿，200ms以内肯定能返回了 写QPS扩大10倍，但是经过刚才的测算，就知道，单机支撑写QPS几百没问题，那么就扩容机器，扩容10倍的机器，10台机器，每个机器20个队列，200个队列 大部分的情况下，应该是这样的，大量的读请求过来，都是直接走缓存取到数据的 少量情况下，可能遇到读跟数据更新冲突的情况，如上所述，那么此时更新操作如果先入队列，之后可能会瞬间来了对这个数据大量的读请求，但是因为做了去重的优化，所以也就一个更新缓存的操作跟在它后面 等数据更新完了，读请求触发的缓存更新操作也完成，然后临时等待的读请求全部可以读到缓存中的数据 （2）读请求并发量过高 这里还必须做好压力测试，确保恰巧碰上上述情况的时候，还有一个风险，就是突然间大量读请求会在几十毫秒的延时hang在服务上，看服务能不能抗的住，需要多少机器才能抗住最大的极限情况的峰值 但是因为并不是所有的数据都在同一时间更新，缓存也不会同一时间失效，所以每次可能也就是少数数据的缓存失效了，然后那些数据对应的读请求过来，并发量应该也不会特别大 按1:99的比例计算读和写的请求，每秒5万的读QPS，可能只有500次更新操作 如果一秒有500的写QPS，那么要测算好，可能写操作影响的数据有500条，这500条数据在缓存中失效后，可能导致多少读请求，发送读请求到库存服务来，要求更新缓存 一般来说，1:1，1:2，1:3，每秒钟有1000个读请求，会hang在库存服务上，每个读请求最多hang多少时间，200ms就会返回 在同一时间最多hang住的可能也就是单机200个读请求，同时hang住 单机hang200个读请求，还是ok的 1:20，每秒更新500条数据，这500秒数据对应的读请求，会有20 * 500 = 1万 1万个读请求全部hang在库存服务上，就死定了 （3）多服务实例部署的请求路由 可能这个服务部署了多个实例，那么必须保证说，执行数据更新操作，以及执行缓存更新操作的请求，都通过nginx服务器路由到相同的服务实例上 （4）热点商品的路由问题，导致请求的倾斜 万一某个商品的读写请求特别高，全部打到相同的机器的相同的队列里面去了，可能造成某台机器的压力过大 就是说，因为只有在商品数据更新的时候才会清空缓存，然后才会导致读写并发，所以更新频率不是太高的话，这个问题的影响并不是特别大 但是的确可能某些机器的负载会高一些 在库存服务中实现缓存与数据库双写一致性保障方案更新数据的时候，根据数据的唯一标识，将操作路由之后，发送到一个jvm内部的队列中 读取数据的时候，如果发现数据 不在缓存中，那么将重新读取数据+更新缓存的操作，根据唯一标识路由之后，也发送同一个jvm内部的队列中 一个队列对应一个工作线程 每个工作线程串行拿到对应的操作，然后一条一条的执行 这样的话，一个数据变更的操作，先执行，删除缓存，然后再去更新数据库，但是还没完成更新 此时如果一个读请求过来，读到了空的缓存，那么可以先将缓存更新的请求发送到队列中，此时会在队列中积压，然后同步等待缓存更新完成 这里有一个优化点，一个队列中，其实多个更新缓存请求串在一起是没意义的，因此可以做过滤，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新请求操作进去了，直接等待前面的更新操作请求完成即可 待那个队列对应的工作线程完成了上一个操作的数据库的修改之后，才会去执行下一个操作，也就是缓存更新的操作，此时会从数据库中读取最新的值，然后写入缓存中 如果请求还在等待时间范围内，不断轮询发现可以取到值了，那么就直接返回; 如果请求等待的时间超过一定时长，那么这一次直接从数据库中读取当前的旧值 int h;return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); (queueNum - 1) &amp; hash 1、线程池+内存队列 化 @Beanpublic ServletListenerRegistrationBean servletListenerRegistrationBean(){ ServletListenerRegistrationBean servletListenerRegistrationBean = new ServletListenerRegistrationBean(); servletListenerRegistrationBean.setListener(new InitListener()); return servletListenerRegistrationBean;} java web应用，做系统的初始化，一般在哪里做呢？ ServletContextListener里面做，listener，会跟着整个web应用的启动，就初始化，类似于线程池初始化的构建 使用单例初始化线程池，基于静态内部类（静态代码块）初始化线程池的方式， spring boot应用，Application，搞一个listener的注册 2、两种请求对象封装 3、请求异步执行Service封装 4、两种请求Controller接口封装 5、读请求去重优化 6、空数据读请求过滤优化 队列 对一个商品的库存的数据库更新操作已经在内存队列中了 然后对这个商品的库存的读取操作，要求读取数据库的库存数据，然后更新到缓存中，多个读 这多个读，其实只要有一个读请求操作压到队列里就可以了 其他的读操作，全部都wait那个读请求的操作，刷新缓存，就可以读到缓存中的最新数据了 如果读请求发现redis缓存中没有数据，就会发送读请求给库存服务，但是此时缓存中为空，可能是因为写请求先删除了缓存，也可能是数据库里压根儿没这条数据 如果是数据库中压根儿没这条数据的场景，那么就不应该将读请求操作给压入队列中，而是直接返回空就可以了 都是为了减少内存队列中的请求积压，内存队列中积压的请求越多，就可能导致每个读请求hang住的时间越长，也可能导致多个读请求被hang住]]></content>
      <tags>
        <tag>项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[扩展]]></title>
    <url>%2F2019%2F09%2F29%2F%E6%89%A9%E5%B1%95%2F</url>
    <content type="text"><![CDATA[9.28在订单详情部分实现多缓存处理 redis 分布式缓存 + tomcat 堆缓存 二级缓存架构 redis搭建方案12345678redis持久化+备份方案+容灾方案+replication（主从+读写分离）+sentinal（哨兵集群，3个节点，高可用性）可以支撑的数据量在10G以内，可以支撑的写QPS在几万左右，可以支撑的读QPS可以上10万以上（随你的需求，水平扩容slave节点就可以），可用性在99.99%可以用公司里的一些已有的数据，导入进去，几百万，一千万，进去做各种压力测试，性能，redis-benchmark，并发，QPS，高可用的演练，每台机器最大能存储多少数据量，横向扩容支撑更多数据基于测试环境还有测试数据，做各种演练，去摸索一些最适合自己的一些细节的东西 如果页面的数据有变更，及时监听到，并且写入缓存中，提供高并发，高性能的访问 前端请求，请求服务器，先从redis中拿，redis中没有，本地搭建ehcache没有，再调用商品服务，从数据库中拿 企业级的大型缓存架构 小程序写入操作，防止并发中，HashMap的使用，改成使用ConcurrentHashMap 要掌握的很好的，就是redis架构 安装一个虚拟机集群 安装redis集群 配置持久化 RDB 快照手动设置检查点（etc/redis/redis.conf 里面可以修改save） 高并发，高可用，海量数据，备份，随时可以恢复， redis架构，每秒钟几十万的访问量QPS，99.99%的高可用性，TB级的海量数据，备份和恢复，缓存架构就成功了一半。最最简单的模式，无非就是存取redis，存数据，取数据。解决各种各样的高并发下缓存面临的难题，缓存架构中不断引入各种解决方案和技术，解决高并发的问题。 搭建redis集群，从0开始，一步一步搭建一个4个结点的Centos集群。安装4台虚拟机 在hosts文件下,配置好所有的机器的ip地址到hostname的映射关系 使用ssh配置每台机器之间免密登录 make install 要把redis作为一个系统的daemon进程去运行，每次系统启动，redis进程一起启动 如果没有持久化的话，redis遇到灾难性故障的时候，就会丢失所有的数据 如果通过持久化将数据搞一份儿在磁盘上去，然后定期比如说同步和备份到一些云存储服务上去，那么就可以保证数据不丢失全部，还是可以恢复一部分数据回来的。redis如果单单把数据放在内存中，是没有任何方法应对一些灾难性的工作的，redis 在启动会自动从磁盘中恢复数据到内存中。 redis 持久化RDB,AOF利弊比较比如你redis整个挂了，然后redis就不可用了，你要做的事情是让redis变得可用，尽快变得可用 重启redis，尽快让它对外提供服务，但是就像上一讲说，如果你没做数据备份，这个时候redis启动了，也不可用啊，数据都没了 很可能说，大量的请求过来，缓存全部无法命中，在redis里根本找不到数据，这个时候就死定了，缓存雪崩问题，所有请求，没有在redis命中，就会去mysql数据库这种数据源头中去找，一下子mysql承接高并发，然后就挂了 mysql挂掉，你都没法去找数据恢复到redis里面去，redis的数据从哪儿来？从mysql来。。。 具体的完整的缓存雪崩的场景，还有企业级的解决方案，到后面讲 如果你把redis的持久化做好，备份和恢复方案做到企业级的程度，那么即使你的redis故障了，也可以通过备份数据，快速恢复，一旦恢复立即对外提供服务 redis的持久化，跟高可用，是有关系的，企业级redis架构中去讲解 redis持久化：RDB，AOF 1、RDB和AOF两种持久化机制的介绍 RDB持久化机制，对redis中的数据执行周期性的持久化 AOF机制对每条写入命令作为日志，以append-only的模式写入一个日志文件中，在redis重启的时候，可以通过回放AOF日志中的写入指令来重新构建整个数据集 如果我们想要redis仅仅作为纯内存的缓存来用，那么可以禁止RDB和AOF所有的持久化机制 通过RDB或AOF，都可以将redis内存中的数据给持久化到磁盘上面来，然后可以将这些数据备份到别的地方去，比如说阿里云，云服务 如果redis挂了，服务器上的内存和磁盘上的数据都丢了，可以从云服务上拷贝回来之前的数据，放到指定的目录中，然后重新启动redis，redis就会自动根据持久化数据文件中的数据，去恢复内存中的数据，继续对外提供服务 如果同时使用RDB和AOF两种持久化机制，那么在redis重启的时候，会使用AOF来重新构建数据，因为AOF中的数据更加完整 2、RDB持久化机制的优点 （1）RDB会生成多个数据文件，每个数据文件都代表了某一个时刻中redis的数据，这种多个数据文件的方式，非常适合做冷备，可以将这种完整的数据文件发送到一些远程的安全存储上去，比如说Amazon的S3云服务上去，在国内可以是阿里云的ODPS分布式存储上，以预定好的备份策略来定期备份redis中的数据 （2）RDB对redis对外提供的读写服务，影响非常小，可以让redis保持高性能，因为redis主进程只需要fork一个子进程，让子进程执行磁盘IO操作来进行RDB持久化即可 （3）相对于AOF持久化机制来说，直接基于RDB数据文件来重启和恢复redis进程，更加快速 3、RDB持久化机制的缺点 （1）如果想要在redis故障时，尽可能少的丢失数据，那么RDB没有AOF好。一般来说，RDB数据快照文件，都是每隔5分钟，或者更长时间生成一次，这个时候就得接受一旦redis进程宕机，那么会丢失最近5分钟的数据 （2）RDB每次在fork子进程来执行RDB快照数据文件生成的时候，如果数据文件特别大，可能会导致对客户端提供的服务暂停数毫秒，或者甚至数秒 4、AOF持久化机制的优点 （1）AOF可以更好的保护数据不丢失，一般AOF会每隔1秒，通过一个后台线程执行一次fsync操作，最多丢失1秒钟的数据 （2）AOF日志文件以append-only模式写入，所以没有任何磁盘寻址的开销，写入性能非常高，而且文件不容易破损，即使文件尾部破损，也很容易修复 （3）AOF日志文件即使过大的时候，出现后台重写操作，也不会影响客户端的读写。因为在rewrite log的时候，会对其中的指导进行压缩，创建出一份需要恢复数据的最小日志出来。再创建新日志文件的时候，老的日志文件还是照常写入。当新的merge后的日志文件ready的时候，再交换新老日志文件即可。 （4）AOF日志文件的命令通过非常可读的方式进行记录，这个特性非常适合做灾难性的误删除的紧急恢复。比如某人不小心用flushall命令清空了所有数据，只要这个时候后台rewrite还没有发生，那么就可以立即拷贝AOF文件，将最后一条flushall命令给删了，然后再将该AOF文件放回去，就可以通过恢复机制，自动恢复所有数据 5、AOF持久化机制的缺点 （1）对于同一份数据来说，AOF日志文件通常比RDB数据快照文件更大 （2）AOF开启后，支持的写QPS会比RDB支持的写QPS低，因为AOF一般会配置成每秒fsync一次日志文件，当然，每秒一次fsync，性能也还是很高的 （3）以前AOF发生过bug，就是通过AOF记录的日志，进行数据恢复的时候，没有恢复一模一样的数据出来。所以说，类似AOF这种较为复杂的基于命令日志/merge/回放的方式，比基于RDB每次持久化一份完整的数据快照文件的方式，更加脆弱一些，容易有bug。不过AOF就是为了避免rewrite过程导致的bug，因此每次rewrite并不是基于旧的指令日志进行merge的，而是基于当时内存中的数据进行指令的重新构建，这样健壮性会好很多。 6、RDB和AOF到底该如何选择 （1）不要仅仅使用RDB，因为那样会导致你丢失很多数据 （2）也不要仅仅使用AOF，因为那样有两个问题，第一，你通过AOF做冷备，没有RDB做冷备，来的恢复速度更快; 第二，RDB每次简单粗暴生成数据快照，更加健壮，可以避免AOF这种复杂的备份和恢复机制的bug （3）综合使用AOF和RDB两种持久化机制，用AOF来保证数据不丢失，作为数据恢复的第一选择; 用RDB来做不同程度的冷备，在AOF文件都丢失或损坏不可用的时候，还可以使用RDB来进行快速的数据恢复 RDB持久化详解1、如何配置RDB持久化机制2、RDB持久化机制的工作流程3、基于RDB持久化机制的数据恢复实验 1、如何配置RDB持久化机制 redis.conf文件，也就是/etc/redis/6379.conf，去配置持久化 save 60 1000 每隔60s，如果有超过1000个key发生了变更，那么就生成一个新的dump.rdb文件，就是当前redis内存中完整的数据快照，这个操作也被称之为snapshotting，快照 也可以手动调用save或者bgsave命令，同步或异步执行rdb快照生成 save可以设置多个，就是多个snapshotting检查点，每到一个检查点，就会去check一下，是否有指定的key数量发生了变更，如果有，就生成一个新的dump.rdb文件 2、RDB持久化机制的工作流程 （1）redis根据配置自己尝试去生成rdb快照文件（2）fork一个子进程出来（3）子进程尝试将数据dump到临时的rdb快照文件中（4）完成rdb快照文件的生成之后，就替换之前的旧的快照文件 dump.rdb，每次生成一个新的快照，都会覆盖之前的老快照 3、基于RDB持久化机制的数据恢复实验 （1）在redis中保存几条数据，立即停掉redis进程，然后重启redis，看看刚才插入的数据还在不在 数据还在，为什么？ 带出来一个知识点，通过redis-cli SHUTDOWN这种方式去停掉redis，其实是一种安全退出的模式，redis在退出的时候会将内存中的数据立即生成一份完整的rdb快照 /var/redis/6379/dump.rdb （2）在redis中再保存几条新的数据，用kill -9粗暴杀死redis进程，模拟redis故障异常退出，导致内存数据丢失的场景 这次就发现，redis进程异常被杀掉，数据没有进dump文件，几条最新的数据就丢失了 （2）手动设置一个save检查点，save 5 1（3）写入几条数据，等待5秒钟，会发现自动进行了一次dump rdb快照，在dump.rdb中发现了数据（4）异常停掉redis进程，再重新启动redis，看刚才插入的数据还在 rdb的手动配置检查点，以及rdb快照的生成，包括数据的丢失和恢复，全都演示过了 AOF持久化详解1、AOF持久化的配置2、AOF持久化的数据恢复实验3、AOF rewrite4、AOF破损文件的修复5、AOF和RDB同时工作 1、AOF持久化的配置 AOF持久化，默认是关闭的，默认是打开RDB持久化 appendonly yes，可以打开AOF持久化机制，在生产环境里面，一般来说AOF都是要打开的，除非你说随便丢个几分钟的数据也无所谓。如果开启AOF，就算没有AOF文件，redis在重启时，也会创建一个新的空的AOF文件恢复数据，在这个时候就需要先关闭AOF，拷贝dump.rdb恢复redis先，接着应该直接在命令行热修改redis配置，打开AOF。此时磁盘上的配置文件还是no（关闭）的，还需要在磁盘上将AOF打开。 AOF append-only ，顺序写入，如果AOF文件破损，那么用redis-check-aof fix修复文件打开AOF持久化机制之后，redis每次接收到一条写命令，就会写入日志文件中，当然是先写入os cache的，然后每隔一定时间再fsync一下 而且即使AOF和RDB都开启了，redis重启的时候，也是优先通过AOF进行数据恢复的，因为aof数据比较完整 可以配置AOF的fsync策略，有三种策略可以选择，一种是每次写入一条数据就执行一次fsync; 一种是每隔一秒执行一次fsync; 一种是不主动执行fsync always: 每次写入一条数据，立即将这个数据对应的写日志fsync到磁盘上去，性能非常非常差，吞吐量很低; 确保说redis里的数据一条都不丢，那就只能这样了 mysql -&gt; 内存策略，大量磁盘，QPS到多少，一两k。QPS，每秒钟的请求数量redis -&gt; 内存，磁盘持久化，QPS到多少，单机，一般来说，上万QPS没问题 everysec: 每秒将os cache中的数据fsync到磁盘，这个最常用的，生产环境一般都这么配置，性能很高，QPS还是可以上万的 no: 仅仅redis负责将数据写入os cache就撒手不管了，然后后面os自己会时不时有自己的策略将数据刷入磁盘，不可控了 2、AOF持久化的数据恢复实验 （1）先仅仅打开RDB，写入一些数据，然后kill -9杀掉redis进程，接着重启redis，发现数据没了，因为RDB快照还没生成（2）打开AOF的开关，启用AOF持久化（3）写入一些数据，观察AOF文件中的日志内容 其实你在appendonly.aof文件中，可以看到刚写的日志，它们其实就是先写入os cache的，然后1秒后才fsync到磁盘中，只有fsync到磁盘中了，才是安全的，要不然光是在os cache中，机器只要重启，就什么都没了 （4）kill -9杀掉redis进程，重新启动redis进程，发现数据被恢复回来了，就是从AOF文件中恢复回来的 redis进程启动的时候，直接就会从appendonly.aof中加载所有的日志，把内存中的数据恢复回来 3、AOF rewrite redis中的数据其实有限的，很多数据可能会自动过期，可能会被用户删除，可能会被redis用缓存清除的算法清理掉 redis中的数据会不断淘汰掉旧的，就一部分常用的数据会被自动保留在redis内存中 所以可能很多之前的已经被清理掉的数据，对应的写日志还停留在AOF中，AOF日志文件就一个，会不断的膨胀，到很大很大 所以AOF会自动在后台每隔一定时间做rewrite操作，比如日志里已经存放了针对100w数据的写日志了; redis内存只剩下10万; 基于内存中当前的10万数据构建一套最新的日志，到AOF中; 覆盖之前的老日志; 确保AOF日志文件不会过大，保持跟redis内存数据量一致 redis 2.4之前，还需要手动，开发一些脚本，crontab，通过BGREWRITEAOF命令去执行AOF rewrite，但是redis 2.4之后，会自动进行rewrite操作 在redis.conf中，可以配置rewrite策略 auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb 比如说上一次AOF rewrite之后，是128mb 然后就会接着128mb继续写AOF的日志，如果发现增长的比例，超过了之前的100%，256mb，就可能会去触发一次rewrite 但是此时还要去跟min-size，64mb去比较，256mb &gt; 64mb，才会去触发rewrite （1）redis fork一个子进程（2）子进程基于当前内存中的数据，构建日志，开始往一个新的临时的AOF文件中写入日志（3）redis主进程，接收到client新的写操作之后，在内存中写入日志，同时新的日志也继续写入旧的AOF文件（4）子进程写完新的日志文件之后，redis主进程将内存中的新日志再次追加到新的AOF文件中（5）用新的日志文件替换掉旧的日志文件 4、AOF破损文件的修复 如果redis在append数据到AOF文件时，机器宕机了，可能会导致AOF文件破损 用redis-check-aof –fix命令来修复破损的AOF文件 5、AOF和RDB同时工作 （1）如果RDB在执行snapshotting操作，那么redis不会执行AOF rewrite; 如果redis再执行AOF rewrite，那么就不会执行RDB snapshotting（2）如果RDB在执行snapshotting，此时用户执行BGREWRITEAOF命令，那么等RDB快照生成之后，才会去执行AOF rewrite（3）同时有RDB snapshot文件和AOF日志文件，那么redis重启的时候，会优先使用AOF进行数据恢复，因为其中的日志更完整 6、最后一个小实验，让大家对redis的数据恢复有更加深刻的体会 （1）在有rdb的dump和aof的appendonly的同时，rdb里也有部分数据，aof里也有部分数据，这个时候其实会发现，rdb的数据不会恢复到内存中（2）我们模拟让aof破损，然后fix，有一条数据会被fix删除（3）再次用fix得aof文件去重启redis，发现数据只剩下一条了 数据恢复完全是依赖于底层的磁盘的持久化的，主要rdb和aof上都没有数据，那就没了 9.29redis在企业级数据备份方案以及数据恢复负灾演练到这里为止，其实还是停留在简单学习知识的程度，学会了redis的持久化的原理和操作，但是在企业中，持久化到底是怎么去用得呢？ 企业级的数据备份和各种灾难下的数据恢复，是怎么做得呢？ 1、企业级的持久化的配置策略 在企业中，RDB的生成策略，用默认的也差不多 save 60 10000：如果你希望尽可能确保说，RDB最多丢1分钟的数据，那么尽量就是每隔1分钟都生成一个快照，低峰期，数据量很少，也没必要 10000-&gt;生成RDB，1000-&gt;RDB，这个根据你自己的应用和业务的数据量，你自己去决定 AOF一定要打开，fsync，everysec auto-aof-rewrite-percentage 100: 就是当前AOF大小膨胀到超过上次100%，上次的两倍auto-aof-rewrite-min-size 64mb: 根据你的数据量来定，16mb，32mb 2、企业级的数据备份方案 RDB非常适合做冷备，每次生成之后，就不会再有修改了 数据备份方案 （1）写crontab定时调度脚本去做数据备份（2）每小时都copy一份rdb的备份，到一个目录中去，仅仅保留最近48小时的备份（3）每天都保留一份当日的rdb的备份，到一个目录中去，仅仅保留最近1个月的备份（4）每次copy备份的时候，都把太旧的备份给删了（5）每天晚上将当前服务器上所有的数据备份，发送一份到远程的云服务上去 /usr/local/redis 每小时copy一次备份，删除48小时前的数据 crontab -e 0 sh /usr/local/redis/copy/redis_rdb_copy_hourly.sh redis_rdb_copy_hourly.sh #!/bin/sh cur_date=date +%Y%m%d%krm -rf /usr/local/redis/snapshotting/$cur_datemkdir /usr/local/redis/snapshotting/$cur_datecp /var/redis/6379/dump.rdb /usr/local/redis/snapshotting/$cur_date del_date=date -d -48hour +%Y%m%d%krm -rf /usr/local/redis/snapshotting/$del_date 每天copy一次备份 crontab -e 0 0 * sh /usr/local/redis/copy/redis_rdb_copy_daily.sh redis_rdb_copy_daily.sh #!/bin/sh cur_date=date +%Y%m%drm -rf /usr/local/redis/snapshotting/$cur_datemkdir /usr/local/redis/snapshotting/$cur_datecp /var/redis/6379/dump.rdb /usr/local/redis/snapshotting/$cur_date del_date=date -d -1month +%Y%m%drm -rf /usr/local/redis/snapshotting/$del_date 每天一次将所有数据上传一次到远程的云服务器上去 3、数据恢复方案 （1）如果是redis进程挂掉，那么重启redis进程即可，直接基于AOF日志文件恢复数据 不演示了，在AOF数据恢复那一块，演示了，fsync everysec，最多就丢一秒的数 （2）如果是redis进程所在机器挂掉，那么重启机器后，尝试重启redis进程，尝试直接基于AOF日志文件进行数据恢复 AOF没有破损，也是可以直接基于AOF恢复的 AOF append-only，顺序写入，如果AOF文件破损，那么用redis-check-aof fix （3）如果redis当前最新的AOF和RDB文件出现了丢失/损坏，那么可以尝试基于该机器上当前的某个最新的RDB数据副本进行数据恢复 当前最新的AOF和RDB文件都出现了丢失/损坏到无法恢复，一般不是机器的故障，人为 大数据系统，hadoop，有人不小心就把hadoop中存储的大量的数据文件对应的目录，rm -rf一下，我朋友的一个小公司，运维不太靠谱，权限也弄的不太好 /var/redis/6379下的文件给删除了 找到RDB最新的一份备份，小时级的备份可以了，小时级的肯定是最新的，copy到redis里面去，就可以恢复到某一个小时的数据 容灾演练 我跟大家解释一下，我其实上课，为什么大量的讲师可能讲课就是纯PPT，或者是各种复制粘贴，都不是现场讲解和写代码演示的 很容易出错，为了避免出错，一般就会那样玩儿 吐槽，念PPT，效果很差 真实的，备课，讲课不可避免，会出现一些问题，但是我觉得还好，真实 appendonly.aof + dump.rdb，优先用appendonly.aof去恢复数据，但是我们发现redis自动生成的appendonly.aof是没有数据的 然后我们自己的dump.rdb是有数据的，但是明显没用我们的数据 redis启动的时候，自动重新基于内存的数据，生成了一份最新的rdb快照，直接用空的数据，覆盖掉了我们有数据的，拷贝过去的那份dump.rdb 你停止redis之后，其实应该先删除appendonly.aof，然后将我们的dump.rdb拷贝过去，然后再重启redis 很简单，就是虽然你删除了appendonly.aof，但是因为打开了aof持久化，redis就一定会优先基于aof去恢复，即使文件不在，那就创建一个新的空的aof文件 停止redis，暂时在配置中关闭aof，然后拷贝一份rdb过来，再重启redis，数据能不能恢复过来，可以恢复过来 脑子一热，再关掉redis，手动修改配置文件，打开aof，再重启redis，数据又没了，空的aof文件，所有数据又没了 在数据安全丢失的情况下，基于rdb冷备，如何完美的恢复数据，同时还保持aof和rdb的双开 停止redis，关闭aof，拷贝rdb备份，重启redis，确认数据恢复，直接在命令行热修改redis配置，打开aof，这个redis就会将内存中的数据对应的日志，写入aof文件中 此时aof和rdb两份数据文件的数据就同步了 redis config set热修改配置参数，可能配置文件中的实际的参数没有被持久化的修改，再次停止redis，手动修改配置文件，打开aof的命令，再次重启redis （4）如果当前机器上的所有RDB文件全部损坏，那么从远程的云服务上拉取最新的RDB快照回来恢复数据 （5）如果是发现有重大的数据错误，比如某个小时上线的程序一下子将数据全部污染了，数据全错了，那么可以选择某个更早的时间点，对数据进行恢复 举个例子，12点上线了代码，发现代码有bug，导致代码生成的所有的缓存数据，写入redis，全部错了 找到一份11点的rdb的冷备，然后按照上面的步骤，去恢复到11点的数据，不就可以了吗 redis通过主从架构实现读写分离，完成10万+QPS1、redis高并发跟整个系统的高并发之间的关系 redis，你要搞高并发的话，不可避免，要把底层的缓存搞得很好 mysql，高并发，做到了，那么也是通过一系列复杂的分库分表，订单系统，事务要求的，QPS到几万，比较高了 要做一些电商的商品详情页，真正的超高并发，QPS上十万，甚至是百万，一秒钟百万的请求量 光是redis是不够的，但是redis是整个大型的缓存架构中，支撑高并发的架构里面，非常重要的一个环节 首先，你的底层的缓存中间件，缓存系统，必须能够支撑的起我们说的那种高并发，其次，再经过良好的整体的缓存架构的设计（多级缓存架构、热点缓存），支撑真正的上十万，甚至上百万的高并发 2、redis不能支撑高并发的瓶颈在哪里？ 单机 3、如果redis要支撑超过10万+的并发，那应该怎么做？ 单机的redis几乎不太可能说QPS超过10万+，除非一些特殊情况，比如你的机器性能特别好，配置特别高，物理机，维护做的特别好，而且你的整体的操作不是太复杂 单机在几万 读写分离，一般来说，对缓存，一般都是用来支撑读高并发的，写的请求是比较少的，可能写请求也就一秒钟几千，一两千 大量的请求都是读，一秒钟二十万次读 读写分离 主从架构 -&gt; 读写分离 -&gt; 支撑10万+读QPS的架构 4、接下来要讲解的一个topic redis replication redis主从架构 -&gt; 读写分离架构 -&gt; 可支持水平扩展的读高并发架构 redis replication（主从架构）基本原理课程大纲 1、图解redis replication基本原理2、redis replication的核心机制3、master持久化对于主从架构的安全保障的意义 redis replication -&gt; 主从架构 -&gt; 读写分离 -&gt; 水平扩容支撑读高并发 redis replication的最最基本的原理，铺垫 1、图解redis replication基本原理 2、redis replication的核心机制 （1）redis采用异步方式复制数据到slave节点，不过redis 2.8开始，slave node会周期性地确认自己每次复制的数据量（2）一个master node是可以配置多个slave node的（3）slave node也可以连接其他的slave node（4）slave node做复制的时候，是不会block master node的正常工作的（5）slave node在做复制的时候，也不会block对自己的查询操作，它会用旧的数据集来提供服务; 但是复制完成的时候，需要删除旧数据集，加载新数据集，这个时候就会暂停对外服务了（6）slave node主要用来进行横向扩容，做读写分离，扩容的slave node可以提高读的吞吐量 slave，高可用性，有很大的关系 3、master持久化对于主从架构的安全保障的意义 如果采用了主从架构，那么建议必须开启master node的持久化！ 不建议用slave node作为master node的数据热备，因为那样的话，如果你关掉master的持久化，可能在master宕机重启的时候数据是空的，然后可能一经过复制，salve node数据也丢了 master -&gt; RDB和AOF都关闭了 -&gt; 全部在内存中 master宕机，重启，是没有本地数据可以恢复的，然后就会直接认为自己IDE数据是空的 master就会将空的数据集同步到slave上去，所有slave的数据全部清空 100%的数据丢失 master节点，必须要使用持久化机制 第二个，master的各种备份方案，要不要做，万一说本地的所有文件丢失了; 从备份中挑选一份rdb去恢复master; 这样才能确保master启动的时候，是有数据的 即使采用了后续讲解的高可用机制，slave node可以自动接管master node，但是也可能sentinal还没有检测到master failure，master node就自动重启了，还是可能导致上面的所有slave node数据清空故障 9.30redis主从复制原理细讲1、复制的完整流程 （1）slave node启动，仅仅保存master node的信息，包括master node的host和ip，但是复制流程没开始 master host和ip是从哪儿来的，redis.conf里面的slaveof配置的 （2）slave node内部有个定时任务，每秒检查是否有新的master node要连接和复制，如果发现，就跟master node建立socket网络连接（3）slave node发送ping命令给master node（4）口令认证，如果master设置了requirepass，那么salve node必须发送masterauth的口令过去进行认证（5）master node第一次执行全量复制，将所有数据发给slave node（6）master node后续持续将写命令，异步复制给slave node 2、数据同步相关的核心机制 指的就是第一次slave连接msater的时候，执行的全量复制，那个过程里面你的一些细节的机制 （1）master和slave都会维护一个offset master会在自身不断累加offset，slave也会在自身不断累加offsetslave每秒都会上报自己的offset给master，同时master也会保存每个slave的offset 这个倒不是说特定就用在全量复制的，主要是master和slave都要知道各自的数据的offset，才能知道互相之间的数据不一致的情况 （2）backlog master node有一个backlog，默认是1MB大小master node给slave node复制数据时，也会将数据在backlog中同步写一份backlog主要是用来做全量复制中断候的增量复制的 （3）master run id info server，可以看到master run id如果根据host+ip定位master node，是不靠谱的，如果master node重启或者数据出现了变化，那么slave node应该根据不同的run id区分，run id不同就做全量复制如果需要不更改run id重启redis，可以使用redis-cli debug reload命令 （4）psync 从节点使用psync从master node进行复制，psync runid offsetmaster node会根据自身的情况返回响应信息，可能是FULLRESYNC runid offset触发全量复制，可能是CONTINUE触发增量复制 3、全量复制 （1）master执行bgsave，在本地生成一份rdb快照文件（2）master node将rdb快照文件发送给salve node，如果rdb复制时间超过60秒（repl-timeout），那么slave node就会认为复制失败，可以适当调节大这个参数（3）对于千兆网卡的机器，一般每秒传输100MB，6G文件，很可能超过60s（4）master node在生成rdb时，会将所有新的写命令缓存在内存中，在salve node保存了rdb之后，再将新的写命令复制给salve node（5）client-output-buffer-limit slave 256MB 64MB 60，如果在复制期间，内存缓冲区持续消耗超过64MB，或者一次性超过256MB，那么停止复制，复制失败（6）slave node接收到rdb之后，清空自己的旧数据，然后重新加载rdb到自己的内存中，同时基于旧的数据版本对外提供服务（7）如果slave node开启了AOF，那么会立即执行BGREWRITEAOF，重写AOF rdb生成、rdb通过网络拷贝、slave旧数据的清理、slave aof rewrite，很耗费时间 如果复制的数据量在4G~6G之间，那么很可能全量复制时间消耗到1分半到2分钟 4、增量复制 （1）如果全量复制过程中，master-slave网络连接断掉，那么salve重新连接master时，会触发增量复制（2）master直接从自己的backlog中获取部分丢失的数据，发送给slave node，默认backlog就是1MB（3）msater就是根据slave发送的psync中的offset来从backlog中获取数据的 5、heartbeat 主从节点互相都会发送heartbeat信息 master默认每隔10秒发送一次heartbeat，salve node每隔1秒发送一个heartbeat 6、异步复制 master每次接收到写命令之后，现在内部写入数据，然后异步发送给slave node 搭建redis主从节点之前几讲都是在铺垫各种redis replication的原理，和知识，主从，读写分离，画图 知道了这些东西，关键是怎么搭建呢？？？ 一主一从，往主节点去写，在从节点去读，可以读到，主从架构就搭建成功了 1、启用复制，部署slave node wget http://downloads.sourceforge.net/tcl/tcl8.6.1-src.tar.gztar -xzvf tcl8.6.1-src.tar.gzcd /usr/local/tcl8.6.1/unix/./configuremake &amp;&amp; make install 使用redis-3.2.8.tar.gz（截止2017年4月的最新稳定版）tar -zxvf redis-3.2.8.tar.gzcd redis-3.2.8make &amp;&amp; make test &amp;&amp; make install （1）redis utils目录下，有个redis_init_script脚本（2）将redis_init_script脚本拷贝到linux的/etc/init.d目录中，将redis_init_script重命名为redis_6379，6379是我们希望这个redis实例监听的端口号（3）修改redis_6379脚本的第6行的REDISPORT，设置为相同的端口号（默认就是6379）（4）创建两个目录：/etc/redis（存放redis的配置文件），/var/redis/6379（存放redis的持久化文件）（5）修改redis配置文件（默认在根目录下，redis.conf），拷贝到/etc/redis目录中，修改名称为6379.conf （6）修改redis.conf中的部分配置为生产环境 daemonize yes 让redis以daemon进程运行pidfile /var/run/redis_6379.pid 设置redis的pid文件位置port 6379 设置redis的监听端口号dir /var/redis/6379 设置持久化文件的存储位置 (设置持久化文件的位置) （7）让redis跟随系统启动自动启动 在redis_6379脚本中，最上面，加入两行注释 chkconfig: 2345 90 10description: Redis is a persistent key-value databasechkconfig redis_6379 on（随着系统启动+） 在slave node上配置（redis.conf文件中只要修改slaveof相关配置）： slaveof 192.168.1.1（主节点） 6379（redis端口号），即可也可以使用slaveof命令 则配置成自己是从节点2、强制读写分离 基于主从复制架构，实现读写分离 redis slave node只读，默认开启，slave-read-only 开启了只读的redis slave node，会拒绝所有的写操作，这样可以强制搭建成读写分离的架构 3、集群安全认证 master上启用安全认证，requirepassmaster连接口令，masterauth 4、读写分离架构的测试 先启动主节点，eshop-cache01上的redis实例再启动从节点，eshop-cache02上的redis实例 刚才我调试了一下，redis slave node一直说没法连接到主节点的6379的端口 在搭建生产环境的集群的时候，不要忘记修改一个配置，bind bind 127.0.0.1 -&gt; 本地的开发调试的模式，就只能127.0.0.1本地才能访问到6379的端口 每个redis.conf中的bind 127.0.0.1 -&gt; bind自己的ip地址在每个节点上都: iptables -A INPUT -ptcp –dport 6379 -j ACCEPT redis-cli -h ipaddrinfo replication 在主上写，在从上读ps -ef | grep redis查看redis进程是否已经启动 哨兵架构基础知识讲解1、哨兵的介绍 sentinal，中文名是哨兵 哨兵是redis集群架构中非常重要的一个组件，主要功能如下 （1）集群监控，负责监控redis master和slave进程是否正常工作（2）消息通知，如果某个redis实例有故障，那么哨兵负责发送消息作为报警通知给管理员（3）故障转移，如果master node挂掉了，会自动转移到slave node上（4）配置中心，如果故障转移发生了，通知client客户端新的master地址 哨兵本身也是分布式的，作为一个哨兵集群去运行，互相协同工作 （1）故障转移时，判断一个master node是宕机了，需要大部分的哨兵都同意才行，涉及到了分布式选举的问题（2）即使部分哨兵节点挂掉了，哨兵集群还是能正常工作的，因为如果一个作为高可用机制重要组成部分的故障转移系统本身是单点的，那就很坑爹了 目前采用的是sentinal 2版本，sentinal 2相对于sentinal 1来说，重写了很多代码，主要是让故障转移的机制和算法变得更加健壮和简单 2、哨兵的核心知识 （1）哨兵至少需要3个实例，来保证自己的健壮性（2）哨兵 + redis主从的部署架构，是不会保证数据零丢失的，只能保证redis集群的高可用性（3）对于哨兵 + redis主从这种复杂的部署架构，尽量在测试环境和生产环境，都进行充足的测试和演练 3、为什么redis哨兵集群只有2个节点无法正常工作？ 哨兵集群必须部署2个以上节点 如果哨兵集群仅仅部署了个2个哨兵实例，quorum=1 +—-+ +—-+| M1 |———| R1 || S1 | | S2 |+—-+ +—-+ Configuration: quorum = 1 master宕机，s1和s2中只要有1个哨兵认为master宕机就可以还行切换，同时s1和s2中会选举出一个哨兵来执行故障转移 同时这个时候，需要majority，也就是大多数哨兵都是运行的，2个哨兵的majority就是2（2的majority=2，3的majority=2，5的majority=3，4的majority=2），2个哨兵都运行着，就可以允许执行故障转移 但是如果整个M1和S1运行的机器宕机了，那么哨兵只有1个了，此时就没有majority来允许执行故障转移，虽然另外一台机器还有一个R1，但是故障转移不会执行 4、经典的3节点哨兵集群 +----+ | M1 | | S1 | +----+ | +—-+ | +—-+| R2 |—-+—-| R3 || S2 | | S3 |+—-+ +—-+ Configuration: quorum = 2，majority = 2 如果M1所在机器宕机了，那么三个哨兵还剩下2个，S2和S3可以一致认为master宕机，然后选举出一个来执行故障转移 同时3个哨兵的majority是2，所以还剩下的2个哨兵运行着，就可以允许执行故障转移 redis哨兵主备切换的数据丢失问题：异步复制，集群脑裂课程大纲 1、两种数据丢失的情况2、解决异步复制和脑裂导致的数据丢失 1、两种数据丢失的情况 主备切换的过程，可能会导致数据丢失 （1）异步复制导致的数据丢失 因为master -&gt; slave的复制是异步的，所以可能有部分数据还没复制到slave，master就宕机了，此时这些部分数据就丢失了 （2）脑裂导致的数据丢失 脑裂，也就是说，某个master所在机器突然脱离了正常的网络，跟其他slave机器不能连接，但是实际上master还运行着 此时哨兵可能就会认为master宕机了，然后开启选举，将其他slave切换成了master 这个时候，集群里就会有两个master，也就是所谓的脑裂 此时虽然某个slave被切换成了master，但是可能client还没来得及切换到新的master，还继续写向旧master的数据可能也丢失了 因此旧master再次恢复的时候，会被作为一个slave挂到新的master上去，自己的数据会清空，重新从新的master复制数据 2、解决异步复制和脑裂导致的数据丢失 min-slaves-to-write 1min-slaves-max-lag 10 要求至少有1个slave，数据复制和同步的延迟不能超过10秒 如果说一旦所有的slave，数据复制和同步的延迟都超过了10秒钟，那么这个时候，master就不会再接收任何请求了 上面两个配置可以减少异步复制和脑裂导致的数据丢失 （1）减少异步复制的数据丢失 有了min-slaves-max-lag这个配置，就可以确保说，一旦slave复制数据和ack延时太长，就认为可能master宕机后损失的数据太多了，那么就拒绝写请求，这样可以把master宕机时由于部分数据未同步到slave导致的数据丢失降低的可控范围内 （2）减少脑裂的数据丢失 如果一个master出现了脑裂，跟其他slave丢了连接，那么上面两个配置可以确保说，如果不能继续给指定数量的slave发送数据，而且slave超过10秒没有给自己ack消息，那么就直接拒绝客户端的写请求 这样脑裂后的旧master就不会接受client的新数据，也就避免了数据丢失 上面的配置就确保了，如果跟任何一个slave丢了连接，在10秒后发现没有slave给自己ack，那么就拒绝新的写请求 因此在脑裂场景下，最多就丢失10秒的数据 redis哨兵的多个核心底层原理的深入解析1、sdown和odown转换机制 sdown和odown两种失败状态 sdown是主观宕机，就一个哨兵如果自己觉得一个master宕机了，那么就是主观宕机 odown是客观宕机，如果quorum数量的哨兵都觉得一个master宕机了，那么就是客观宕机 sdown达成的条件很简单，如果一个哨兵ping一个master，超过了is-master-down-after-milliseconds指定的毫秒数之后，就主观认为master宕机 sdown到odown转换的条件很简单，如果一个哨兵在指定时间内，收到了quorum指定数量的其他哨兵也认为那个master是sdown了，那么就认为是odown了，客观认为master宕机 2、哨兵集群的自动发现机制 哨兵互相之间的发现，是通过redis的pub/sub系统实现的，每个哨兵都会往sentinel:hello这个channel里发送一个消息，这时候所有其他哨兵都可以消费到这个消息，并感知到其他的哨兵的存在 每隔两秒钟，每个哨兵都会往自己监控的某个master+slaves对应的sentinel:hello channel里发送一个消息，内容是自己的host、ip和runid还有对这个master的监控配置 每个哨兵也会去监听自己监控的每个master+slaves对应的sentinel:hello channel，然后去感知到同样在监听这个master+slaves的其他哨兵的存在 每个哨兵还会跟其他哨兵交换对master的监控配置，互相进行监控配置的同步 3、slave配置的自动纠正 哨兵会负责自动纠正slave的一些配置，比如slave如果要成为潜在的master候选人，哨兵会确保slave在复制现有master的数据; 如果slave连接到了一个错误的master上，比如故障转移之后，那么哨兵会确保它们连接到正确的master上 4、slave-&gt;master选举算法 如果一个master被认为odown了，而且majority哨兵都允许了主备切换，那么某个哨兵就会执行主备切换操作，此时首先要选举一个slave来 会考虑slave的一些信息 （1）跟master断开连接的时长（2）slave优先级（3）复制offset（4）run id 如果一个slave跟master断开连接已经超过了down-after-milliseconds的10倍，外加master宕机的时长，那么slave就被认为不适合选举为master (down-after-milliseconds * 10) + milliseconds_since_master_is_in_SDOWN_state 接下来会对slave进行排序 （1）按照slave优先级进行排序，slave priority越低，优先级就越高(slave priority是在redis.conf文件中自己设置的)（2）如果slave priority相同，那么看replica offset，哪个slave复制了越多的数据，offset越靠后，优先级就越高（3）如果上面两个条件都相同，那么选择一个run id比较小的那个slave 5、quorum和majority 每次一个哨兵要做主备切换，首先需要quorum数量的哨兵认为odown，然后选举出一个哨兵来做切换，这个哨兵还得得到majority哨兵的授权，才能正式执行切换 如果quorum &lt; majority，比如5个哨兵，majority就是3，quorum设置为2，那么就3个哨兵授权就可以执行切换 但是如果quorum &gt;= majority，那么必须quorum数量的哨兵都授权，比如5个哨兵，quorum是5，那么必须5个哨兵都同意授权，才能执行切换 6、configuration epoch 哨兵会对一套redis master+slave进行监控，有相应的监控的配置 执行切换的那个哨兵，会从要切换到的新master（salve-&gt;master）那里得到一个configuration epoch，这就是一个version号，每次切换的version号都必须是唯一的 如果第一个选举出的哨兵切换失败了，那么其他哨兵，会等待failover-timeout时间，然后接替继续执行切换，此时会重新获取一个新的configuration epoch，作为新的version号 7、configuraiton传播 哨兵完成切换之后，会在自己本地更新生成最新的master配置，然后同步给其他的哨兵，就是通过之前说的pub/sub消息机制 这里之前的version号就很重要了，因为各种消息都是通过一个channel去发布和监听的，所以一个哨兵完成一次新的切换之后，新的master配置是跟着新的version号的 其他的哨兵都是根据版本号的大小来更新自己的master配置的 redis哨兵集群的实战配置quorum的解释如下： （1）至少多少个哨兵要一致同意，master进程挂掉了，或者slave进程挂掉了，或者要启动一个故障转移操作（2）quorum是用来识别故障的，真正执行故障转移的时候，还是要在哨兵集群执行选举，选举一个哨兵进程出来执行故障转移操作（3）假设有5个哨兵，quorum设置了2，那么如果5个哨兵中的2个都认为master挂掉了; 2个哨兵中的一个就会做一个选举，选举一个哨兵出来，执行故障转移; 如果5个哨兵中有3个哨兵都是运行的，那么故障转移就会被允许执行 动手实操，练习如何操作部署哨兵集群，如何基于哨兵进行故障转移，还有一些企业级的配置方案 1、哨兵的配置文件(存放在redis安装目录下)sentinel.conf 最小的配置 每一个哨兵都可以去监控多个maser-slaves的主从架构 因为可能你的公司里，为不同的项目，部署了多个master-slaves的redis主从集群 相同的一套哨兵集群，就可以去监控不同的多个redis主从集群 你自己给每个redis主从集群分配一个逻辑的名称 sentinel monitor mymaster 127.0.0.1 6379 2sentinel down-after-milliseconds mymaster 60000sentinel failover-timeout mymaster 180000sentinel parallel-syncs mymaster 1 sentinel monitor resque 192.168.1.3 6380 4sentinel down-after-milliseconds resque 10000sentinel failover-timeout resque 180000sentinel parallel-syncs resque 5 sentinel monitor mymaster 127.0.0.1 6379 类似这种配置，来指定对一个master的监控，给监控的master指定的一个名称，因为后面分布式集群架构里会讲解，可以配置多个master做数据拆分 sentinel down-after-milliseconds mymaster 60000sentinel failover-timeout mymaster 180000sentinel parallel-syncs mymaster 1 上面的三个配置，都是针对某个监控的master配置的，给其指定上面分配的名称即可 上面这段配置，就监控了两个master node 这是最小的哨兵配置，如果发生了master-slave故障转移，或者新的哨兵进程加入哨兵集群，那么哨兵会自动更新自己的配置文件 sentinel monitor master-group-name hostname port quorum quorum的解释如下： （1）至少多少个哨兵要一致同意，master进程挂掉了，或者slave进程挂掉了，或者要启动一个故障转移操作（2）quorum是用来识别故障的，真正执行故障转移的时候，还是要在哨兵集群执行选举，选举一个哨兵进程出来执行故障转移操作（3）假设有5个哨兵，quorum设置了2，那么如果5个哨兵中的2个都认为master挂掉了; 2个哨兵中的一个就会做一个选举，选举一个哨兵出来，执行故障转移; 如果5个哨兵中有3个哨兵都是运行的，那么故障转移就会被允许执行 down-after-milliseconds，超过多少毫秒跟一个redis实例断了连接，哨兵就可能认为这个redis实例挂了 parallel-syncs，新的master别切换之后，同时有多少个slave被切换到去连接新master，重新做同步，数字越低，花费的时间越多 假设你的redis是1个master，4个slave 然后master宕机了，4个slave中有1个切换成了master，剩下3个slave就要挂到新的master上面去 这个时候，如果parallel-syncs是1，那么3个slave，一个一个地挂接到新的master上面去，1个挂接完，而且从新的master sync完数据之后，再挂接下一个 如果parallel-syncs是3，那么一次性就会把所有slave挂接到新的master上去 failover-timeout，执行故障转移的timeout超时时长 2、在eshop-cache03上再部署一个redis 只要安装redis就可以了，不需要去部署redis实例的启动 wget http://downloads.sourceforge.net/tcl/tcl8.6.1-src.tar.gztar -xzvf tcl8.6.1-src.tar.gzcd /usr/local/tcl8.6.1/unix/./configuremake &amp;&amp; make install 使用redis-3.2.8.tar.gz（截止2017年4月的最新稳定版）tar -zxvf redis-3.2.8.tar.gzcd redis-3.2.8make &amp;&amp; make testmake install 2、正式的配置 哨兵默认用26379端口，默认不能跟其他机器在指定端口连通，只能在本地访问 mkdir /etc/sentinalmkdir -p /var/sentinal/5000 /etc/sentinel/5000.conf port 5000bind 192.168.31.187dir /var/sentinal/5000sentinel monitor mymaster 192.168.31.187 6379 2sentinel down-after-milliseconds mymaster 30000sentinel failover-timeout mymaster 60000sentinel parallel-syncs mymaster 1 port 5000bind 192.168.31.19dir /var/sentinal/5000sentinel monitor mymaster 192.168.31.187 6379 2sentinel down-after-milliseconds mymaster 30000sentinel failover-timeout mymaster 60000sentinel parallel-syncs mymaster 1 port 5000bind 192.168.31.227dir /var/sentinal/5000sentinel monitor mymaster 192.168.31.187 6379 2sentinel down-after-milliseconds mymaster 30000sentinel failover-timeout mymaster 60000sentinel parallel-syncs mymaster 1 3、启动哨兵进程 在eshop-cache01、eshop-cache02、eshop-cache03三台机器上，分别启动三个哨兵进程，组成一个集群，观察一下日志的输出 redis-sentinel /etc/sentinal/5000.confredis-server /etc/sentinal/5000.conf –sentinel 日志里会显示出来，每个哨兵都能去监控到对应的redis master，并能够自动发现对应的slave 哨兵之间，互相会自动进行发现，用的就是之前说的pub/sub，消息发布和订阅channel消息系统和机制 4、检查哨兵状态 redis-cli -h 192.168.31.187 -p 5000 sentinel master mymasterSENTINEL slaves mymasterSENTINEL sentinels mymaster SENTINEL get-master-addr-by-name mymaster redis cluster横向扩容master1、单机redis在海量数据面前的瓶颈 2、怎么才能够突破单机瓶颈，让redis支撑海量数据？ 3、redis的集群架构 redis cluster 支撑N个redis master node，每个master node都可以挂载多个slave node 读写分离的架构，对于每个master来说，写就写到master，然后读就从mater对应的slave去读 高可用，因为每个master都有salve节点，那么如果mater挂掉，redis cluster这套机制，就会自动将某个slave切换成master redis cluster（多master + 读写分离 + 高可用） 我们只要基于redis cluster去搭建redis集群即可，不需要手工去搭建replication复制+主从架构+读写分离+哨兵集群+高可用 4、redis cluster vs. replication + sentinal 如果你的数据量很少，主要是承载高并发高性能的场景，比如你的缓存一般就几个G，单机足够了 replication，一个mater，多个slave，要几个slave跟你的要求的读吞吐量有关系，然后自己搭建一个sentinal集群，去保证redis主从架构的高可用性，就可以了 redis cluster，主要是针对海量数据+高并发+高可用的场景，海量数据，如果你的数据量很大，那么建议就用redis cluster redis阶段性总结1、讲解redis是为了什么？ topic：高并发、亿级流量、高性能、海量数据的场景，电商网站的商品详情页系统的缓存架构 商品详情页系统，大型电商网站，会有很多部分组成，但是支撑高并发、亿级流量的，主要就是其中的大型的缓存架构 在这个大型的缓存架构中，redis是最最基础的一层 高并发，缓存架构中除了redis，还有其他的组成部分，但是redis至关重要 大量的离散请求，随机请求，各种你未知的用户过来的请求，上千万用户过来访问，每个用户访问10次; 集中式的请求，1个用户过来，一天访问1亿次 支撑商品展示的最重要的，就是redis cluster，去抗住每天上亿的请求流量，支撑高并发的访问 redis cluster在整个缓存架构中，如何跟其他几个部分搭配起来组成一个大型的缓存系统，后面再讲 2、讲解的redis可以实现什么效果？ 我之前一直在redis的各个知识点的讲解之前都强调一下，我们要讲解的每个知识点，要解决的问题是什么？？？ redis：持久化、复制（主从架构）、哨兵（高可用，主备切换）、redis cluster（海量数据+横向扩容+高可用/主备切换） 持久化：高可用的一部分，在发生redis集群灾难的情况下（比如说部分master+slave全部死掉了），如何快速进行数据恢复，快速实现服务可用，才能实现整个系统的高可用 复制：主从架构，master -&gt; slave 复制，读写分离的架构，写master，读slave，横向扩容slave支撑更高的读吞吐，读高并发，10万，20万，30万，上百万，QPS，横向扩容 哨兵：高可用，主从架构，在master故障的时候，快速将slave切换成master，实现快速的灾难恢复，实现高可用性 redis cluster：多master读写，数据分布式的存储，横向扩容，水平扩容，快速支撑高达的数据量+更高的读写QPS，自动进行master -&gt; slave的主备切换，高可用 让底层的缓存系统，redis，实现能够任意水平扩容，支撑海量数据（1T+，几十T，10G * 600 redis = 6T），支撑很高的读写QPS（redis单机在几万QPS，10台，几十万QPS），高可用性（给我们每个redis实例都做好AOF+RDB的备份策略+容灾策略，slave -&gt; master主备切换） 1T+海量数据、10万+读写QPS、99.99%高可用性 3、redis的第一套企业级的架构 如果你的数据量不大，单master就可以容纳，一般来说你的缓存的总量在10G以内就可以，那么建议按照以下架构去部署redis redis持久化+备份方案+容灾方案+replication（主从+读写分离）+sentinal（哨兵集群，3个节点，高可用性） 可以支撑的数据量在10G以内，可以支撑的写QPS在几万左右，可以支撑的读QPS可以上10万以上（随你的需求，水平扩容slave节点就可以），可用性在99.99% 4、redis的第二套企业级架构 如果你的数据量很大，比如我们课程的topic，大型电商网站的商品详情页的架构（对标那些国内排名前三的大电商网站，宝，东，*宁易购），数据量是很大的 海量数据 redis cluster 多master分布式存储数据，水平扩容 支撑更多的数据量，1T+以上没问题，只要扩容master即可 读写QPS分别都达到几十万都没问题，只要扩容master即可，redis cluster，读写分离，支持不太好，readonly才能去slave上读 支撑99.99%可用性，也没问题，slave -&gt; master的主备切换，冗余slave去进一步提升可用性的方案（每个master挂一个slave，但是整个集群再加个3个slave冗余一下） 我们课程里，两套架构都讲解了，后续的业务系统的开发，主要是基于redis cluster去做 5、我们现在课程讲解的项目进展到哪里了？ 我们要做后续的业务系统的开发，redis的架构部署好，是第一件事情，也是非常重要的，也是你作为一个架构师而言，在对系统进行设计的时候，你必须要考虑到底层的redis的并发、性能、能支撑的数据量、可用性 redis：水平扩容，海量数据，上10万的读写QPS，99.99%高可用性 从架构的角度，我们的redis是可以做到的，水平扩容，只要机器足够，到1T数据量，50万读写QPS，99.99% 正式开始做大型电商网站的商品详情页系统，大规模的缓存架构设计 10.1商品详情页的多级缓存架构我们之前的三十讲，主要是在讲解redis如何支撑海量数据、高并发读写、高可用服务的架构，redis架构 redis架构，在我们的真正类似商品详情页读高并发的系统中，redis就是底层的缓存存储的支持 从这一讲开始，我们正式开始做业务系统的开发 亿级流量以上的电商网站的商品详情页的系统，商品详情页系统，大量的业务，十几个人做一两年，堆出来复杂的业务系统 几十个小时的课程，讲解复杂的业务 把整体的架构给大家讲解清楚，然后浓缩和精炼里面的业务，提取部分业务，做一些简化，把整个详情页系统的流程跑出来 架构，骨架，有少量的业务，血和肉，把整个项目串起来，在业务背景下，去学习架构 讲解商品详情页系统，缓存架构，90%大量的业务代码（没有什么技术含量），10%的最优技术含量的就是架构，上亿流量，每秒QPS几万，上十万的，读并发 读并发，缓存架构 1、上亿流量的商品详情页系统的多级缓存架构 很多人以为，做个缓存，其实就是用一下redis，访问一下，就可以了，简单的缓存 做复杂的缓存，支撑电商复杂的场景下的高并发的缓存，遇到的问题，非常非常之多，绝对不是说简单的访问一下redsi就可以了 采用三级缓存：nginx本地缓存+redis分布式缓存+tomcat堆缓存的多级缓存架构 时效性要求非常高的数据：库存 一般来说，显示的库存，都是时效性要求会相对高一些，因为随着商品的不断的交易，库存会不断的变化 当然，我们就希望当库存变化的时候，尽可能更快将库存显示到页面上去，而不是说等了很长时间，库存才反应到页面上去 时效性要求不高的数据：商品的基本信息（名称、颜色、版本、规格参数，等等） 时效性要求不高的数据，就还好，比如说你现在改变了商品的名称，稍微晚个几分钟反应到商品页面上，也还能接受 商品价格/库存等时效性要求高的数据，而且种类较少，采取相关的服务系统每次发生了变更的时候，直接采取数据库和redis缓存双写的方案，这样缓存的时效性最高 商品基本信息等时效性不高的数据，而且种类繁多，来自多种不同的系统，采取MQ异步通知的方式，写一个数据生产服务，监听MQ消息，然后异步拉取服务的数据，更新tomcat jvm缓存+redis缓存 nginx+lua脚本做页面动态生成的工作，每次请求过来，优先从nginx本地缓存中提取各种数据，结合页面模板，生成需要的页面 如果nginx本地缓存过期了，那么就从nginx到redis中去拉取数据，更新到nginx本地 如果redis中也被LRU算法清理掉了，那么就从nginx走http接口到后端的服务中拉取数据，数据生产服务中，现在本地tomcat里的jvm堆缓存中找，ehcache，如果也被LRU清理掉了，那么就重新发送请求到源头的服务中去拉取数据，然后再次更新tomcat堆内存缓存+redis缓存，并返回数据给nginx，nginx缓存到本地 2、多级缓存架构中每一层的意义 nginx本地缓存，抗的是热数据的高并发访问，一般来说，商品的购买总是有热点的，比如每天购买iphone、nike、海尔等知名品牌的东西的人，总是比较多的 这些热数据，利用nginx本地缓存，由于经常被访问，所以可以被锁定在nginx的本地缓存内 大量的热数据的访问，就是经常会访问的那些数据，就会被保留在nginx本地缓存内，那么对这些热数据的大量访问，就直接走nginx就可以了 那么大量的访问，直接就可以走到nginx就行了，不需要走后续的各种网络开销了 redis分布式大规模缓存，抗的是很高的离散访问，支撑海量的数据，高并发的访问，高可用的服务 redis缓存最大量的数据，最完整的数据和缓存，1T+数据; 支撑高并发的访问，QPS最高到几十万; 可用性，非常好，提供非常稳定的服务 nginx本地内存有限，也就能cache住部分热数据，除了各种iphone、nike等热数据，其他相对不那么热的数据，可能流量会经常走到redis那里 利用redis cluster的多master写入，横向扩容，1T+以上海量数据支持，几十万的读写QPS，99.99%高可用性，那么就可以抗住大量的离散访问请求 tomcat jvm堆内存缓存，主要是抗redis大规模灾难的，如果redis出现了大规模的宕机，导致nginx大量流量直接涌入数据生产服务，那么最后的tomcat堆内存缓存至少可以再抗一下，不至于让数据库直接裸奔 同时tomcat jvm堆内存缓存，也可以抗住redis没有cache住的最后那少量的部分缓存 Cache Aside Pattern缓存 + 数据库读写模式的分析最经典的缓存+数据库读写的模式，cache aside pattern 1、Cache Aside Pattern （1）读的时候，先读缓存，缓存没有的话，那么就读数据库，然后取出数据后放入缓存，同时返回响应 （2）更新的时候，先删除缓存，然后再更新数据库 2、为什么是删除缓存，而不是更新缓存呢？ 原因很简单，很多时候，复杂点的缓存的场景，因为缓存有的时候，不简单是数据库中直接取出来的值 商品详情页的系统，修改库存，只是修改了某个表的某些字段，但是要真正把这个影响的最终的库存计算出来，可能还需要从其他表查询一些数据，然后进行一些复杂的运算，才能最终计算出 现在最新的库存是多少，然后才能将库存更新到缓存中去 比如可能更新了某个表的一个字段，然后其对应的缓存，是需要查询另外两个表的数据，并进行运算，才能计算出缓存最新的值的 更新缓存的代价是很高的 是不是说，每次修改数据库的时候，都一定要将其对应的缓存去跟新一份？也许有的场景是这样的，但是对于比较复杂的缓存数据计算的场景，就不是这样了 如果你频繁修改一个缓存涉及的多个表，那么这个缓存会被频繁的更新，频繁的更新缓存 但是问题在于，这个缓存到底会不会被频繁访问到？？？ 举个例子，一个缓存涉及的表的字段，在1分钟内就修改了20次，或者是100次，那么缓存跟新20次，100次; 但是这个缓存在1分钟内就被读取了1次，有大量的冷数据 28法则，黄金法则，20%的数据，占用了80%的访问量 实际上，如果你只是删除缓存的话，那么1分钟内，这个缓存不过就重新计算一次而已，开销大幅度降低 每次数据过来，就只是删除缓存，然后修改数据库，如果这个缓存，在1分钟内只是被访问了1次，那么只有那1次，缓存是要被重新计算的，用缓存才去算缓存 其实删除缓存，而不是更新缓存，就是一个lazy计算的思想，不要每次都重新做复杂的计算，不管它会不会用到，而是让它到需要被使用的时候再重新计算 mybatis，hibernate，懒加载，思想 查询一个部门，部门带了一个员工的list，没有必要说每次查询部门，都里面的1000个员工的数据也同时查出来啊 80%的情况，查这个部门，就只是要访问这个部门的信息就可以了 先查部门，同时要访问里面的员工，那么这个时候只有在你要访问里面的员工的时候，才会去数据库里面查询1000个员工 高并发下的缓存与+数据库双写不一致问题分析与设计马上开始去开发业务系统 从哪一步开始做，从比较简单的那一块开始做，实时性要求比较高的那块数据的缓存去做 实时性比较高的数据缓存，选择的就是库存的服务 库存可能会修改，每次修改都要去更新这个缓存数据; 每次库存的数据，在缓存中一旦过期，或者是被清理掉了，前端的nginx服务都会发送请求给库存服务，去获取相应的数据 库存这一块，写数据库的时候，直接更新redis缓存 实际上没有这么的简单，这里，其实就涉及到了一个问题，数据库与缓存双写，数据不一致的问题 围绕和结合实时性较高的库存服务，把数据库与缓存双写不一致问题以及其解决方案，给大家讲解一下 数据库与缓存双写不一致，很常见的问题，大型的缓存架构中，第一个解决方案 大型的缓存架构全部讲解完了以后，整套架构是非常复杂，架构可以应对各种各样奇葩和极端的情况 也有一种可能，不是说，来讲课的就是超人，万能的 讲课，就跟写书一样，很可能会写错，也可能有些方案里的一些地方，我没考虑到 也可能说，有些方案只是适合某些场景，在某些场景下，可能需要你进行方案的优化和调整才能适用于你自己的项目 大家觉得对这些方案有什么疑问或者见解，都可以找我，沟通一下 如果的确我觉得是我讲解的不对，或者有些地方考虑不周，那么我可以在视频里补录，更新到网站上面去 多多包涵 1、最初级的缓存不一致问题以及解决方案 问题：先修改数据库，再删除缓存，如果删除缓存失败了，那么会导致数据库中是新数据，缓存中是旧数据，数据出现不一致 解决思路 先删除缓存，再修改数据库，如果删除缓存成功了，如果修改数据库失败了，那么数据库中是旧数据，缓存中是空的，那么数据不会不一致 因为读的时候缓存没有，则读数据库中旧数据，然后更新到缓存中 2、比较复杂的数据不一致问题分析 数据发生了变更，先删除了缓存，然后要去修改数据库，此时还没修改 一个请求过来，去读缓存，发现缓存空了，去查询数据库，查到了修改前的旧数据，放到了缓存中 数据变更的程序完成了数据库的修改 完了，数据库和缓存中的数据不一样了。。。。 3、为什么上亿流量高并发场景下，缓存会出现这个问题？ 只有在对一个数据在并发的进行读写的时候，才可能会出现这种问题 其实如果说你的并发量很低的话，特别是读并发很低，每天访问量就1万次，那么很少的情况下，会出现刚才描述的那种不一致的场景 但是问题是，如果每天的是上亿的流量，每秒并发读是几万，每秒只要有数据更新的请求，就可能会出现上述的数据库+缓存不一致的情况 高并发了以后，问题是很多的 4、数据库与缓存更新与读取操作进行异步串行化 更新数据的时候，根据数据的唯一标识，将操作路由之后，发送到一个jvm内部的队列中 读取数据的时候，如果发现数据不在缓存中，那么将重新读取数据+更新缓存的操作，根据唯一标识路由之后，也发送同一个jvm内部的队列中 一个队列对应一个工作线程 每个工作线程串行拿到对应的操作，然后一条一条的执行 这样的话，一个数据变更的操作，先执行，删除缓存，然后再去更新数据库，但是还没完成更新 此时如果一个读请求过来，读到了空的缓存，那么可以先将缓存更新的请求发送到队列中，此时会在队列中积压，然后同步等待缓存更新完成 这里有一个优化点，一个队列中，其实多个更新缓存请求串在一起是没意义的，因此可以做过滤，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新请求操作进去了，直接等待前面的更新操作请求完成即可 待那个队列对应的工作线程完成了上一个操作的数据库的修改之后，才会去执行下一个操作，也就是缓存更新的操作，此时会从数据库中读取最新的值，然后写入缓存中 如果请求还在等待时间范围内，不断轮询发现可以取到值了，那么就直接返回; 如果请求等待的时间超过一定时长，那么这一次直接从数据库中读取当前的旧值 5、高并发的场景下，该解决方案要注意的问题 （1）读请求长时阻塞 由于读请求进行了非常轻度的异步化，所以一定要注意读超时的问题，每个读请求必须在超时时间范围内返回 该解决方案，最大的风险点在于说，可能数据更新很频繁，导致队列中积压了大量更新操作在里面，然后读请求会发生大量的超时，最后导致大量的请求直接走数据库 务必通过一些模拟真实的测试，看看更新数据的频繁是怎样的 另外一点，因为一个队列中，可能会积压针对多个数据项的更新操作，因此需要根据自己的业务情况进行测试，可能需要部署多个服务，每个服务分摊一些数据的更新操作 如果一个内存队列里居然会挤压100个商品的库存修改操作，每隔库存修改操作要耗费10ms区完成，那么最后一个商品的读请求，可能等待10 * 100 = 1000ms = 1s后，才能得到数据 这个时候就导致读请求的长时阻塞 一定要做根据实际业务系统的运行情况，去进行一些压力测试，和模拟线上环境，去看看最繁忙的时候，内存队列可能会挤压多少更新操作，可能会导致最后一个更新操作对应的读请求，会hang多少时间，如果读请求在200ms返回，如果你计算过后，哪怕是最繁忙的时候，积压10个更新操作，最多等待200ms，那还可以的 如果一个内存队列可能积压的更新操作特别多，那么你就要加机器，让每个机器上部署的服务实例处理更少的数据，那么每个内存队列中积压的更新操作就会越少 其实根据之前的项目经验，一般来说数据的写频率是很低的，因此实际上正常来说，在队列中积压的更新操作应该是很少的 针对读高并发，读缓存架构的项目，一般写请求相对读来说，是非常非常少的，每秒的QPS能到几百就不错了 一秒，500的写操作，5份，每200ms，就100个写操作 单机器，20个内存队列，每个内存队列，可能就积压5个写操作，每个写操作性能测试后，一般在20ms左右就完成 那么针对每个内存队列中的数据的读请求，也就最多hang一会儿，200ms以内肯定能返回了 写QPS扩大10倍，但是经过刚才的测算，就知道，单机支撑写QPS几百没问题，那么就扩容机器，扩容10倍的机器，10台机器，每个机器20个队列，200个队列 大部分的情况下，应该是这样的，大量的读请求过来，都是直接走缓存取到数据的 少量情况下，可能遇到读跟数据更新冲突的情况，如上所述，那么此时更新操作如果先入队列，之后可能会瞬间来了对这个数据大量的读请求，但是因为做了去重的优化，所以也就一个更新缓存的操作跟在它后面 等数据更新完了，读请求触发的缓存更新操作也完成，然后临时等待的读请求全部可以读到缓存中的数据 （2）读请求并发量过高 这里还必须做好压力测试，确保恰巧碰上上述情况的时候，还有一个风险，就是突然间大量读请求会在几十毫秒的延时hang在服务上，看服务能不能抗的住，需要多少机器才能抗住最大的极限情况的峰值 但是因为并不是所有的数据都在同一时间更新，缓存也不会同一时间失效，所以每次可能也就是少数数据的缓存失效了，然后那些数据对应的读请求过来，并发量应该也不会特别大 按1:99的比例计算读和写的请求，每秒5万的读QPS，可能只有500次更新操作 如果一秒有500的写QPS，那么要测算好，可能写操作影响的数据有500条，这500条数据在缓存中失效后，可能导致多少读请求，发送读请求到库存服务来，要求更新缓存 一般来说，1:1，1:2，1:3，每秒钟有1000个读请求，会hang在库存服务上，每个读请求最多hang多少时间，200ms就会返回 在同一时间最多hang住的可能也就是单机200个读请求，同时hang住 单机hang200个读请求，还是ok的 1:20，每秒更新500条数据，这500秒数据对应的读请求，会有20 * 500 = 1万 1万个读请求全部hang在库存服务上，就死定了 （3）多服务实例部署的请求路由 可能这个服务部署了多个实例，那么必须保证说，执行数据更新操作，以及执行缓存更新操作的请求，都通过nginx服务器路由到相同的服务实例上 （4）热点商品的路由问题，导致请求的倾斜 万一某个商品的读写请求特别高，全部打到相同的机器的相同的队列里面去了，可能造成某台机器的压力过大 就是说，因为只有在商品数据更新的时候才会清空缓存，然后才会导致读写并发，所以更新频率不是太高的话，这个问题的影响并不是特别大 但是的确可能某些机器的负载会高一些]]></content>
      <tags>
        <tag>项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络零散知识]]></title>
    <url>%2F2019%2F09%2F28%2F%E7%BD%91%E7%BB%9C%E9%9B%B6%E6%95%A3%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[TCP和UDP的区别（1）TCP是面向连接的，udp是无连接的即发送数据前不需要先建立链接。 （2）TCP提供可靠的服务。也就是说，通过TCP连接传送的数据，无差错，不丢失，不重复，且按序到达;UDP尽最大努力交付，即不保证可靠交付。 并且因为tcp可靠，面向连接，不会丢失数据因此适合大数据量的交换。 （3）TCP是面向字节流，UDP面向报文，并且网络出现拥塞不会使得发送速率降低（因此会出现丢包，对实时的应用比如IP电话和视频会议等）。 （4）TCP只能是1对1的，UDP支持1对1,1对多。 （5）TCP的首部较大为20字节，而UDP只有8字节。 （6）TCP是面向连接的可靠性传输，而UDP是不可靠的。 基于TCP的应用层协议有：POP3、SMTP（简单邮件传输协议）、TELNET（远程登陆协议）、HTTP（超文本传输协议）、HTTPS（超文本传输安全协议）、FTP（文件传输协议） 基于UDP的应用层协议：TFTP（简单文件传输协议）、RIP（路由信息协议）、DHCP（动态主机设置协议）、BOOTP（引导程序协议，DHCP的前身）、IGMP（Internet组管理协议） 基于TCP和UDP协议：DNS（域名系统）、ECHO（回绕协议） 以下内容来自：公众号 前端指南，作者 前端指南 简述 http 1.1 与 http 1.0 的区别 http 1.0 对于每个连接都得建立一次连接, 一次只能传送一个请求和响应, 请求就会关闭, http1.0 没有 Host 字段 而 http1.1 在同一个连接中可以传送多个请求和响应, 多个请求可以重叠和同时进行, http1.1 必须有 host 字段 http1.1 中引入了 ETag 头, 它的值 entity tag 可以用来唯一的描述一个资源. 请求消息中可以使用 If-None-Match 头域来匹配资源的 entitytag 是否有变化 http1.1 新增了 Cache-Control 头域(消息请求和响应请求都可以使用), 它支持一个可扩展的指令子集 http1.0 中只定义了 16 个状态响应码, 对错误或警告的提示不够具体. http1.1 引入了一个 Warning 头域, 增加对错误或警告信息的描述. 且新增了 24 个状态响应码 从输入 URL 到页面加载发生了什么[必考]总体来说分为以下几个过程: DNS 解析 TCP 连接 发送 HTTP 请求 服务器处理请求并返回 HTTP 报文 浏览器解析渲染页面 连接结束 Socket 连接与 HTTP 连接的联系与区别由于通常情况下 Socket 连接就是 TCP 连接，因此 Socket 连接一旦建立，通信双方即可开始相互发送数据内容，直到双方连接断开。但在实际网络应用中，客户端到服务器之间的通信往往需要穿越多个中间节点，例如路由器、网关、防火墙等，大部分防火墙默认会关闭长时间处于非活跃状态的连接而导致 Socket 连接断连，因此需要通过轮询告诉网络，该连接处于活跃状态。 而 HTTP 连接使用的是“请求—响应”的方式，不仅在请求时需要先建立连接，而且需要客户端向服务器发出请求后，服务器端才能回复数据。 很多情况下，需要服务器端主动向客户端推送数据，保持客户端与服务器数据的实时与同步。此时若双方建立的是 Socket 连接，服务器就可以直接将数据传送给客户端;若双方建立的是 HTTP 连接，则服务器需要等到客户端发送一次请求后才能将数据传回给客户端，因此，客户端定时向服务器端发送连接请求，不仅可以保持在线，同时也是在“询问”服务器是否有新的数据，如果有就将数据传给客户端。 Http2.0 与 http1.x 相比有什么优点(常考) 二进制格式:http1.x 是文本协议，而 http2.0 是二进制以帧为基本单位，是一个二进制协议，一帧中除了包含数据外同时还包含该帧的标识：Stream Identifier，即标识了该帧属于哪个 request,使得网络传输变得十分灵活。 多路复用: 一个很大的改进，原先 http1.x 一个连接一个请求的情况有比较大的局限性，也引发了很多问题，如建立多个连接的消耗以及效率问题。 http1.x 为了解决效率问题，可能会尽量多的发起并发的请求去加载资源，然而浏览器对于同一域名下的并发请求有限制，而优化的手段一般是将请求的资源放到不同的域名下来突破这种限制。 而 http2.0 支持的多路复用可以很好的解决这个问题，多个请求共用一个 TCP 连接，多个请求可以同时在这个 TCP 连接上并发，一个是解决了建立多个 TCP 连接的消耗问题，一个也解决了效率的问题。那么是什么原理支撑多个请求可以在一个 TCP 连接上并发呢？基本原理就是上面的二进制分帧，因为每一帧都有一个身份标识，所以多个请求的不同帧可以并发的无序发送出去，在服务端会根据每一帧的身份标识，将其整理到对应的 request 中。 header 头部压缩:主要是通过压缩 header 来减少请求的大小，减少流量消耗，提高效率。因为之前存在一个问题是，每次请求都要带上 header，而这个 header 中的数据通常是一层不变的。 支持服务端推送]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[查漏补缺]]></title>
    <url>%2F2019%2F09%2F26%2F%E6%9F%A5%E6%BC%8F%E8%A1%A5%E7%BC%BA%2F</url>
    <content type="text"><![CDATA[自19.09.26开始，对复习的知识遗漏点进行总结 19.09.26JAVA锁Java SE1.6为了减少获得锁和释放锁所带来的性能消耗，引入了“偏向锁”和“轻量级锁”，所以在Java SE1.6里锁一共有四种状态，无锁状态，偏向锁状态，轻量级锁状态和重量级锁状态，它会随着竞争情况逐渐升级。锁可以升级但不能降级，意味着偏向锁升级成轻量级锁后不能降级成偏向锁。这种锁升级却不能降级的策略，目的是为了提高获得锁和释放锁的效率。 悲观锁与乐观锁悲观锁总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁，读锁，写锁等，都是在做操作之前先上锁。JAVA中synchronize和ReentrantLock等独占锁就是悲观锁思想的实现。 乐观锁总是假设最好的情况，每次拿数据都认为别人不会修改，所以不会上锁。但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。 乐观锁适用于多读的应用类型，这样可以提高吞吐量。 java中的4种锁，分别是重量级锁，自旋锁，轻量级锁和偏向锁。重量级锁是悲观锁的一种，自旋锁，轻量级锁和偏向锁属于乐观锁。 乐观锁常见的两种实现方式 乐观锁一般会使用版本号机制或CAS算法实现。 1. 版本号机制一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一。当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值为当前数据库中的version值相等时才更新，否则重试更新操作，直到更新成功。 举一个简单的例子： 假设数据库中帐户信息表中有一个 version 字段，当前值为 1 ；而当前帐户余额字段（ balance ）为 $100 。 操作员 A 此时将其读出（ version=1 ），并从其帐户余额中扣除 $50（ $100-$50 ）。 在操作员 A 操作的过程中，操作员B 也读入此用户信息（ version=1 ），并从其帐户余额中扣除 $20 （ $100-$20 ）。 操作员 A 完成了修改工作，将数据版本号加一（ version=2 ），连同帐户扣除后余额（ balance=$50 ），提交至数据库更新，此时由于提交数据版本大于数据库记录当前版本，数据被更新，数据库记录 version 更新为 2 。 操作员 B 完成了操作，也将版本号加一（ version=2 ）试图向数据库提交数据（ balance=$80 ），但此时比对数据库记录版本时发现，操作员 B 提交的数据版本号为 2 ，数据库记录当前版本也为 2 ，不满足 “ 提交版本必须大于记录当前版本才能执行更新 “ 的乐观锁策略，因此，操作员 B 的提交被驳回。 这样，就避免了操作员 B 用基于 version=1 的旧数据修改的结果覆盖操作员A 的操作结果的可能。 2.简单回顾一下CAS算法CAS算法 即compare and swap（比较与交换），是一种有名的无锁算法。无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）。CAS算法涉及到三个操作数 需要读写的内存值 V 进行比较的值 A 拟写入的新值 B 当且仅当 V 的值等于 A时，CAS通过原子方式用新值B来更新V的值，否则不会执行任何操作（比较和替换是一个原子操作）。一般情况下是一个自旋操作，即不断的重试。 乐观锁的缺点 ABA 问题是乐观锁一个常见的问题 1 ABA 问题如果一个变量V初次读取的时候是A值，并且在准备赋值的时候检查到它仍然是A值，那我们就能说明它的值没有被其他线程修改过了吗？很明显是不能的，因为在这段时间它的值可能被改为其他值，然后又改回A，那CAS操作就会误认为它从来没有被修改过。这个问题被称为CAS操作的 “ABA”问题。 JDK 1.5 以后的 AtomicStampedReference 类就提供了此种能力，其中的 compareAndSet 方法就是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 2 循环时间长开销大自旋CAS（也就是不成功就一直循环执行直到成功）如果长时间不成功，会给CPU带来非常大的执行开销。 如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，pause指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline flush），从而提高CPU的执行效率。 3 只能保证一个共享变量的原子操作CAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效。但是从 JDK 1.5开始，提供了AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作.所以我们可以使用锁或者利用AtomicReference类把多个共享变量合并成一个共享变量来操作。 什么是自旋锁？自旋锁（spinlock）：是指当一个线程在获取锁的时候，如果锁已经被其它线程获取，那么该线程将循环等待，然后不断的判断锁是否能够被成功获取，直到获取到锁才会退出循环。 获取锁的线程一直处于活跃状态，但是并没有执行任何有效的任务，使用这种锁会造成busy-waiting。 它是为实现保护共享资源而提出一种锁机制。其实，自旋锁与互斥锁比较类似，它们都是为了解决对某项资源的互斥使用。无论是互斥锁，还是自旋锁，在任何时刻，最多只能有一个保持者，也就说，在任何时刻最多只能有一个执行单元获得锁。但是两者在调度机制上略有不同。对于互斥锁，如果资源已经被占用，资源申请者只能进入睡眠状态。但是自旋锁不会引起调用者睡眠，如果自旋锁已经被别的执行单元保持，调用者就一直循环在那里看是否该自旋锁的保持者已经释放了锁，”自旋”一词就是因此而得名。 Java如何实现自旋锁？下面是个简单的例子： 1234567891011121314public class SpinLock &#123; private AtomicReference&lt;Thread&gt; cas = new AtomicReference&lt;Thread&gt;(); public void lock() &#123; Thread current = Thread.currentThread(); // 利用CAS while (!cas.compareAndSet(null, current)) &#123; // DO nothing &#125; &#125; public void unlock() &#123; Thread current = Thread.currentThread(); cas.compareAndSet(current, null); &#125;&#125;1234567891011121314 lock（)方法利用的CAS，当第一个线程A获取锁的时候，能够成功获取到，不会进入while循环，如果此时线程A没有释放锁，另一个线程B又来获取锁，此时由于不满足CAS，所以就会进入while循环，不断判断是否满足CAS，直到A线程调用unlock方法释放了该锁。 自旋锁存在的问题 如果某个线程持有锁的时间过长，就会导致其它等待获取锁的线程进入循环等待，消耗CPU。使用不当会造成CPU使用率极高。 上面Java实现的自旋锁不是公平的，即无法满足等待时间最长的线程优先获取锁。不公平的锁就会存在“线程饥饿”问题。 自旋锁的优点 自旋锁不会使线程状态发生切换，一直处于用户态，即线程一直都是active的；不会使线程进入阻塞状态，减少了不必要的上下文切换，执行速度快 非自旋锁在获取不到锁的时候会进入阻塞状态，从而进入内核态，当获取到锁的时候需要从内核态恢复，需要线程上下文切换。 （线程被阻塞后便进入内核（Linux）调度状态，这个会导致系统在用户态与内核态之间来回切换，严重影响锁的性能） TicketLock主要解决的是公平性的问题。思路：每当有线程获取锁的时候，就给该线程分配一个递增的id，我们称之为排队号，同时，锁对应一个服务号，每当有线程释放锁，服务号就会递增，此时如果服务号与某个线程排队号一致，那么该线程就获得锁，由于排队号是递增的，所以就保证了最先请求获取锁的线程可以最先获取到锁，就实现了公平性。 可以想象成银行办理业务排队，排队的每一个顾客都代表一个需要请求锁的线程，而银行服务窗口表示锁，每当有窗口服务完成就把自己的服务号加一，此时在排队的所有顾客中，只有自己的排队号与服务号一致的才可以得到服务。 偏向锁Java偏向锁(Biased Locking)是Java6引入的一项多线程优化。偏向锁，顾名思义，它会偏向于第一个访问锁的线程，如果在运行过程中，同步锁只有一个线程访问，不存在多线程争用的情况，则线程是不需要触发同步的，这种情况下，就会给线程加一个偏向锁。如果在运行过程中，遇到了其他线程抢占锁，则持有偏向锁的线程会被挂起，JVM会消除它身上的偏向锁，将锁恢复到标准的轻量级锁。 它通过消除资源无竞争情况下的同步原语，进一步提高了程序的运行性能。 偏向锁的实现偏向锁获取过程： 访问Mark Word中偏向锁的标识是否设置成1，锁标志位是否为01，确认为可偏向状态。 如果为可偏向状态，则测试线程ID是否指向当前线程，如果是，进入步骤5，否则进入步骤3。 如果线程ID并未指向当前线程，则通过CAS操作竞争锁。如果竞争成功，则将Mark Word中线程ID设置为当前线程ID，然后执行5；如果竞争失败，执行4。 如果CAS获取偏向锁失败，则表示有竞争。当到达全局安全点（safepoint）时获得偏向锁的线程被挂起，偏向锁升级为轻量级锁，然后被阻塞在安全点的线程继续往下执行同步代码。（撤销偏向锁的时候会导致stop the word） 执行同步代码。 注意：第四步中到达安全点safepoint会导致stop the word，时间很短。 偏向锁的释放：偏向锁的撤销在上述第四步骤中有提到。偏向锁只有遇到其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁，线程不会主动去释放偏向锁。偏向锁的撤销，需要等待全局安全点（在这个时间点上没有字节码正在执行），它会首先暂停拥有偏向锁的线程，判断锁对象是否处于被锁定状态，撤销偏向锁后恢复到未锁定（标志位为“01”）或轻量级锁（标志位为“00”）的状态。 偏向锁的适用场景始终只有一个线程在执行同步块，在它没有执行完释放锁之前，没有其它线程去执行同步块，在锁无竞争的情况下使用，一旦有了竞争就升级为轻量级锁，升级为轻量级锁的时候需要撤销偏向锁，撤销偏向锁的时候会导致stop the word操作；在有锁的竞争时，偏向锁会多做很多额外操作，尤其是撤销偏向所的时候会导致进入安全点，安全点会导致stw，导致性能下降，这种情况下应当禁用； 总结自旋锁线程的阻塞和唤醒需要CPU从用户态转为核心态，频繁的阻塞和唤醒对CPU来说是一件负担很重的工作。同时我们可以发现，很多对象锁的锁定状态只会持续很短的一段时间，例如整数的自加操作，在很短的时间内阻塞并唤醒线程显然不值得，为此引入了自旋锁。 所谓“自旋”，就是让线程去执行一个无意义的循环，循环结束后再去重新竞争锁，如果竞争不到继续循环，循环过程中线程会一直处于running状态，但是基于JVM的线程调度，会出让时间片，所以其他线程依旧有申请锁和释放锁的机会。 自旋锁省去了阻塞锁的时间空间（队列的维护等）开销，但是长时间自旋就变成了“忙式等待”，忙式等待显然还不如阻塞锁。所以自旋的次数一般控制在一个范围内，例如10,100等，在超出这个范围后，自旋锁会升级为阻塞锁。（即从轻量级锁转变为重量级锁） 轻量级锁加锁 线程在执行同步块之前，JVM会先在当前线程的栈桢中创建用于存储锁记录的空间，并将对象头中的Mark Word复制到锁记录中，官方称为Displaced Mark Word。然后线程尝试使用CAS将对象头中的Mark Word替换为指向锁记录的指针。如果成功，当前线程获得锁，如果失败，则自旋获取锁，当自旋获取锁仍然失败时，表示存在其他线程竞争锁(两条或两条以上的线程竞争同一个锁)，则轻量级锁会膨胀成重量级锁。 解锁 轻量级解锁时，会使用原子的CAS操作来将Displaced Mark Word替换回到对象头，如果成功，则表示同步过程已完成。如果失败，表示有其他线程尝试过获取该锁，则要在释放锁的同时唤醒被挂起的线程。 重量级锁重量锁在JVM中又叫对象监视器（Monitor），它很像C中的Mutex，除了具备Mutex(0|1)互斥的功能，它还负责实现了Semaphore(信号量)的功能，也就是说它至少包含一个竞争锁的队列，和一个信号阻塞队列（wait队列），前者负责做互斥，后一个用于做线程同步。 如果说轻量级锁是在无竞争的情况下使用CAS操作去消除同步使用的互斥量，那偏向锁就是在无竞争的情况下把整个同步都消除掉，连CAS都不做了。 ConcurrentHashMapJDk7版本 ConcurrentHashMap的锁分段技术可有效提升并发访问率 HashTable容器在竞争激烈的并发环境下表现出效率低下的原因是所有访问HashTable的线程都必须竞争同一把锁，假如容器中有多把锁，每一个把锁用于锁容器其中一部分数据，那么当多线程访问容器里不同数据段的数据时，线程间就不会存在锁竞争，从而可以有效提高并发访问效率，这就是ConcurrentHashMap所使用的锁分段技术。首先将数据分为一段一段地存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能够别其他数据访问。 ConcurrentHashMap的结构ConcurrentHashMap是由Segment数组结构和HashEntry数组结构组成。Segment是一种可重入锁（ReentrantLock），在ConcurrentHashMap里扮演锁的角色；HashEntry则用于存储键值对数据。Segment的结构和HashMap类似，是一种数组和链表结构。一个Segment里包含一个HashEntry数组，每个HashEntry里是一个链表结构的元素，每个Segment守护着一个HashEntry数组里的元素，当对HashEntry数组的数据进行修改时，必须首先获得与它对应的Segment锁。 getJDK1.7的ConcurrentHashMap的get操作是不加锁的，因为在每个Segment中定义的HashEntry数组和在每个HashEntry中定义的value和next HashEntry节点都是volatile类型的，volatile类型的变量可以保证其在多线程之间的可见性，因此可以被多个线程同时读，从而不用加锁。而其get操作步骤也比较简单，定位Segment –&gt; 定位HashEntry –&gt; 通过getObjectVolatile()方法获取指定偏移量上的HashEntry –&gt; 通过循环遍历链表获取对应值。 定位Segment：(((h &gt;&gt;&gt; segmentShift) &amp; segmentMask) &lt;&lt; SSHIFT) + SBASE 定位HashEntry：(((tab.length - 1) &amp; h)) &lt;&lt; TSHIFT) + TBASE put在Segment的put方法中，首先需要调用tryLock()方法获取锁，然后通过hash算法定位到对应的HashEntry，然后遍历整个链表，如果查到key值，则直接插入元素即可；而如果没有查询到对应的key，则需要调用rehash()方法对Segment中保存的table进行扩容，扩容为原来的2倍，并在扩容之后插入对应的元素。插入一个key/value对后，需要将统计Segment中元素个数的count属性加1。最后，插入成功之后，需要使用unLock()释放锁。 JDK8版本在JDK1.7之前，ConcurrentHashMap是通过分段锁机制来实现的，所以其最大并发度受Segment的个数限制。因此，在JDK1.8中，ConcurrentHashMap的实现原理摒弃了这种设计，而是选择了与HashMap类似的数组+链表+红黑树的方式实现，而加锁则采用CAS和synchronized实现。 数据结构JDK1.8的ConcurrentHashMap数据结构比JDK1.7之前的要简单的多，其使用的是HashMap一样的数据结构：数组+链表+红黑树。ConcurrentHashMap中包含一个table数组，其类型是一个Node数组；而Node是一个继承自Map.Entry&lt;K, V&gt;的链表，而当这个链表结构中的数据大于8，则将数据结构升级为TreeBin类型的红黑树结构。另外， JDK1.8中的ConcurrentHashMap中还包含一个重要属性sizeCtl，其是一个控制标识符，不同的值代表不同的意思：其为0时，表示hash表还未初始化，而为正数时这个数值表示初始化或下一次扩容的大小，相当于一个阈值；即如果hash表的实际大小&gt;=sizeCtl，则进行扩容，默认情况下其是当前ConcurrentHashMap容量的0.75倍；而如果sizeCtl为-1，表示正在进行初始化操作；而为-N时，则表示有N-1个线程正在进行扩容。 源码12345678910111213141516//内部类node里的val，next都使用了volatile关键字static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; volatile V val; volatile Node&lt;K,V&gt; next; Node(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.val = val; this.next = next; &#125; &#125;// 使用node数组， 也是用volatile数组transient volatile Node&lt;K,V&gt;[] table; get方法同1.7的一致，由于node都是使用volatile修饰，在get的源码实现中，并不用加锁。 SQL注入简介SQL注入是网站存在最多也是最简单的漏洞，主要原因是程序员在开发用户和数据库交互的系统时，没有对用户输入的字符串进行过滤，转义，限制或处理不严谨，导致用户可以通过输入精心构造的字符串去非法获取到数据库中的数据。 SQL注入原理一般用户登录用的SQL语句为：SELECT FROM user WHERE username=’admin’ AND password=’passwd’，此处admin和passwd分别为用户输入的用户名和密码，如果程序员没有对用户输入的用户名和密码做处理，就可以构造万能密码成功绕过登录验证，如用户输入‘or 1#,SQL语句将变为：SELECT FROM user WHERE username=’’or 1#’ AND password=’’，‘’or 1为TRUE，#注释掉后面的内容，所以查询语句可以正确执行。 mybatis是如何防止SQL注入的1、首先看一下下面两个sql语句的区别：123456&lt;select id="selectByNameAndPassword" parameterType="java.util.Map" resultMap="BaseResultMap"&gt;select id, username, password, rolefrom userwhere username = #&#123;username,jdbcType=VARCHAR&#125;and password = #&#123;password,jdbcType=VARCHAR&#125;&lt;/select&gt; 123456&lt;select id="selectByNameAndPassword" parameterType="java.util.Map" resultMap="BaseResultMap"&gt;select id, username, password, rolefrom userwhere username = $&#123;username,jdbcType=VARCHAR&#125;and password = $&#123;password,jdbcType=VARCHAR&#125;&lt;/select&gt; mybatis中的#和$的区别： 1、#将传入的数据都当成一个字符串，会对自动传入的数据加一个双引号。如：where username=#{username}，如果传入的值是111,那么解析成sql时的值为where username=”111”, 如果传入的值是id，则解析成的sql为where username=”id”. 2、$将传入的数据直接显示生成在sql中。如：where username=${username}，如果传入的值是111,那么解析成sql时的值为where username=111；如果传入的值是;drop table user;，则解析成的sql为：select id, username, password, role from user where username=;drop table user;3、#方式能够很大程度防止sql注入，$方式无法防止Sql注入。4、$方式一般用于传入数据库对象，例如传入表名.5、一般能用#的就别用$，若不得不使用“${xxx}”这样的参数，要手工地做好过滤工作，来防止sql注入攻击。6、在MyBatis中，“${xxx}”这样格式的参数会直接参与SQL编译，从而不能避免注入攻击。但涉及到动态表名和列名时，只能使用“${xxx}”这样的参数格式。所以，这样的参数需要我们在代码中手工进行处理来防止注入。【结论】在编写MyBatis的映射语句时，尽量采用“#{xxx}”这样的格式。若不得不使用“${xxx}”这样的参数，要手工地做好过滤工作，来防止SQL注入攻击。 2、什么是sql注入 sql注入解释：是一种代码注入技术，用于攻击数据驱动的应用，恶意的SQL语句被插入到执行的实体字段中（例如，为了转储数据库内容给攻击者） SQL**注入，大家都不陌生，是一种常见的攻击方式。攻击者在界面的表单信息或URL上输入一些奇怪的SQL片段（例如“or ‘1’=’1’”这样的语句），有可能入侵参数检验不足的应用程序。所以，在我们的应用中需要做一些工作，来防备这样的攻击方式。在一些安全性要求很高的应用中（比如银行软件），经常使用将SQL**语句全部替换为存储过程这样的方式，来防止SQL注入。这当然是一种很安全的方式，但我们平时开发中，可能不需要这种死板的方式。 3、mybatis是如何做到防止sql注入的 MyBatis框架作为一款半自动化的持久层框架，其SQL语句都要我们自己手动编写，这个时候当然需要防止SQL注入。其实，MyBatis的SQL是一个具有“输入+输出”的功能，类似于函数的结构，参考上面的两个例子。其中，parameterType表示了输入的参数类型，resultType表示了输出的参数类型。回应上文，如果我们想防止SQL注入，理所当然地要在输入参数上下功夫。上面代码中使用 # 的即输入参数在SQL中拼接的部分，传入参数后，打印出执行的SQL语句，会看到SQL是这样的： 1select id, username, password, role from user where username=? and password=? 不管输入什么参数，打印出的SQL都是这样的。这是因为MyBatis启用了预编译功能，在SQL执行前，会先将上面的SQL发送给数据库进行编译；执行时，直接使用编译好的SQL，替换占位符“?”就可以了。因为SQL注入只能对编译过程起作用，所以这样的方式就很好地避免了SQL注入的问题。 【底层实现原理】MyBatis是如何做到SQL预编译的呢？其实在框架底层，是JDBC中的PreparedStatement类在起作用，PreparedStatement是我们很熟悉的Statement的子类，它的对象包含了编译好的SQL语句。这种“准备好”的方式不仅能提高安全性，而且在多次执行同一个SQL时，能够提高效率。原因是SQL已编译好，再次执行时无需再编译。 1234567//安全的，预编译了的Connection conn = getConn();//获得连接String sql = &quot;select id, username, password, role from user where id=?&quot;; //执行sql前会预编译号该条语句PreparedStatement pstmt = conn.prepareStatement(sql); pstmt.setString(1, id); ResultSet rs=pstmt.executeUpdate(); ...... 12345678910//不安全的，没进行预编译private String getNameByUserId(String userId) &#123; Connection conn = getConn();//获得连接 String sql = &quot;select id,username,password,role from user where id=&quot; + id; //当id参数为&quot;3;drop table user;&quot;时，执行的sql语句如下: //select id,username,password,role from user where id=3; drop table user; PreparedStatement pstmt = conn.prepareStatement(sql); ResultSet rs=pstmt.executeUpdate(); ......&#125; 【 结论：】 #{}：相当于JDBC中的PreparedStatement ${}：是输出变量的值 简单说，#{}是经过预编译的，是安全的；${}是未经过预编译的，仅仅是取变量的值，是非安全的，存在SQL注入。如果我们order by语句后用了${}，那么不做任何处理的时候是存在SQL注入危险的。你说怎么防止，那我只能悲惨的告诉你，你得手动处理过滤一下输入的内容。如判断一下输入的参数的长度是否正常（注入语句一般很长），更精确的过滤则可以查询一下输入的参数是否在预期的参数集合中。 作者：淼淼之森 ### 19.09.27mybatis面试题总结 Spring常见面试 dockerDocker 可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。 容器虚拟化的是操作系统而不是硬件，容器之间是共享同一套操作系统资源的。虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统。因此容器的隔离级别会稍低一些。 容器是完全使用沙箱机制，相互之间不会有任何接口（类似 iPhone 的 app）,更重要的是容器性能开销极低。 Spring Security区分认证 (Authentication) 和授权 (Authorization)这是一个绝大多数人都会混淆的问题。首先先从读音上来认识这两个名词，很多人都会把它俩的读音搞混，所以我建议你先先去查一查这两个单词到底该怎么读，他们的具体含义是什么。 Authentication（认证） 是验证您的身份的凭据（例如用户名/用户ID和密码），通过这个凭据，系统得以知道你就是你，也就是说系统存在你这个用户。所以，Authentication 被称为身份/用户验证。 Authorization（授权） 发生在 Authentication（认证）之后。授权嘛，光看意思大家应该就明白，它主要掌管我们访问系统的权限。比如有些特定资源只能具有特定权限的人才能访问比如admin，有些对系统资源操作比如删除、添加、更新只能特定人才具有。 这两个一般在我们的系统中被结合在一起使用，目的就是为了保护我们系统的安全性。 Redis为什么要用 redis/为什么要用缓存主要从“高性能”和“高并发”这两点来看待这个问题。 高性能： 假如用户第一次访问数据库中的某些数据。这个过程会比较慢，因为是从硬盘上读取的。将该用户访问的数据存在缓存中，这样下一次再访问这些数据的时候就可以直接从缓存中获取了。操作缓存就是直接操作内存，所以速度相当快。如果数据库中的对应数据改变的之后，同步改变缓存中相应的数据即可！ 高并发： 直接操作缓存能够承受的请求是远远大于直接访问数据库的，所以我们可以考虑把数据库中的部分数据转移到缓存中去，这样用户的一部分请求会直接到缓存这里而不用经过数据库。 redis 设置过期时间Redis中有个设置时间过期的功能，即对存储在 redis 数据库中的值可以设置一个过期时间。作为一个缓存数据库，这是非常实用的。如我们一般项目中的 token 或者一些登录信息，尤其是短信验证码都是有时间限制的，按照传统的数据库处理方式，一般都是自己判断过期，这样无疑会严重影响项目性能。 我们 set key 的时候，都可以给一个 expire time，就是过期时间，通过过期时间我们可以指定这个 key 可以存活的时间。 如果假设你设置了一批 key 只能存活1个小时，那么接下来1小时后，redis是怎么对这批key进行删除的？ 定期删除+惰性删除。 通过名字大概就能猜出这两个删除方式的意思了。 定期删除：redis默认是每隔 100ms 就随机抽取一些设置了过期时间的key，检查其是否过期，如果过期就删除。注意这里是随机抽取的。为什么要随机呢？你想一想假如 redis 存了几十万个 key ，每隔100ms就遍历所有的设置过期时间的 key 的话，就会给 CPU 带来很大的负载！ 惰性删除 ：定期删除可能会导致很多过期 key 到了时间并没有被删除掉。所以就有了惰性删除。假如你的过期 key，靠定期删除没有被删除掉，还停留在内存里，除非你的系统去查一下那个 key，才会被redis给删除掉。这就是所谓的惰性删除，也是够懒的哈！ 但是仅仅通过设置过期时间还是有问题的。我们想一下：如果定期删除漏掉了很多过期 key，然后你也没及时去查，也就没走惰性删除，此时会怎么样？如果大量过期key堆积在内存里，导致redis内存块耗尽了。怎么解决这个问题呢？ redis 内存淘汰机制。 redis 持久化机制(怎么保证 redis 挂掉之后再重启数据可以进行恢复)很多时候我们需要持久化数据也就是将内存中的数据写入到硬盘里面，大部分原因是为了之后重用数据（比如重启机器、机器故障之后恢复数据），或者是为了防止系统故障而将数据备份到一个远程位置。 Redis不同于Memcached的很重一点就是，Redis支持持久化，而且支持两种不同的持久化操作。Redis的一种持久化方式叫快照（snapshotting，RDB），另一种方式是只追加文件（append-only file,AOF）。这两种方法各有千秋，下面我会详细这两种持久化方法是什么，怎么用，如何选择适合自己的持久化方法。 快照（snapshotting）持久化（RDB） Redis可以通过创建快照来获得存储在内存里面的数据在某个时间点上的副本。Redis创建快照之后，可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本（Redis主从结构，主要用来提高Redis性能），还可以将快照留在原地以便重启服务器的时候使用。 快照持久化是Redis默认采用的持久化方式，在redis.conf配置文件中默认有此下配置： 12345save 900 1 #在900秒(15分钟)之后，如果至少有1个key发生变化，Redis就会自动触发BGSAVE命令创建快照。save 300 10 #在300秒(5分钟)之后，如果至少有10个key发生变化，Redis就会自动触发BGSAVE命令创建快照。save 60 10000 #在60秒(1分钟)之后，如果至少有10000个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 AOF（append-only file）持久化 与快照持久化相比，AOF持久化 的实时性更好，因此已成为主流的持久化方案。默认情况下Redis没有开启AOF（append only file）方式的持久化，可以通过appendonly参数开启： 1appendonly yes 开启AOF持久化后每执行一条会更改Redis中的数据的命令，Redis就会将该命令写入硬盘中的AOF文件。AOF文件的保存位置和RDB文件的位置相同，都是通过dir参数设置的，默认的文件名是appendonly.aof。 在Redis的配置文件中存在三种不同的 AOF 持久化方式，它们分别是： 123appendfsync always #每次有数据修改发生时都会写入AOF文件,这样会严重降低Redis的速度appendfsync everysec #每秒钟同步一次，显示地将多个写命令同步到硬盘appendfsync no #让操作系统决定何时进行同步 为了兼顾数据和写入性能，用户可以考虑 appendfsync everysec选项 ，让Redis每秒同步一次AOF文件，Redis性能几乎没受到任何影响。而且这样即使出现系统崩溃，用户最多只会丢失一秒之内产生的数据。当硬盘忙于执行写入操作的时候，Redis还会优雅的放慢自己的速度以便适应硬盘的最大写入速度。 Redis 4.0 对于持久化机制的优化 Redis 4.0 开始支持 RDB 和 AOF 的混合持久化（默认关闭，可以通过配置项 aof-use-rdb-preamble 开启）。 如果把混合持久化打开，AOF 重写的时候就直接把 RDB 的内容写到 AOF 文件开头。这样做的好处是可以结合 RDB 和 AOF 的优点, 快速加载同时避免丢失过多的数据。当然缺点也是有的， AOF 里面的 RDB 部分是压缩格式不再是 AOF 格式，可读性较差。 缓存雪崩和缓存穿透问题解决方案缓存雪崩 简介：缓存同一时间大面积的失效，所以，后面的请求都会落到数据库上，造成数据库短时间内承受大量请求而崩掉。 解决办法（中华石杉老师在他的视频中提到过，视频地址在最后一个问题中有提到）： 事前：尽量保证整个 redis 集群的高可用性，发现机器宕机尽快补上。选择合适的内存淘汰策略。 事中：本地ehcache缓存 + hystrix限流&amp;降级，避免MySQL崩掉 事后：利用 redis 持久化机制保存的数据尽快恢复缓存 缓存穿透 简介：一般是黑客故意去请求缓存中不存在的数据，导致所有的请求都落到数据库上，造成数据库短时间内承受大量请求而崩掉。 解决办法： 有很多种方法可以有效地解决缓存穿透问题，最常见的则是采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被 这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。另外也有一个更为简单粗暴的方法（我们采用的就是这种），如果一个查询返回的数据为空（不管是数 据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。 ThreadLocal]]></content>
      <tags>
        <tag>复习知识总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络]]></title>
    <url>%2F2019%2F09%2F10%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[OSI参考模型OSI参考模型结构包括以下7层：物理层，数据链路层，网络层，传输层，会话层，表示层和应用层。 物理层实现比特流的透明传输，为数据链路层提供数据传输服务物理层的数据传输单位是比特（bit） 数据链路层数据链路层在物理层提供比特流传输的基础上，通过建立数据链路连接，采用差错控制与流量控制方法，使有差错的物理线路变成无差错的数据链路。数据链路层的数据传输单位是帧。 网络层网络层通过路由选择算法为分组通过通信子网选择适当的传输路径，实现流量控制，拥塞控制与网络互联的功能。网络层的数据传输功能是分组。 传输层传输层为分布在不同地理位置计算机的进程通信提供可靠的端-端连接与数据传输服务。传输层的数据传输单元是报文。TCP/IP参考模型TCP/IP起源目前TCP/IP已经成为公认的Internet工业标准与事实上的Internet协议标准。目前使用的TCP/IP是版本4，即IPv4。TCP/IP参考模型的层次主机-网络层与OSI参考模型的数据链路层和物理层对应。TCP/IP协议对主机-网络层并没有规定具体的协议。互联网络层（网络层）(IP)使用的是IP协议，IP是一种不可控，无连接的数据报传输服务协议，它提供的是一种“尽力而为”的服务。传输层（UDP，TCP）传输层定义两种不同的协议，传输控制层（Transport Control Protocol,TCP）与用户数据报协议（User Datagram Protocol，UDP）。TCP是一种可靠的，面向连接，面向字节流的传输层协议。TCP提供比较完善的流量控制与拥塞控制功能。UDP是一种不可靠的，无连接的传输层协议。TCP（Transmisson Control Protocol）TCP 是面向连接的（需要先建立连接）；每一条 TCP 连接只能有两个端点，每一条 TCP 连接只能是一对一；TCP提供可靠交付的服务。通过TCP连接传送的数据，无差错、不丢失、不重复、并且按序到达；TCP 提供全双工通信。TCP 允许通信双方的应用进程在任何时候都能发送数据。TCP 连接的两端都设有发送缓存和接收缓存，用来临时存放双方通信的数据；面向字节流。TCP 中的“流”（Stream）指的是流入进程或从进程流出的字节序列。UDP（User Datagram Protocol）UDP 是无连接的；UDP 是尽最大努力交付，即不保证可靠交付，因此主机不需要维持复杂的链接状态；UDP 是面向报文的；UDP 没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（对实时应用很有用，如直播，实时视频会议等）；UDP 支持一对一、一对多、多对一和多对多的交互通信；UDP 的首部开销小，只有 8 个字节，比 TCP 的 20 个字节的首部要短。 1231.单工数据传输只支持数据在一个方向上传输2.半双工数据传输允许数据在两个方向上传输，但是，在某一时刻，只允许数据在一个方向上传输，它实际上是一种切换方向的单工通信；3.全双工数据通信允许数据同时在两个方向上传输，因此，全双工通信是两个单工通信方式的结合，它要求发送设备和接收设备都有独立的接收和发送能力。 应用层()应用层包括各种标准的网络应用协议，并且总有不断有新的协议加入。TCP/IP应用层基本的协议主要有： 远程登录协议（TELNET） 文件传输协议（File Transfer Protocol,FTP） 超文本传输协议（HTTP） 域名服务协议（DNS）IPv4协议的基本内容IP协议的主要特点 IP协议是一种无连接，不可靠的分组传送服务的协议。 无连接意味着IP协议并不维护IP分组发送后的任何状态信息。每个分组的传输过程是相互独立的。 不可靠意味着IP协议不能保证每个IP分组都能够正确地，不丢失和顺序地到达目的主机。 IP协议是点-点的网络层通信协议。网络层需要在Internet中为通信的两个主机之间寻找一条路径，而这条路径通常是由多个路由器，点-点链路组成。因此，IP协议是针对源主机-路由器，路由器-路由器，路由器-目的主机之间的数据传输的点-点线路的网络层通信协议。IP地址的点分十进制的表示方法“网络号-主机号”的两级IP地址结构。IPv4的地址长度为32位，用点分十进制表示。通常采用x.x.x.x的格式来表示，每个x为8位，每个X的值为0~255. 标准IP地址的分类 A类地址A类地址网络号的第一位为0，其余的7位可以分配，0 网络号（7位），主机号（24位）。A类地址共分为大小相同的128（2^7 = 128）,每一块的netID不同。 B类地址B类地址的前两位为10，其余14位可以分配，可分配的网络号为2^14。B类地址的主机号长度为16位。10 网络号（14） 主机号（16位） C类地址C类地址的前三位为110，其余的21位可以分配。主机号为8位。 D类地址前4位为1110 E类地址前5位为11110。特殊地址形式特殊的IP地址包括以下四种类型 直接广播地址在A类，B类与C类IP地址中，如果主机号是全1，那么这个地址为直接广播地址，路由器将这个分组以广播方式发送给特定网络的所有主机。 受限广播地址32位网络号与主机号为全1的IP地址（255.255.255.255）为受限广播地址。它是用来将一个分组以广播方式发送给本网络中的所有主机。 “这个网络上的特定主机”地址在A类，B类，C类IP地址中，如果网络号是全0（如0.0.0.25），该地址是这个网络上的特定主机地址。划分子网的三级地址结构子网划分的基本思想是：借用主机号的一部分作为子网的子网号，划分出更多的子网IP地址。IPv6地址IPv6的128地址按每16位划分为一个位段，每个位段被转换为一个4位的十六进制数，并用冒号隔开，这种表示法称为“冒号十六进制表示法”。x:x:x:x:x:x:x:x零压缩法双冒号在一个地址中只能出现一次，如何确定双冒号之间被压缩0的位数？可以数一下地址中还有多少个位段，然后用8减去这个数，再将结果乘以16。例如，在地址FF02:3::5中有三个位段，可以根据公式计算：(8-3)*16=80,则 ::表示有80位的二进制数字0被压缩。2]]></content>
      <categories>
        <category>复习</category>
      </categories>
      <tags>
        <tag>复习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[事务]]></title>
    <url>%2F2019%2F09%2F10%2F%E4%BA%8B%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[事务的概念事务就是一组原子性的sql,或者说一个独立的工作单元。事务就是说，要么MySql引擎会全部执行这一组SQL语句，要么全部都不执行。 事务的ACIDAtomicity，原子性：一个事务必须保证其中的操作要么全部执行，要么全部回滚，不可能存在只执行了一部分这种情况出现。Consistency，一致性：事务必须保证从一种一致性状态转换为另一种一致性状态。Isolation,隔离性：事务互相隔离，在一个事务未执行完毕时，通常会保证其他Session无法看到这个事务的执行结果。Durability，持久性：事务一旦commit，则数据就会保存下来，即使提交完之后系统崩溃，数据也不会丢失。 隔离级别以下，我们来详细来说一说隔离性我们都知道，事务控制的太严格，程序在并发訪问的情况下，会减少程序的性能。所以。人们总是想让事务为性能做出让步。那么就分出了四种隔离级别：为未提交读、提交读、可重复度读、序列化。可是。因为隔离界别限制的程度不同，那么就会产生脏读、不可反复读、幻读的情况。 脏读：脏读就是指当一个事务正在訪问数据，而且对数据进行了改动，而这样的改动还没有提交到数据库中，这时。另外一个事务也訪问这个数据，然后使用了这个数据。 不可反复读：是指在一个事务内，多次读同一数据。在这个事务还没有结束时。另外一个事务也訪问该同一数据。那么，在第一个事务中的两次读数据之间，因为第二个事务的改动，那么第一个事务两次读到的的数据可能是不一样的。这样就发生了在一个事务内两次读到的数据是不一样的，因此称为是不可反复读。比如。一个编辑人员两次读取同一文档，但在两次读取之间，作者重写了该文档。当编辑人员第二次读取文档时，文档已更改。原始读取不可反复。假设仅仅有在作者所有完毕编写后编辑人员才干够读取文档，则能够避免该问题。 幻读:是指当事务不是独立运行时发生的一种现象，比如第一个事务对一个表中的数据进行了改动，这样的改动涉及到表中的所有数据行。同一时候。第二个事务也改动这个表中的数据。这样的改动是向表中插入一行新数据。那么，以后就会发生操作第一个事务的用户发现表中还有没有改动的数据行，就好象发生了幻觉一样。比如。一个编辑人员更改作者提交的文档。但当生产部门将其更改内容合并到该文档的主复本时，发现作者已将未编辑的新材料加入到该文档中。事务并发下隔离级别的场景 未提交读公司发工资了，领导把5000元打到singo的账号上，但是该事务并未提交，而singo正好去查看账户，发现工资已经到账，是5000元整。很高兴。但是不幸的是。领导发现发给singo的工资金额不正确。是2000元。于是迅速回滚了事务，改动金额后，将事务提交，最后singo实际的工资仅仅有2000元，singo空欢喜一场。出现上述情况，即我们所说的脏读。两个并发的事务，“事务A：领导给singo发工资”、“事务B：singo查询工资账户”，事务B读取了事务A尚未提交的数据。当隔离级别设置为Readuncommitted时，就可能出现脏读，怎样避免脏读。请看下一个隔离级别。 读提交singo拿着工资卡去消费。系统读取到卡里确实有2000元。而此时她的老婆也正好在网上转账，把singo工资卡的2000元转到还有一账户，并在singo之前提交了事务，当singo扣款时，系统检查到singo的工资卡已经没有钱，扣款失败，singo十分纳闷，明明卡里有钱，为何……出现上述情况，即我们所说的不可反复读。两个并发的事务，“事务A：singo消费”、“事务B：singo的老婆网上转账”。事务A事先读取了数据。事务B紧接了更新了数据，并提交了事务，而事务A再次读取该数据时，数据已经发生了改变。当隔离级别设置为Readcommitted时，避免了脏读。可是可能会造成不可反复读。大多数数据库的默认级别就是Readcommitted。比方Sql Server , Oracle。怎样解决不可反复读这一问题。请看下一个隔离级别。 反复读当隔离级别设置为Repeatableread时。能够避免不可反复读。当singo拿着工资卡去消费时。一旦系统開始读取工资卡信息（即事务開始）。singo的老婆就不可能对该记录进行改动，也就是singo的老婆不能在此时转账。尽管Repeatableread避免了不可反复读，但还有可能出现幻读。singo的老婆工作在银行部门。她时常通过银行内部系统查看singo的信用卡消费记录。有一天，她正在查询到singo当月信用卡的总消费金额（select sum(amount) from transaction where month =本月）为80元。而singo此时正好在外面胡吃海塞后在收银台买单。消费1000元，即新增了一条1000元的消费记录（insert transaction… ）。并提交了事务。随后singo的老婆将singo当月信用卡消费的明细打印到A4纸上。却发现消费总额为1080元，singo的老婆非常诧异，以为出现了幻觉。幻读就这样产生了。注：Mysql的默认隔离级别就是Repeatableread。 序列化Serializable是最高的事务隔离级别。同一时候代价也花费最高，性能非常低，一般非常少使用，在该级别下，事务顺序运行，不仅能够避免脏读、不可反复读，还避免了幻像读。mysql开启事务默认情况下，mysql每执行一条sql语句都是一条事务，自动提交。如果一条事务有多条sql语句需要执行，要开启一个新的事务12345678910开启事务start transaction....结束事务commit/rollback或 通过AutoCommit设置事务开启或关闭show variables like "autocommit"set autocommit = 0; //0表示AutoCommit关闭set autocommit = 1； //1表示AutoCommit开启 在执行SQL语句之前，先执行start transaction，这就开启了一个事务（事务的起点），然后可以去执行多条SQL语句，最后要结束事务，commit表示提交，即事务中的多条SQL语句所作出的影响会持久到数据库中，或者rollback，表示回滚到事务的起点，之前做的所有操作都被撤销了。 jdbc 开启事务默认自动提交事务，若要手动提交，则关闭事务。在JDBC中处理事务，都是通过Connection完成的。同一事务中所有的操作，都在使用同一个Connection对象。1234con.setAutoCommit(false) 表示开启事务。...(进行多条sql)commit（）：提交结束事务。rollback（）：回滚结束事务。 附上jdbc代码练习12345678910111213141516171819202122232425262728public static void main(String[] args) throws ClassNotFoundException &#123; Connection conn= null; String driver = "com.mysql.jdbc.Driver"; //时区问题，serverTimezone=UTC String url = "jdbc:mysql://localhost:3306/czt?characterEncoding=utf-8&amp;useSSL=false&amp;serverTimezone=UTC"; String name = "root"; String password = "666666"; //先获取链接驱动 Class.forName(driver); String sql =null; try &#123; //通过driverManager获取链接 conn = DriverManager.getConnection(url, name, password); if (conn != null )&#123; System.out.println("成功连接"); &#125; //通过链接，发送statement preperStatement的区别是，可以预编译sql语句，再注入参数， Statement statement = conn.createStatement(); sql = "create table student(NO char(20),name varchar(20),primary key(NO))"; //返回影响行数 int i = statement.executeUpdate(sql); if (i!=-1)&#123; System.out.println("修改成功"); &#125; conn.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; spring 开启事务spring 事务管理详解在xml配置文件中第一步：配置事务管理器第二步：开启事务注解 12345678910&lt;!-- 第一步：配置事务管理器 (和配置文件方式一样)--&gt; &lt;bean id="dataSourceTransactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager"&gt; &lt;!-- 注入dataSource --&gt; &lt;property name="dataSource" ref="dataSource"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 第二步： 开启事务注解 --&gt; &lt;tx:annotation-driven transaction-manager="dataSourceTransactionManager" /&gt; &lt;!-- 第三步 在方法所在类上加注解 --&gt;]]></content>
      <categories>
        <category>事务</category>
      </categories>
      <tags>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图]]></title>
    <url>%2F2019%2F09%2F07%2F%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[图是大大大]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[位运算符]]></title>
    <url>%2F2019%2F09%2F07%2F%E4%BD%8D%E8%BF%90%E7%AE%97%E7%AC%A6%2F</url>
    <content type="text"><![CDATA[二进制的正负二进制的正负是从高位看，最高位如果1则是负数，如果是0则是正数。如果负数单纯是把最高位变为1的话，在运算中会出现不是我们想要的值，所以引入了：原码，反码，补码。正数的原码，反码，补码都一样，负数的反码是对除了符号位（最高位）对原码取反，补码是对反码+1（最高位不变）-5的原码是 1000 0000 0000 0101求出反码的是 1111 1111 1111 1010求出补码是 1111 1111 1111 1011 JAVA按位运算符1、“与”、“位与”（&amp;）按位“与”操作符，如果两个数的二进制，相同位数都是1，则该位结果是1，否则是0.例1 5&amp;45的二进制是 0000 0000 0000 01014的二进制是 0000 0000 0000 0100则结果是 0000 0000 0000 0100 转为十进制是4。2、“或”、“位或”（|）按位“或”操作符，如果两个数的二进制，相同位数有一个是1，则该位结果是1，否则是0例2 5 | 45的二进制是 0000 0000 0000 01014的二进制是 0000 0000 0000 0100则结果是 0000 0000 0000 0101 转为十进制是5。3、“异或、“位异或”（^）按位“异或”操作符，如果两个数的二进制，相同位数只有一个是1，则该位结果是1，否则是0（相同为0，不同为1） 例3 5 ^ 45的二进制是 0000 0000 0000 01014的二进制是 0000 0000 0000 0100则结果是 0000 0000 0000 0001 转为十进制是14、“非”、“位非”（~）也称为取反操作符按位“非”操作符，属于一元操作符，只对一个操作数进行操作，（其他按位操作符是二元操作符）。按位“非”生成与输入位相反的值，——若输入0，则输出1，若输入1，则输出0。例4 ~55的二进制是 0000 0000 0000 0101则~5是 1111 1111 1111 1010 转为十进制是 -6。 JAVA位运算符移位操作符操作的运算对象也是二进制的“位” 移位操作符只可用来处理整数类型，左移位操作符（&lt;&lt;）能按照操作符右侧指定的位数将操作符左边的操作数向左移动（在低位补0） 有符号”右移位操作符（&gt;&gt;）则按照操作符右侧指定的位数将操作符左边的操作数向右移。“有符号”右移位操作符使用“符号扩展”；若符号位正，则在高位插入0；若符号位负。则在高位插入1。 java中增加了一种“无符号”右移位操作符（&gt;&gt;&gt;）,他使用“零扩展”；无论正负，都在高位插入0。这一操作符是C或C++中所没有的。例6 5&lt;&gt;2 等于 15的二进制是 0000 0000 0000 0101右移两位 0000 0000 0000 0001例8 -5&gt;&gt;2 等于 -2 计算机中对于负数的计算都是通过补码。-5的二进制是 1111 1111 1111 1011右移两位 1111 1111 1111 1110 转十进制，例5反着来，先-1，然后取反 ————————————————版权声明：本文为CSDN博主「mxiaoyem」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/mxiaoyem/article/details/78569782 JAVA 基础数据类型的转换java语言提供了八种基本类型。六种数字类型（四个整数型，两个浮点型），一种字符类型，还有一种布尔型。自动转换按从低到高的顺序转换。int —-&gt; long —-&gt;float —-&gt; double int：int 数据类型是32位、有符号的以二进制补码表示的整数；最小值是 -2,147,483,648（-2^31）；最大值是 2,147,483,647（2^31 - 1）；一般地整型变量默认为 int 类型；默认值是 0 ；例子：int a = 100000, int b = -200000。 long：long 数据类型是 64 位、有符号的以二进制补码表示的整数；最小值是 -9,223,372,036,854,775,808（-2^63）；最大值是 9,223,372,036,854,775,807（2^63 -1）；这种类型主要使用在需要比较大整数的系统上；默认值是 0L；例子： long a = 100000L，Long b = -200000L。“L”理论上不分大小写，但是若写成”l”容易与数字”1”混淆，不容易分辩。所以最好大写。 float：float 数据类型是单精度、32位、符合IEEE 754标准的浮点数；float 在储存大型浮点数组的时候可节省内存空间；默认值是 0.0f；浮点数不能用来表示精确的值，如货币；例子：float f1 = 234.5f。 double：double 数据类型是双精度、64 位、符合IEEE 754标准的浮点数；浮点数的默认类型为double类型；double类型同样不能表示精确的值，如货币；默认值是 0.0d；例子：double d1 = 123.4。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发艺术编程]]></title>
    <url>%2F2019%2F09%2F03%2F%E5%B9%B6%E5%8F%91%E8%89%BA%E6%9C%AF%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[启动一个java程序，操作系统就会创建一个进程，一个进程可以创建多个线程，这些线程都用友各自的计数器，堆栈和局部变量等属性，访问共享的内存变量。JAVA程序天生就是多线程程序，执行main（）方法的是一个名为main的线程 为什么要多线程 更多的处理器核心 更多的响应时间 更好的编程模型线程优先级通过一个整型变量priority控制优先级，thread.setPriority(priority)。 默认优先级 ： 5 范围从 1-10，优先级逐级增高 对于优先级的选择 对于频繁阻塞的任务（休眠 或 I/O操作） ，优先级应该高。对于偏重计算的任务（需要较多的CPU） , 优先级应该低，确保处理器不会被独占注意 ：线程优先级不能作为程序正确性的依赖，因为操作系统可以完成不用理会JAVA线程对于优先级的设定，对线程优先级的设置会被忽略。线程中断中断好比其他线程对该线程打了个招呼，其他线程通过调用该线程的interrupt()方法对其进行了中断操作。等待/通知机制等待/通知机制，是指一个线程A调用了对象O的wait（）方法进入等待状态，而另一个线程B调用了对象O的notify（）或者notifyAll（）方法，线程A收到通知后从对象O的wait（）方法返回，进而执行后续操作。notify（）：通知一个在对象上等待的线程，使其从wait()返回，前提是获得对象的锁notifyAll（）：通知所有等待在该对象上的线程wait（）：调用该方法的线程进入waiting状态，只有等待另外线程的通知或中断才会返回，需要注意，调用wait（）方法后，会返回对象的锁。 使用wait() , notify() 和 notify() 时需要先对调用对象加锁。 notify（）或notifyAll（）方法调用后，等待线程依旧不会从wait返回，需要调用notify（）或notifyAll（）的线程释放锁后，才有机会。 notify()方法将等待队列中的一个等待线程从等待队列中移到同步队列中，而notifyAll（）方法则是将等待队列中所有线程全部移到同步队列中，被移动的线程状态由waiting变为blocked 从wait（）方法返回的前提是获得了调用对象的锁。volatile关键字 —最轻量级的同步机制当一个变量被定义为volatile，两种特性 保证此变量对所有线程的可见性“可见性” ：当一个线程修改此变量，其他线程是立即得知新值。但由于JAVA里面的运算并非原子操作，导致volatile变量在并发下也并不安全。（例如 a++）。具有可见性和原子性，但类似于volatile++这种复合操作不具有原子性。通过字节码可以知道，其指令多了一个Lock前缀，使得本CPU的cache写入内存，该写入使得别的CPU无效其Cache，会重新从内存中读取，保证可见性。 禁止指令重排序优化（会干扰并发）保证变量赋值操作的顺序与程序代码中的执行顺序一致。通过内存屏障，使重排序时后面的指令不能比屏障之前的先执行。意味着所有之前的操作都已经执行完成。重排序：编译器和处理器为了优化程序性能而对指令序列进行重新排序的一种手段。Synchronized关键字，最基本的互斥同步手段Synchronize经过编译后，会在同步块前后分别形成两个monitorenter，monitorexit两个字节码指令。工作原理 –&gt;monitorenter –&gt;尝试获取对象的锁 —&gt;失败，线程阻塞—- &gt;成功，对象没被锁定，当前线程拥有对象的锁，锁的计数器加一— &gt; monitorexit —&gt;锁计数器减一 ，当计数器为0，锁就被释放 —&gt;唤醒被阻塞的线程锁释放 - 获取的内存语义与volatile写 - 读的内存语义是相同的 线程A释放一个锁（写一个volatile变量），实质上线程A向接下来将要获取这个锁的某个线程发出了（线程A对共享变量所做修改的）消息。 线程B获取了一个锁（读一个volatile变量），实质上是线程B接收了之前某个线程发出的（在释放之前对共享变量所做修改的）消息。 线程A释放锁，随后线程B获取锁，这个过程实质上是线程A通过主内存向线程B发送消息。实现多线程的几种方法 继承Thread类通过JDK提供的Thread类，重写Thread类run方法即可 123class Thread1 extends Thread&#123;...&#125;//启动 new Thread1().start; 实现Runnable接口 Runnable接口中仅定义一个run（）方法 1234class Thread2 implements Runnable&#123; @Override public void run()&#123;...&#125;&#125; 使用内部类的方式上面两种方式都需要再定义一个类，显得麻烦，通过匿名内部类实现，依然有两种 继承Thread a 实现 Runnable b 1234new Thread(new Runnable()&#123; @Override public void run()&#123;&#125;&#125;).start; 带返回值的callable实现callable接口 基于线程池的方式Thread.join()若一个线程A执行了thread.join()，含义是当前线程A等待Thread线程终止后才从join方法返回。锁锁的内部实现依赖于队列同步器同步器底层通过一个双向队列，存放工作队列，当一个线程成功获取同步状态，其他线程将无法获取到同步状态，转而被构造成为节点并加入同步队列中。可重入锁 自己可以再次获取自己的内部锁，任意线程在获取到锁之后能够再次获取该锁而不会被该锁所阻塞。Synchronize 和 基于Lock 实现的ReentrantLock都是可重入锁。 可重入锁 有两个特性 线程再次获取锁锁需要识别获取锁的线程是否是当前占据锁的线程，如果是，可再成功获取 锁的最终释放重复n次获取锁，在第n次释放锁后，其他线程才能获取到该锁 读写锁维护一对锁，一个读锁，一个写锁。通过分离读锁，写锁提高效率。JAVA并发包提供读写锁的实现是 ReetrantReadWriteLock。 123ReetrantReadWriteLock rwl = new ReetrantReadWriteLock();Lock r = rwl.readLock(); // 读锁Lock w = rwl.writeLock(); // 写锁 公平性选择 默认是非公平性 重进入锁 锁降级 先拿读锁（保证可见性） 放读锁，拿写锁，再拿读锁。读写锁的实现是通过int变量维护多种状态，读写锁将变量切分，高16位表示读，低16位表示写。ReentrantLock ***重点 通过调用Lock（）方法获取锁，unlock（）释放锁 12ReentrantLock lock = new ReentrantLock();lock.lock() ; lock.unlock(); 实现依赖于AQS框架，使用一个整型的volatile变量（state）来维持同步 **重点 公平锁的实现 *重点happens-before程序顺序规则是JMM核心的概念JMM通过happens-before来指定两个操作之间的执行顺序定义： ①如果一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个可见，且第一个操作的执行顺序排在第二个之前 ———————-对程序员可见②如果一个操作happens-before另一个操作，并不意味着必须按照happens-before关系指定的顺序执行，如果重排序之后的执行结果，与按happens-before顺序结果一致，那么这种重排序并不违法。 ————————对编译器和处理器的约束原则CAS 比较与替换是设计并发算法时用到的技术CAS是使用一个期望值和一个变量的当前值进行比较，如果当前变量的值与我们期望的值相等，就使用一个新值替换当前变量的值1234567public class MyLock&#123; private AtomicBoolean locked = nnew AtomicBoolean(false); public boolean lock()&#123; return locked.compareAndSet(false,true); &#125; //比较locked 和 false ，如果相等，则把它修改成true&#125; ConcurrentHashMap 是线程安全且高效的HashMapConcurrentHashMap的锁分段技术可有效提升并发访问率 与HashTable容器相比，HashTable容器并发环境效率低下是所有访问HashTable的线程都必须访问同一把锁。假如容器中有很多锁，每把锁用于锁容器其中一部分数据，那么当多线程访问容器里不同数据段的数据时，线程间就不存在竞争。这就是ConcurrentHashMap的锁分段技术。 ConcurrentHashMap的结构是有Segment数组结构 和 HashEntry 数组结构组成 Segment是一种可重入锁，结构和HahMap类似，是一种数组和链表结构。 一个ConcurrentHashMap里包含一个Segment数组，扮演锁的角色。 每个HashEntry是一个链表结构的元素，一个Segment里包含一个HashEntry数组。 每个Segment守护着一个HashEntry数组里的元素，当对HashEntry数组的数据进行修改时，必须首先获得与他对应的Segment锁。get操作 在定位Segment时，都会对元素的hashcode进行一次再散列。在HashTable中，get方法是需要加锁的，但在ConcurrentHashMap中的get操作是不用加锁的。原因： 在它的get方法中将要使用的共享变量都定义成volatile类型。能够在线程之间保持可见性，能够被多线程读，但只能被单线程写（有种情况可被多线程写，就是写入的值不依赖原值）。就像有多线程写， 也能get（）到最新的值。根据JMM的happens-before规则，对volatile的写入操作时优先于读操作的。这是用volatile替换锁的经典场景。put操作put方法首先定位到Segment,然后在Segment里进入插入操作。ConcurrentHashMap的扩容首先会创建一个容量是原来容量两倍的数组，然后将原来的元素再散列插入到新数组中。为了高校，ConcurrentHashMap不会对整个容器进行扩容，而只对某个Segment进行扩容。比HashMap更高校，HashMap是在插入后进行判断是否需要扩容， 但扩容后可能就不插入新元素。浪费了扩容的其他空间。阻塞队列（BlockingQueue）是一个支持两个附加操作的队列，这两个附加的操作支持阻塞的插入和移除 支持阻塞的插入方法，意思是当队列满时，队列会阻塞插入元素的线程，直到队列不满。 支持阻塞的移除方法，当队列为空时，会阻塞移除元素的线程，等待队列不为空。并发工具类等待多线程完成的CountDownLatchCountDownLatch允许一个或多个线程等待其他线程完成操作。假如需求，解析一个Excel里多个sheet的数据，每个线程解析一个sheet，等所有的sheet都解析完，程序需要提示解析完成。实现主线程等待所有线程完成sheet的解析，最简单是使用join（）方法1234Thread1.start();Thread2.start();Thread1.join(); //主线程需等待join线程执行结束。THread2.join(); CountDownLatch c = new CountDownLatch(2);CountDownLatch接收一个int类型的参数作为计数器，如果想等待N个点完成，这里就传入N。调用c.countDown()方法，N就会-1,c.await()方法会阻塞当前线程，直到N变成0； 同步屏障 CyclicBarrier让一组线程到达一个屏障（同步点）时被阻塞，直至最后一个屏障到达才会开门，所有被屏障拦截的线程才会继续运行。每个线程调用await（）告诉其已到屏障。new CyclicBarrier（2）; 如果修改成3，但只有两个线程调用await（），主线程和子线程和永远等待，因为没有第3个线程执行await方法，即没有第3个线程到达屏障，所以之前到达屏障的线程都不会执行。 CountDownLatch 和 CyclicBarrier的区别CountDownLatch的计数器只能使用一次， CyclicBarrier的计数器可以使用reset（）重置。 线程池的实现原理 线程池判断核心线程池里的线程是否都在执行任务（运行的线程少于corePoolSize，核心线程池还能添加工作线程），则创建新的工作线程来执行任务。（线程池创建线程时，会将线程封装成工作线程worker，Worker在执行任务后，还会循环获取工作队列里的任务来执行）若工作线程都在执行任务，且核心线程池已满，则进入下个流程。注意：创建新线程这一步需要获取全局锁，消耗资源。 线程池判断能否将任务加入工作队列，可以则加入工作队列FIFO，不可以则创建新线程。 如果创建新线程将使当前运行的线程超过MaxnumPoolSize（线程池大小），则交由饱和策略。线程池的创建 通过ThreadPoolExecutor来创建new ThreadPoolExecutor（corePoolSize–线程池基本大小，MaxnumPoolSize—线程池最大数量，keepAliveTime，millsecond，runnablequeue—保存任务的阻塞队列，hanlder—饱和策略）向线程池提交任务 threadPool.execute (new Runnable(){…})execute()方法用于提交不需要返回值的任务。（无法判断任务是否成功） Future(Object) future = threadPool.submit(new Runnable{})submit()方法用于提交需要返回值的任务，会返回一个future对象，并且可以通过future的get（）方法来获取返回值，get（）会阻塞当前线程直到任务完成。关闭线程池threadPool.shutdown();原理：遍历工作线程，逐个调用线程的interrupt方法中断线程。]]></content>
      <categories>
        <category>并发</category>
      </categories>
      <tags>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序]]></title>
    <url>%2F2019%2F09%2F02%2F%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[排序的稳定性假设在排序前的序列中ri 领先于rj(即i&lt;j)。如果排序后ri仍领先于rj，则称所用的排序方法是稳定的；反之，若可能使得排序后的序列中rj领先ri,则称所用的排序方法是不稳定的。 冒泡排序冒泡排序一种交换排序，它的基本思想是：两两比较相邻记录的关键字，如果反序则交换，直到没有反序的记录为止。1234567891011//对顺序表做冒泡排序void BubbleSort(SqList list)&#123; int i,j; for (i = 1;i&lt;list.length;i++)&#123; for (j = list.length-1;j&gt;=i;j--)&#123; //注意j是从后往前递减 if(list[j-1]&gt;list[j])&#123; //若前者大于后者 swap(list,j-1,j); //交换前者和后者的位置 &#125; &#125; &#125;&#125; 较小的数字如同气泡般慢慢浮到上面，因此将此算法命名为冒泡排序。 冒泡排序优化如果待排序的序列是{2,1,3,4,5,6,7,8}，也就是说，除了第一和第二的关键字需要交换外，别的已经是正常的顺序。当i=1时，交换了2和1，此时序列已将有序，但是算法仍然将i=2到8都执行了一遍，尽管没有交换数据，但是之后的大量比较大大的多余。当i=2时，我们已经对9与8,8与7……3与2作了比较，没有任何数据交换，这就说明此序列已将有序，不需要再继续后面的循环判断工作。为了实现这个想法，需要改进代码，增加一个标记变量flag来实现算法的改进。12345678910111213void BubbleSort2(SqList list)&#123; int i,j; Boolean flag = false; //flag用来做标记 for (i = 1;i&lt;list.length &amp;&amp; flag;i++)&#123; //若flag为false则退出循环 flag = false; //初始flag为false for (j = list.length-1;j&gt;=i;j--)&#123; if(list[j-1]&gt;list[j])&#123; swap(list,j-1,j); flag = true; //若发生交换，flag=true &#125; &#125; &#125;&#125; 复杂度分析最好的情况，排序的表本身就是有序的，根据最后改进的代码，可以推断出就是n-1次的比较，没有数据交换，时间复杂度是O(n)。最坏的情况，即排序表时逆序的情况，此时需要比较1+2+3+4+5…+（n-1）= n(n-1)/2次，因此，总的时间复杂度为O(n2)。 简单选择排序选择排序法的初步思想，冒泡排序的思想是不断的交换，通过交换完成最终的排序，我们可以在排序时找到合适的关键字再做交换，并且只移动一次就完成相应关键字的排序定位工作。简单选择排序法就是通过n-i次关键字间的比较，从n-i+1个记录中选出关键字最小的记录，并和第i个记录交换之。123456789101112131415/*对顺序表L作简单选择排序*/void SelectSort(SqList list)&#123; int i,j,min; for (i=1;i&lt;list.length;i++)&#123; min = i; //将当前下标定义为最小值下标 for(j=i+1;j&lt;list.length;j++)&#123; if (list[min] &gt; list[j])&#123; //从n-i+1中选出关键字最小的记录 min = j; //将此关键字的下标赋值给min &#125; &#125; if (min != i)&#123; //若min不等于i,找到最小值，交换 swap(list,i,min); //交换list[i]和list[min]的值 &#125; &#125;&#125; 简单选择排序复杂度分析从过程来看，它最大的特点就是交换移动数据次数相当少，这样也就节约了相应的时间。分析复杂度发现，无论最好最坏情况，其比较次数都是一样的多，第i趟排序需要进行n-i次关键字的比较，此时需要比较n-1+n-2+…=1 = n(n-1)/2次。因此，总的时间复杂度依然是O(n2).应该说，尽管与冒泡排序同为O(n2),但简单选择排序的性能上还是要略优于冒泡排序。 直接插入排序直接插入排序的基本操作是将一个记录插入到已经排好序的有序表中，从而得到一个新的，记录数增1的有序表。12345678910111213void InsertSort(SqList list)&#123; int i,j; //0的位置当成哨兵，假设list[1]已经排好位置，后面的牌其实就是插入到它的左侧还是右侧的问题 for (i = 2,j&lt;=list.length;i++)&#123; if (list[i] &lt; list[i-1])&#123; //升序，需将list[i]插入有序子表 list[0] = list[i]; //设置哨兵 for (j=i-1;list[j]&gt;list[0];j--) &#123; list[j+1] = list[j]; //记录后移 &#125; list[j+1] = list[0]; //插入到正确位置 &#125; &#125;&#125; 直接插入排序时间复杂度分析最好的情况，也就是要排序的表本身就是有序的，那么比较次数，其实就是代码第6行每个list[i]与list[i-1]的比较。时间复杂度为O(n)。最坏的情况，2+3+4+…+n=(n+2)(n-1)/2,直接插入排序的时间复杂度为O(n2),同样的O(n2)时间复杂度，直接插入排序法比冒泡和简单选择排序的性能要好一些。 希尔排序在这之前排序算法的时间复杂度基本都是O(n2)，希尔排序算法是突破这个时间复杂度的第一批算法之一。在直接插入排序的基础上进行改进。将原本有大量记录数的记录进行分组，分割成若干序列，采取跳跃分割的策略：将相距某个“增量”的记录组成一个子序列，这样才能保证在子序列内分别进行直接插入排序后得到的结果是基本有序而不是局部有序。12345678910111213141516171819//对顺序表L做希尔排序//将关键字较小的记录，不是一步一步地往前挪动，而是跳跃式地往前移，使得每完成一轮循环后，整个序列就朝着有序坚持地迈进了一步void SheelSort(SqList list)&#123; int i,j; int increment = list.length(); do &#123; increment = increment/3 +1; //增量序列 for(i=increment+1;i&lt;=list.length();i++)&#123; if (list[i] &lt; list[i-increment])&#123; //跳跃判断 list[0] = list[i]; //暂存在list[0] for (j = i-increment;j&gt;0&amp;&amp;list[0]&lt;list[i];j-=increment)&#123; list[j+increment] = list[j]; //记录后移，查找插入位置 &#125; list[j+increment] = list[0]; //插入 &#125; &#125; &#125;while(increment&gt;1); //当增量为1时，就停止循环&#125; 希尔排序复杂度分析希尔排序的关键并不是随便分组后各自排序，而是将相隔某个“增量”的记录组成一个子序列，实现跳跃式的移动，使得排序的效率提高。其时间复杂度为O(n3/2)，要好于直接排序的O(n2)，需要注意的是，增量序列的最后一个增量值必须等于1才行，另外记录是跳跃式的移动，希尔排序并不是一种稳定的排序算法。 堆排序前面讲到简单选择排序，他在待排序的n个记录中选择一个最小的记录需要比较n-1次，本来可以理解，查找第一个数据需要比较这么多次是正常的，可惜的是，这样的操作并没有把每一趟的比较结果保存下来，在后一趟的比较中，有许多比较在前一趟已经做过了，但由于前一趟排序时未保存这些比较结果，所以后一趟排序时又重复执行了这些比较操作，因而记录的比较次数较多，如果可以做到每次在选择到最小记录的同时，并根据比较结果对其他记录做出相应的调整，那排序的总体效率就会非常高。而堆排序就是对简单排序进行的一种改进。同时，他们发明了“堆”这样的数据结构。 堆堆是具有下列性质的完成二叉树：每个结点的值都大于或等于其左右孩子结点的值，称为大顶堆。或者每个结点的值都小于或等于其左右孩子结点的值，称为小顶堆。根结点一定是堆中所有结点最大（小）者。如果按照层序遍历的方式给结点从1开始编号，则结点之间满足如下关系：完全二叉树的特性，下标i与2i和2i+1是双亲子女关系。核心 大顶堆：ki &gt;= k2i , ki &gt;= k2i+1 小顶堆： ki &lt;=k2i , ki&lt;=k2i+1 ( 1&lt;= i &lt;= n/2 )堆排序利用此规则，完成排序。 堆排序算法将要排序的数组，看成是按照层序遍历的完全二叉树。堆排序就是利用堆（假设利用大顶堆）进行排序的方法。它的基本思想是，将待排序的序列构造成一个大顶堆，此时整个序列的最大值就是堆顶的根结点。将它移走（其实就是将其与堆数组的末尾元素交换，此时末尾元素就是最大值），然后将剩余的n-1个序列重新构造成一个堆，这样就会得到n个元素的次大顶堆，如此反复执行，便能得到一个有序序列。12345678910111213141516171819//本函数调整list[s]的关键字，使list[s..m]成为一个大顶堆//s 根节点 ，m 数组长度void HeadAdjust(SqlList list,int s,int m)&#123; int temp,j; temp = list[s]; for (j=2*s;j&lt;=m;j*=2)&#123; //沿关键字较大的孩子结点向下筛选 //找出s结点下，最大的孩子结点。j为关键字中较大的记录的下标。 if (j&lt;m &amp;&amp; list[j]&lt;list[j+1])&#123; ++j; &#125; if (temp &gt;= list[j])&#123; break; //根节点是最大的值，不需要改变位置 &#125; list[s] =list[j]; //将根节点与孩子结点数值交换， //s替换为j,进入下一次循环，看是否需要与下一个根节点互换 s=j; &#125; list[s] = temp; //将孩子结点换成根节点的数值&#125; 堆排序的核心算法已经有了，接下来就是对数组进行排序。12345678910void HeapSort(SqList list)&#123; int i; for (i = list.length()/2;i&gt;0;i--)&#123; ** HeadAdjust(list,i,list.length()); //把list构建成一个大顶堆 &#125; for (i = list.length() ;i&gt;1;i--)&#123; swap(list,1,i); //将堆顶记录和当前子序列的最后一个记录交换。 HeadAdjust(list,1,i-1); //将list[1..i-1]重新调整为大顶堆 &#125;&#125; 假设list长度是9，从4往下递减，是因为都是有孩子的结点。我们所谓的将待排序的序列构建成为一个大顶堆，其实就是从下往上，从右往左，将每个非终端结点（非叶结点）当成根节点，将其和其子树调整成大顶堆。 堆排序的时间复杂度堆排序的运行时间主要是消耗在初始构建堆和重建堆时的反复筛选上。在构建堆的过程中，因为我们是完成二叉树从最下层最右边的非终端结点开始构建将它与其孩子进行和若有必要的互换，对于每个非终端结点来说，其实最多进行两次比较和互换操作，因此整个构建堆的时间复杂度为O(n)。在正式排序时，第i次取堆顶记录重建堆需要用O(logi)的时间（完全二叉树的某个结点到根节点的距离为logi+1）,并且需要去n-1次堆顶记录，因此，重建堆的时间复杂度为O(nlogn)。所以，总体来说，堆排序的时间复杂度为O(nlogn). 归并排序前面我们讲 了堆排序，因为它用到了完全二叉树，充分利用了完全二叉树的深度是log2n + 1的特性，所以效率比较高。不过堆结构的设计本身是比较复杂的，有没有更直接简单的方法利用完成二叉树来排序。当然有。归并排序法涉及到完全二叉树结构的排序算法。归并排序就像是一颗倒置的完全二叉树 归并排序算法归并排序就是利用归并的思想实现的排序方法。他的原理是假设初始序列含有n个记录，则可以看成是n个有序的子序列，每个子序列的长度为1，然后两两归并，得到[n/2]个长度为2或1的有序子序列，再两两归并，…..,如此重复，直至得到一个长度为n的有序序列位置，这种排序方法称为2路归并排序。12345678910111213141516//对顺序表L作归并排序void MergeSort(Sqlist list)&#123; Msort(list,list,1,list.length());&#125;void Msort(int sr[],int tr1[],int s,int t)&#123; int m; int tr2[MAXSIZE+1]; if (s == t)&#123; //当细分到一个记录填入tr2后，此时s和t相等，递归返回 tr1[s] = sr[s]; &#125;else&#123; m = (s+t) /2; //将sr[s..t]平分为sr[s..m]和sr[m+1..t] 10 Msort(sr,tr2,s,m); //递归将sr[s..m]归并为有序的tr2[s..m] Msort(sr,tr2,m+1,t); //递归将sr[m+1..t]归并成有序的tr2[m+1..m] 12 Merge(tr2,tr1,s,m,t); //将tr2[s..m]和tr2[m+1..t],归并到tr1[s..t] &#125;&#125; 看第10行继续递归进去后，直到细分为一个记录填入tr2,此时s与t相等，递归返回，每次递归返回后都会执行当前递归函数的第12行，将tr2归并到tr1中，最终使得当前序列有序。现在我们来看看Merge函数的代码是如果实现的。123456789101112131415161718192021//sr[] 待归并的数组，tr[]归并排序后的数组，i=1，m正中间值,n数组长度void Merge(int sr[],int tr[],int i,int m,int n)&#123; int j,k,l; //k记录tr数组坐标。 j记录sr数组右半段数组下标 for (j = m+1,k=i;i&lt;=m &amp;&amp; j&lt;=n;k++)&#123; //将sr中记录由小到大归并到tr if (sr[i] &lt;sr[j])&#123; tr[k] = sr[i++]; //将前后两段数组进行对比，牛逼 &#125;else&#123; tr[k] = sr[j++]; &#125; &#125; if (i &lt;=m)&#123; //将没有归并到tr的sr[1..m]复制到tr for(l=0;l&lt;m-i;l++)&#123; tr[k+1]=sr[i+1]; &#125; &#125; if (j &lt;=n)&#123; //将没有归并到tr的sr[m+1..n]复制到tr for(l=0;l&lt;n-j;l++)&#123; tr[k+1]=sr[i+1]; &#125; &#125;&#125; 归并排序复杂度分析总的时间复杂度为O(nlogn),这是归并排序算法中最好，最坏，平均的时间性能。归并排序Merge函数中有if（sr[i]&lt;sr[j]）,这就说明它需要两两比较，不存在跳跃，因此归并排序是一种稳定的排序算法。 快速排序希尔排序相当于直接插入排序的升级，堆排序相当于简单选择排序的升级，它们同属于选择排序类，而快速排序其实就是最慢的冒泡排序的升级，都属于交换排序类，只不过它的实现，增大了记录的比较和移动的距离，将关键字较大的记录从前面直接移动到后面，关键字较小的记录从后面直接移动到前面，从而减小了总的比较次数和移动交换次数。 快速排序算法快速排序的基本思想是:通过一趟排序将待排序记录分割成独立的两部分，其中一部分记录的关键字均比另一部分的关键字小，则可分别对这两部分记录继续进行排序，以达到整个序列有序的目的。1234567891011121314151617181920212223242526//对顺序表L的子序列作快速排序void QSort(SqList list , int low ,int high)&#123; int pivot; if (low &lt; high)&#123; pivot = Partition(L,low,high); //将list一分为二 算出枢轴值pivot Qsort(list,low,pivot-1); //对低子表递归排序 Qsort(list,pivot+1,length); //对高子表递归排序 &#125;&#125;int Partition(SqList list,int low ,int high)&#123; int pivotkey; pivotkey = list[low]; //用子表的第一个记录做枢轴记录 while(low &lt; high)&#123; //将第一个记录作为枢轴，则一定要从后往前扫 while (low &lt;high &amp;&amp; list[high] &gt;=pivotkey)&#123; high --; &#125; swap(list,low,high); //将比枢轴小的记录交换到低端 while(low &gt;high &amp;&amp; list[low] &lt;=pivotkey)&#123; low ++; &#125; swap(list,low,high); //将比枢轴大的记录交换到高端 &#125; return low; //返回枢轴所在位置&#125; 快速排序时间复杂度分析在最优的情况下，快速排序算法的时间复杂度为O(nlogn)。最坏情况下，其时间复杂度是O(n*n)。快速排序是一种不稳定的排序方法。java对快速的实现 java.util.Arrays —-&gt; static void sort(type[] a)]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[递归]]></title>
    <url>%2F2019%2F08%2F21%2F%E9%80%92%E5%BD%92%2F</url>
    <content type="text"><![CDATA[http://lylblog.cn/blog/4]]></content>
  </entry>
  <entry>
    <title><![CDATA[react框架]]></title>
    <url>%2F2019%2F08%2F09%2Freact%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[通过react脚手架创建项目create-react-app [项目名] 启动脚手架npm start打包生产环境npm run build JSXJSX本身也是一种表达式 组件 组件是可以复用的UI元素]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>react</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前端学习网站]]></title>
    <url>%2F2019%2F08%2F08%2F%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%AB%99%2F</url>
    <content type="text"><![CDATA[React官方文档 https://reactjs.org/ECMAScript 6入门 http://es6.ruanyifeng.com/（特别是解构赋值、箭头函数和 Class）MDN - JavaScript https://developer.mozilla.org/zh-CN/docs/Web/JavaScript（JS基础语法）MDN - Array.prototype.map() https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/map（数组map方法） ES6入门javaScript JavaScript里一切皆对象，一切皆可储存在变量里。 let 声明变量。 浏览器出现Cannot set property ‘onclick’ of null的问题浏览器先加载玩按钮节点才执行的js ,所以当浏览器自顶向下解析时，找不到onclick绑定的按钮节点。因此，需要把js文件放在底部加载，就会避免该问题 ES6 中let 命令，用来声明变量，所声明的变量，只在let命令所在的代码块中有效。var命令声明的，在全局范围内都有效 1234567 for (let i = 0; i &lt; 3; i++) &#123; let i = &apos;abc&apos;; console.log(i);&#125;// abc// abc// abc 上面代码正确运行，输出了 3 次abc。let 命令在for循环中，函数内部变量的i,与循环变量i不在同一个作用域。 变量的结构赋值 1234567891011121314151617181920let [foo, [[bar], baz]] = [1, [[2], 3]];foo // 1bar // 2baz // 3let [ , , third] = [&quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;];third // &quot;baz&quot;let [x, , y] = [1, 2, 3];x // 1y // 3let [head, ...tail] = [1, 2, 3, 4];head // 1tail // [2, 3, 4]let [x, y, ...z] = [&apos;a&apos;];x // &quot;a&quot;y // undefinedz // [] 解构失败例子12let [foo] = [];let [bar, foo] = [1]; 等号左边的模式，只匹配一部分的等号右边的数组。这种情况下，解构依然可以成功。 12345678 let [x, y] = [1, 2, 3];x // 1y // 2let [a, [b], d] = [1, [2, 3], 4];a // 1b // 2d // 4]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>前端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据锁]]></title>
    <url>%2F2019%2F08%2F06%2F%E6%95%B0%E6%8D%AE%E9%94%81%2F</url>
    <content type="text"><![CDATA[解决并发问题，数据库常用的两把锁]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux]]></title>
    <url>%2F2019%2F08%2F05%2FLinux%2F</url>
    <content type="text"><![CDATA[1. vim编辑器一般模式：编辑模式：[i,0,a,r] , [ESC] 退出指令列命令模式 ：[: / ?]:wq 保存后退出:q! 强制退出不保存 Linux的分类：1.内核版本：Linux不是一个操作系统，严格来讲，只是一个操作系统的内核。内核建立计算机软件和硬件之间通讯的平台。2.发行版本：一些组织或公司在内核上进行二次开发的版本。Linux一个重要概念，一切都是文件。 Linux常用命令 mkdir：增加目录 rmdir：删除目录 ls 或 ll :( ll 是ls -l缩写 ll可看到该目录下所有目录和文件的详细信息 ) find 目录 参数 :寻找目录eg : find . -name ‘x.txt’ -o -name ‘x.pdf’当前目录及子目录下 所有以 .txt , .pdf结尾的文件 mv 目录名称 新目录名称 （改名）mv 目录名称 目录新位置 （剪切） cp -r 目录 目录新名字 （拷贝，-r递归拷贝） rm [-rf] 目录 ：删除目录tar -zcvf 压缩后的名字 要打包的文件-s 还原文件的顺序和备份文件内的存放顺序相同。-t 列出备份文件的内容。-v 显示指令执行过程。-f 指定压缩文件-x 从备份文件中还原文件。tar -xvf 压缩文件 -x 解压 Linux 查看负载 uptime 显示当前机器负载 w 列出所有user分别的情况 top wa超10%表示IO压力很大。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务架构]]></title>
    <url>%2F2019%2F08%2F02%2F%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[idea编码Enoding 记得设置UTF-8spring-boot如官网所说，帮我们构建一个spring项目，直接可以运行。微服务架构，将每一个模块均分成每一个项目。每个项目之间通过接口等，相互联系。将项目拆分几个独立的功能单元（服务） 的架构 优点 技术异构性：开发者可以自由的选择合理的技术和框架，只要服务遵守API协定即可。 弹性：将不同服务部署在不同机器上，降低整体功能不可用的概率 可扩展性：可以只对需要扩展的服务进行扩展 简化部署：各个服务模块福利部署，可以快速对特定代码进行更新。缺点 实现复杂度搞 测试微服务复杂。对于微服务的一个类似的测试则需要运行该服务以及依赖的服务，也可能会增加额外的沟通成本。 分割的数据库架构：对于微服务间的事务性操作，因为不同的微服务采用了不同的数据库，将无法利用数据库本身的事务机制保证一致性。 接口匹配问题。服务依赖于彼此间的接口进行通信。改变一个服务的接口会对其他服务造成影响。 部署复杂 运维复杂SpringCloudSpringCloud是基于SpringBoot提供了一套为微服务解决方案，包括服务注册与发现。 Eureka:服务发现/注册中心，用于定位服务。 Hystrix:熔断器，容错管理工具。 Zuul：是在云平台上提供动态路由，监控，弹性，安全等边缘服务的框架 Ribbon/Feign:客户端负载均衡1.1 微服务架构 - 常见微服务框架 Dubbo/Dubbox 一个分布性，高性能，透明化的RPC服务框架 阿里巴巴开发 ， 当当改良 基于RPC Spring Cloud Spring团队开发 基于RESTful1.2 微服务架构 - 通信方式 RPC Remote Procedure Call 支持RPC的微服务框架：Dubbo/Dubbox 基于TCP,平台有关 RESTful Representational State Transfer 支持RESTful 的微服务框架：Spring Cloud/Dubbox 基于HTTP，平台无关其他概念 — 分布式和集群 分布式 关注项目拆分（水平拆分，垂直拆分） 集群关注项目部署 1.3 主要组件Eureka：服务发现/注册中心，一个基于 REST 的服务，用于定位服务，以实现云端中间层服务发现和故障转移Spring Cloud Eureka是对Netflix的Eureka的进一步封装。Hystrix：熔断器，容错管理工具，旨在通过熔断机制控制服务和第三方库的节点,从而对延迟和故障提供更强大的容错能力Zuul：Zuul 是在云平台上提供动态路由,监控,弹性,安全等边缘服务的框架Ribbon/Feign：客户端负载均衡，Feign是一种声明式、模板化的HTTP客户端Turbine：集群监控Springcloud-config：配置管理工具包，让你可以把配置放到远程服务器，集中化管理集群配置，目前支持本地存储、Git以及SubversionSpringcloud-bus：轻量级消息代理Springcloud-sleuth/zipkin：链路追踪Springcloud-security：基于spring security的安全工具包，为你的应用程序添加安全控制 问题小结Feign客户端customer在启动类中添加客户端EnableFeignClients12345//Eureka客户端@EnableEurekaClient@SpringBootApplication//Feign客户端@EnableFeignClients 当无法注入bean时，可以通过@Configuration手动注入@bean123456789101112@Configurationpublic class FeignConfig &#123; @Bean public UserFeignClient userFeignClient()&#123; return new UserFeignClient() &#123; @Override public String login() &#123; return null; &#125; &#125;; &#125;&#125; Feign,http客户端，替服务发送http请求，解决接口调用问题。使用lombok插件时，Intellij idea开发的话需要安装Lombok plugin，同时设置 Setting -&gt; Compiler -&gt; Annotation Processors -&gt; Enable annotation processing勾选。 容错处理Hystrix（容错机制应该深入理解下），解决调用过程中的异常处理基于feign整合hystrix采用熔断器机制，就像try….catch….一样。通过在调用者实现feign接口，实现具体的失败业务逻辑。 12345678// 注入spring容器中@Component public class UserFeignclientFallback implements UserFeignClient &#123; @Override public Boolean login(User user) &#123; return false; &#125;&#125; 在接口定义中，添加fallback = UserFeignclientFallback.class(发生宕机后处理错误的类)字段，当服务宕机，则执行这个方法。123456@FeignClient(name = "provider-demo",fallback = UserFeignclientFallback.class)public interface UserFeignClient &#123; @RequestMapping(value = "/login",method = RequestMethod.POST) public Boolean login(@RequestBody User user);&#125; Ribbon解决负载均衡的一个组件Nginx解决服务端（被调用方）负载均衡————–Ribbon是解决客户端（调用方）调用默认分配策略是平均分配。 案列测试 123&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt;&lt;/dependency&gt; 先集群provider服务（通过springboot复制一个新服务，设置端口号-Dserver.port=8082） 通过发起多次请求可以发现，ribbon默认算法是均衡的（你一次我一次。） 更换ribbon的负载均衡策略。在调用者方更改。即customer12345# 更改ribbon的负载算法 provider-demo: ribbon: #随机策略算法 NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule Zuul微服务网关 例如filter，对进入服务的请求进行过滤 springcloud实现的过滤器。 ########## 具体实现 通过继承ZuulFilter ，实现默认方法 注入spring容器 12345yml文件中zuul:routes: # 对路径是带有user的，则认为是要进入customer，执行网关拦截。/user --》网关入口 customer-demo: /user/** 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061 @Componentpublic class PreFilter extends ZuulFilter &#123; /*** * 过滤器类型,过滤器运行时间 * FilterConstants.PRE_TYPE == pre .相当于pre （前置，中，后置等） * @return */ @Override public String filterType() &#123; return FilterConstants.PRE_TYPE; &#125; /** * 同级别过滤器优先级，数越大越低 * @return */ @Override public int filterOrder() &#123; return 0; &#125; /*** * 过滤器是否发挥作用,对于过滤器的控制更加灵活 * @return */ @Override public boolean shouldFilter() &#123; return true; &#125; /*** * 过滤器做的事情 * @return * @throws ZuulException */ @Override public Object run() throws ZuulException &#123; //通过requestContext实现接口之间的信息传递 RequestContext context = RequestContext.getCurrentContext(); //通过context拿request请求 HttpServletRequest httpServletRequest = context.getRequest(); /* //在请求头中拿用户验证的token 测试验证 String token = httpServletRequest.getHeader("token"); if (token == null || token.equals(""))&#123; context.setSendZuulResponse(false); context.setResponseStatusCode(401); context.setResponseBody("&#123;\"msg\":\"401,access without permission,login first.\"&#125;"); return "access denied"; &#125; return "pass";*/ String key = httpServletRequest.getParameter("key"); System.out.println(key+"filter 1"); if ("1".equals(key))&#123; context.setSendZuulResponse(false); &#125; return null; &#125;&#125; 过滤器之间的协调作用通过request，context传递信息12345678//通过requestContext实现接口之间的信息传递RequestContext context = RequestContext.getCurrentContext();//通过context拿request请求HttpServletRequest httpServletRequest = context.getRequest(); context.sendZuulResponse(); 获取到上一个过滤器的状态 config 分布式配置 访问路径 /项目名/版本/分支 （get请求）http://localhost:7900/gateway-zull/dev/springcloud在远程仓库上，关于配置文件的命名规范 前缀：/项目名-版本号.properties dev 开发环境 pro 生产环境 test 测试环境远程配置管理，将公共配置集中起来，统一管理通过git配置，将一些配置文件的信息，配置到项目中来 1.添加config依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt; &lt;/dependency&gt; 2. 修改配置信息123456789cloud: config: server: git: uri: https://github.com/czetao/config-server.git username: czetao password: ilzzr888 # 如果不是在根目录下，需要标注下一个包 search-paths: config-file 3. 在启动类上，添加configserver注解。12@EnableDiscoveryClient@EnableConfigServer]]></content>
      <categories>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vue项目小记]]></title>
    <url>%2F2019%2F07%2F29%2Fvue%E9%A1%B9%E7%9B%AE%E5%B0%8F%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[win + Q 打开搜索框npm 与 cnpm 的区别DOM （Document Object Model）是指文档对象模型，通过它，可以访问html文档的所有元素。 下载cnpm(淘宝镜像，更快)npm install cnpm -g –registry=https://registry.npm.taobao.org 全局安装vue -clicnpm install -g vue-cli npm i install下载安装模块vue项目管理系统环境搭建使用vue脚本架搭建工程vue init webpack vuetest(项目名)package.json 总项目的js控制文件启动项目 npm run dev 集成elementUI组件npm i element-ui -S12345//引入element-ui 样式 main.jsimport ElementUI from &apos;element-ui&apos;;import &apos;element-ui/lib/theme-chalk/index.css&apos;;//注册elementuiVue.use(ElementUI); 使用axiosnpm install axios –save 123456789101112//在main.js中引入axiosimport axios from &apos;axios&apos;//挂载在Vue的原型上。Vue.prototype.axios = axios//通过代理方式，将请求地址代理到888端口下，即解决跨域 this.axios.post(&apos;/api/checklogin&apos;,&#123; username : that.loginForm.username, password : that.loginForm.password &#125;) .then(response =&gt; &#123; console.log(&quot;接收后端响应请求的数据：&quot; ,response.data) &#125;) 通过axios与后端交互。 使用express+node.js快速搭建后台。通过后台的/routes/index.js文件，接收请求123router.post(&quot;/checklogin&quot;,(req,res) =&gt; res.send(&quot;1&quot;)) //全局安装expressnpm install express-generator -gexpress -e server 使用nodemon工具启动项目 记得给安装新项目中的模块。cnpm installnpm install -g nodemon12//启动后台服务nodemon app 出现跨域错误error : changeOrigin通过自己添加一个代理 解决1234567891011proxyTable: &#123; &apos;/api&apos;: &#123; target: &apos;http://localhost:888/&apos;, //目标接口域名 changeOrigin: true, //是否跨域 pathRewrite: &#123; &apos;^/api&apos;: &apos;/api&apos; //重写接口 &#125; &#125; &#125;, 这段代码的效果就是将本地8080端口的一个请求代理到了http://www.abc.com这一域名下： &apos;http://localhost:8080/api&apos; ===&gt; &apos;http://localhost:888/api&apos; sql 语法当使用${}拼接时，sql应该用套起1const sql = `select * from users where username=&apos;$&#123;username&#125;&apos; and password=&apos;$&#123;password&#125;&apos;` 使用vuex做到数据之间的共享cnpm i vuex –save通过vuex中的state做到一个全局变量的储存，形同与小程序的全局变量localstore。通过定义mutations操作state123456const mutations = &#123; SAVE_USERINFO(state,userinfo)&#123; console.log(&quot;函数被触发&quot;); state.userinfo = userinfo; &#125;&#125; 通过commit 触发函数1$store.commit(&apos;SAVE_USERINFO&apos;,data); 在样式中加个!imporant路由出口 在属性列表 增添router属性 使用vuex中mapState 获取state数据vuex中state 是保存全局状态的常用方法。改变state通过改变提交方式123456789101112//状态 const state = &#123; userinfo : JSON.parse(localStorage.getItem(&apos;userinfo&apos;))&#125;//mutations 主要用来操作stateconst mutations = &#123; SAVE_USERINFO(state,userinfo)&#123; //存入本地 需要将对象转成字符串 localStorage.setItem(&apos;userinfo&apos;,JSON.stringify(userinfo)) state.userinfo = userinfo; &#125;&#125; 通过vuex actions 异步获取所有数据mapState 辅助组件给state状态中的属性映射。通过提交computed方法123456789101112import &#123;mapState&#125; from &apos;vuex&apos;;computed :&#123; // username ()&#123; // return this.$store.state.userinfo.username // &#125;, //通过mapState 辅助函数获取state数据 ...mapState(&#123; userinfo :state =&gt; state.userinfo, username :state =&gt; state.userinfo.username &#125;) &#125;]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>vue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[牛客java小结]]></title>
    <url>%2F2019%2F07%2F26%2F%E7%89%9B%E5%AE%A2java%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[零散知识存在继承的情况下，初始化顺序为： 父类（静态变量，静态代码块） 子类（静态变量，静态代码块） 父类（实例变量，普通代码块） 父类（构造函数） 子类（实例变量，普通代码块） 子类（构造函数） 继承：在继承关系之中，如果要实例化子类对象，会默认先调用父类构造，为父类之中的属性初始化，之后再调用子类构造，为子类之中的属性初始化，即：默认情况下，子类会找到父类之中的无参构造方法。 牛客java 捕捉到异常时 ， 程序就会停止运行 ，抛出异常 重载 ：仅返回值类型不同时， 不足以构成重载 。重载的基本条件 参数类型不同 参数次序不通过 参数个数不同 重写 ： 函数名， 函数参数，返回值 应该相同 true false null sizeof 不是java的关键字 ， 但是也不能当成java标识符用const goto 是java的保留字（关键字） boolean 的默认值是false this() 和 super() 为构造方法，作用是在jvm 堆中构建出一个对象 。因此避免多次创建对象，同一个方法中只能调用一次this（ ） 和super() . 且必须在第一行实现，避免操作对象时，对象还未构建成功。 实例一个内部类 异常通常分为编译时异常， 运行异常 。编译时异常需要手动进行捕捉处理（文件不存在），运行时异常只有在编译器编译运行才会出现，不需要自己手动捕捉（空指针异常，溢出） 对于外部类来说 ， 只有两种修饰，public 和默认（default ） A instanceOf B ,是判断对象A 是否属于B 或B的子类，子类接口实现类，实现类的实例。 final 类型的变量一定要初始化 ， 因为final 的变量不可更改。 java 类是单继承 ，java 接口可以多继承。 static 方法只能使用 static 变量 ， 想使用非静态变量， 只能通过实例化对象 ，再通过对象引用 堆区 ： 只存放类对象 ， 线程共享 类中的成员变量， 存放在堆区 栈区 ： 存放局部变量，线程不共享 方法区 ： 静态存储区， 存放class文件 和静态数据。线程共享 静态语句块中变量为局部变量，不影响静态变量的值。 权限登记 ：public &gt; protected&gt; default&gt; private 在JDK1.7中，如果通过无参构造的话，初始数组容量为0（应该也是10），当真正对数组进行添加时，才真正分配容量。每次按照1.5倍（位运算）的比率通过copeOf的方式扩容。在JKD1.6中，如果通过无参构造的话，初始数组容量为10.每次通过copeOf的方式扩容后容量为原来的1.5倍加1.以上就是动态扩容的原理。1234567891011121314/**jdk8*/private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); &#125; jdk1.8后 关于arrayList 初始化和扩容详解 public:具有最大访问权限。 可以被同一项目下的任何类所调用，一般用于对外的情况。protected:与public不同的是不同包下的类是不能使用的，但是其子孙类除外。所以我认为这是特意为子类设计的。default:它是针对本包设计的，它所修饰的在本包下的其他类都访问。private:只为类本身提供。是一种封装的体现 A. 非抽象类继承抽象类，必须将抽象类中的方法重写，否则需将方法再次申明为抽象。所以这个方法还可再次声明为抽象，而不用重写。而用重载也错了，重载是在同一个类中，重写、覆盖才是在父子类中。B.抽象类可以没有抽象方法，接口是完全的抽象，只能出现抽象方法。C.抽象类无法实例化，无法创建对象。现实生活中也有抽象类的类子，比如说人类是一个抽象类，无法创建一个叫人类的对象，人继承人类来创建对象。况且抽象类中的抽象方法只有声明，没有主体，如果实例化了，又如何去实现调用呢？D因为类是单继承的，类继承了一个抽象类以后，就不能再继承其他类了。 静态方法是属于类的，当实例化该类时，静态会被优先加载并且只加载一次，不受实例化new 的影响，只要是使用了类，都会加载静态类。静态块只会执行一次。 鲁棒性(Robust,即健壮性)Java在编译和运行程序时，都要对可能出现的问题进行检查，以消除错误的产生。它提供自动垃圾收集来进行内存管理，防止程序员在管理内存时容易产生 的错误。通过集成的面向对象的例外处理机制，在编译时，Java揭示出可能出现但未被处理的例外，帮助程序员正确地进行选择以防止系统的崩溃。另外， Java在编译时还可捕获类型声明中的许多常见错误，防止动态运行时不匹配问题的出现。 Java中定义String数组，有两种定义方式：String a[]和String[] a byte能表示的范围[-128,127] 类的继承，自动向上转型 在定义方法参数时，通常总是应该优先使用父类或接口 对应成list ， 不需要定义多个对象 类里面只有属性和方法。 %取余操作，只适用于整型 如某个JAVA进程的JVM参数配置如下：-Xms1G -Xmx2G -Xmn500M -XX:MaxPermSize=64M -XX:+UseConcMarkSweepGC -XX:SurvivorRatio=3,请问eden区最终分配的大小是多少？java -Xmx2G -Xms1G -Xmn500M -Xss128k-Xmx2G：设置JVM最大可用内存为2G。-Xms1G：设置JVM促使内存为1G。此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存。-Xmn500M：设置年轻代大小为2G。整个JVM内存大小=年轻代大小 + 年老代大小 + 持久代大小。-XX:SurvivorRatio=3:新生代中又会划分为 Eden 区，from Survivor、to Survivor 区。其中 Eden 和 Survivor 区的比例默认是 8:1:1，当然也支持参数调整 -XX:SurvivorRatio=3的话就是3:1:1。故该题为500*（1/3）=300M. interface中合法方法定义?()public void main(String [] args);没有关键字static，可以当成普通方法。 Iterator和 ListIterator主要区别 ListItertor有add()方法，可以向list中添加对象，而 iterator不能。 ListIterator 和 Iterator都有 hashNext()和 next()方法，可以实现顺序向后遍历。但是ListIterator 有 hasPrevious()和previous()方法，可以实现逆向遍历。Iterator就不可以 ListIterator 可以定位当前的索引位置，nextIndex() 和 previousIndex()可以实现。Iterator没有此功能。 都可实现删除对象，但是ListIterator可以实现对象的修改，set()方法可以实现。Iterator仅能遍历，不能修改。因为ListIterator的这些功能，可以实现对LinkedList等list数据结构的操作。 为了更好地组织类，Java 提供了包机制，用于区别类名的命名空间。包的作用 把功能相似或相关的类或接口组织在同一个包中，方便类的查找和使用。 如同文件夹一样，包也采用了树形目录的存储方式。同一个包中的类名字是不同的，不同的包中的类的名字是可以相同的，当同时调用两个不同包中相同类名的类时，应该加上包名加以区别。因此，包可以避免名字冲突。 包也限定了访问权限，拥有包访问权限的类才能访问某个包中的类。Java 使用包（package）这种机制是为了防止命名冲突，访问控制，提供搜索和定位类（class）、接口、枚举（enumerations）和注释（annotation）等。 Garbage Collection：当对象的所有引用都消失后，对象使用的内存将自动回收 cho $$ 当前登录shell 的PIDecho $? 最后运行的命令的结束代码（返回值）即执行上一个指令的返回值 (显示最后命令的退出状态。0表示没有错误，其他任何值表明有错误) 接口中的属性在不提供修饰符修饰的情况下，会自动加上public static final注意（在1.8的编译器下可试）：（1）属性不能用private，protected,default 修饰，因为默认是public（2）如果属性是基本数据类型，需要赋初始值，若是引用类型，也需要初始化，因为默认有final修饰，必须赋初始值；（3）接口中常规的来说不能够定义方法体，所以无法通过get和set方法获取属性值，所以属性不属于对象，属于类（接口），因为默认使用static修饰。 start方法会开启一个新的线程执行run方法，所以start方法执行完，不代表run方法执行完，线程也不一定销毁！ 可以有方法名与类名相同的普通方法 “is”+100+5 在字符串后面的会自动转成字符串在字符串前面的不会转成字符串真题总结 内部类的访问规则 可以直接访问外部类的成员，包括私有 外部类要想访问内部类成员，必须创建对象 父子类中方法执行顺序 静态优先，普通代码块，构造随后 无论静态还是构造，先父再子 序列化：将数据转为n个byte序列的过程，也就是将数据结构转换称为二进制数据流或者文本流的过程。把对象转换为字节序列的过程，序列化后的数据方便在网络上传输和在硬盘上存储。反序列化：与序列化相反，是将二进制数据流或者文本流转换称为易于处理和阅读的数据结构的过程。这是java进程之间通信的方式。 按照流是否直接与特定的地方（如磁盘、内存、设备等）相连，分为节点流和处理流两类。 节点流：可以从或向一个特定的地方（节点）读写数据。如FileReader. 处理流：是对一个已存在的流的连接和封装，通过所封装的流的功能调用实现数据读写。如BufferedReader.处理流的构造方法总是要带一个其他的流对象做参数。一个流对象经过其他流的多次包装，称为流的链接。 JAVA常用的节点流： 文 件 FileInputStream FileOutputStrean FileReader FileWriter 文件进行处理的节点流。 字符串 StringReader StringWriter 对字符串进行处理的节点流。 数 组 ByteArrayInputStream ByteArrayOutputStreamCharArrayReader CharArrayWriter 对数组进行处理的节点流（对应的不再是文件，而是内存中的一个数组）。 管 道 PipedInputStream PipedOutputStream PipedReaderPipedWriter对管道进行处理的节点流。常用处理流（关闭处理流使用关闭里面的节点流）缓冲流：BufferedInputStrean BufferedOutputStream BufferedReader BufferedWriter 增加缓冲功能，避免频繁读写硬盘。转换流：InputStreamReader OutputStreamReader 实现字节流和字符流之间的转换。数据流 DataInputStream DataOutputStream 等-提供将基础数据类型写入到文件中，或者读取出来流的关闭顺序一般情况下是：先打开的后关闭，后打开的先关闭另一种情况：看依赖关系，如果流a依赖流b，应该先关闭流a，再关闭流b。例如，处理流a依赖节点流b，应该先关闭处理流a，再关闭节点流b可以只关闭处理流，不用关闭节点流。处理流关闭的时候，会调用其处理的节点流的关闭方法。 接口是更抽象的东西，属性默认是：public static final的，方法默认是public abstract的！ XML中DTD,XSD的区别DTD和XSD相比：DTD 是使用非 XML 语法编写的。DTD 不可扩展,不支持命名空间,只提供非常有限的数据类型 。DTD即文档类型定义，是一种XML约束模式语言，是XML文件的验证机制,属于XML文件组成的一部分。XML Schema语言也就是XSD。XML Schema描述了XML文档的结构。 JAVA中使用DOM方法解析XML文件XML现在已经成为一种通用的数据交换格式，平台的无关性使得很多场合都需要用到XML，DOM解析是将XML文件全部载入到内存，组装成一颗DOM树，然后通过节点以及节点之间的关系来解析XML文件。 多线程相关 调用run()，与调用start()的区别直接调用run()方法，并没有创建线程，跟调用普通方法是一样的。创建一个线程，需要覆盖Thread类的run（）方法，然后调用Thread类的start（）方法启动。Java集合体系Connection接口:— List 有序,可重复ArrayList优点: 底层数据结构是数组，查询快，增删慢。缺点: 线程不安全，效率高Vector优点: 底层数据结构是数组，查询快，增删慢。缺点: 线程安全，效率低LinkedList优点: 底层数据结构是链表，查询慢，增删快。缺点: 线程不安全，效率高 —Set 无序,唯一HashSet底层数据结构是哈希表。(无序,唯一)如何来保证元素唯一性?1.依赖两个方法：hashCode()和equals()LinkedHashSet底层数据结构是链表和哈希表。(FIFO插入有序,唯一)1.由链表保证元素有序2.由哈希表保证元素唯一TreeSet底层数据结构是红黑树。(唯一，有序) 如何保证元素排序的呢?自然排序比较器排序2.如何保证元素唯一性的呢?Map接口数据类型 基本数据类型包装类：包装类是对象，拥有方法和字段。 包装类型都是有重写equals方法初始值不同，int是0，boolean是false,包装类初始值是null,在定义javabean时，通常使用包装类，为防止数据库中有null数据。更例如定义List中，数据具体类型也是用包装类。 自动拆箱 包装类 ==》基本数据类型 int a = new Integer(100); [-128,127]在这个范围内，会直接从缓存(堆中的常量池)中取。 自动装箱 Double和BigDecimal123double d1 = 2.0;double d2 = 1.1;d1-d2 = 0.89999999; 实际运用中，在对金额的运算，通常使用的是BigDecimal。bigdecimal的equals方法，还会对数据中的小数点进行比较。通常使用compareTO方法比较大小。 {-1：小于， 0 ：等于， 1：大于}为了防止精度丢失，构造方法BigDecimal(“String”)来定义bigdecimal对象，禁止直接使用double。 类设计（jdk8新特性，接口中可以有方法） 接口默认是public,所有方法在接口中不能有实现（java8开始接口可以有默认实现 允许在接口中定义static方法和default方法（带方法体）） 接口本身可以通过extends关键字扩展多个接口 StringBuffer 和StringBuilder的区别 可变性 源码没有用final修饰，是可变的，String是final char[] ,不可变 线程安全性 StringBuilder非线程安全 性能 hashset 去重。HashSet ，通过重写teacher实体类中的eqauls和hashcode方法，定义去重原理。对list中存放对象，需重写equals方法和 list == null || list.size()==0 Date日期转换成指定格式的字符串日期。SimpleDataFormat是线程不安全的类，建议使用DateFormateUtils工具包// 日期String转DateDateUtils.parseDate(“2019-08-04”,”YYYY-MM-DD”) 代码规范 可以抽离代码 鼠标右击 》Refactor 》Extract]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实习第四周]]></title>
    <url>%2F2019%2F07%2F25%2F%E5%AE%9E%E4%B9%A0%E7%AC%AC%E5%9B%9B%E5%91%A8%2F</url>
    <content type="text"><![CDATA[遗漏点 jpa技术 h2 database spring mvc @PathVariable method = RequestMethod.GET @ResponseStatus(HttpStatus.OK)返回状态码 @JsonIgnore作用：在json序列化时将java bean中的一些属性忽略掉，序列化和反序列化都受影响。使用方法：一般标记在属性或者方法上，返回的json数据即不包含该属性。 date parse()返回的是一个Date类型数据，format返回的是一个StringBuffer类型的数据 普通web项目右击项目，Project Structure -&gt; Artifacts -&gt; Output Layout，新建lib文件夹，将基于maven导入的jar包全部加进去，重新运行，顺利解决。 github记得.gitnore eclipse加上get/set shift+alt+s]]></content>
      <categories>
        <category>实习</category>
      </categories>
      <tags>
        <tag>实习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实习第三周]]></title>
    <url>%2F2019%2F07%2F25%2F%E5%AE%9E%E4%B9%A0%E7%AC%AC%E4%B8%89%E5%91%A8%2F</url>
    <content type="text"><![CDATA[新增功能节点之后，给用户分配职责。 使用用户账号登录后，查看功能节点。 用户分配角色，分配职责。 参照设置 nc65sqlinsert into bd_refinfo (CODE, DR, ISNEEDPARA, ISSPECIALREF, LAYER, METADATANAMESPACE,METADATATYPENAME, MODULENAME, NAME, PARA1, PARA2, PARA3, PK_COUNTRY, PK_INDUSTRY, PK_REFINFO, REFCLASS, REFPATH, REFSYSTEM, REFTYPE, RESERV1, RESERV2, RESERV3, RESID, RESIDPATH, TS, WHEREPART)values (‘TR1010020’, 0, null, null, null, ‘uap’, ‘CustVo’, ‘uap’, ‘客户分类’, null, null, null, null, null,‘0001Z0100000002TRAIN’, ‘’, ‘nc.ui.train.pub.ref.CustClassRefModel’, null, 2, null, null, null, ‘客户分类’, ‘ref’,‘2019-05-29 13:44:08’, null);参照注意点：METADATATYPENAME：设置为空，对应的是实体的属性。 ORA-12541 数据库服务未监听服务管理员启动监听服务 –》lsnrctl start]]></content>
      <categories>
        <category>实习</category>
      </categories>
      <tags>
        <tag>实习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实习第二周]]></title>
    <url>%2F2019%2F07%2F25%2F%E5%AE%9E%E4%B9%A0%E7%AC%AC%E4%BA%8C%E5%91%A8%2F</url>
    <content type="text"><![CDATA[oracle数据库配置：在本地oracle环境中，tnsnames.ora文件中配置数据库信息。12345678910APPORCL(连接数据库的别名) = (DESCRIPTION = (ADDRESS_LIST = (ADDRESS = (PROTOCOL = TCP)(HOST = 115.28.**.**)(PORT = 1521)) ) (CONNECT_DATA = (SERVICE_NAME （数据库名）= corcl) )) nc63train/a 创建视图12345678910CREATE [OR REPLACE] [&#123;FORCE|NOFORCE&#125;] VIEW view_nameASSELECT查询[WITH READ ONLY CONSTRAINT]例子：create or REPLACE view p_rateASselect r.userid,r.rateset,p.usercode,p.username,p.alipayid from rate r,pro_user p where r.userid=p.userid; OR REPLACE：如果视图已经存在，则替换旧视图。 FORCE：即使基表不存在，也可以创建该视图，但是该视图不能正常使用，当基表创建成功后，视图才能正常使用。 NOFORCE：如果基表不存在，无法创建视图，该项是默认选项。 WITH READ ONLY：默认可以通过视图对基表执行增删改操作，但是有很多在基表上的限制(比如：基表中某列不能为空，但是该列没有出现在视图中，则不能通过视图执行insert操作)，WITH READ ONLY说明视图是只读视图，不能通过该视图进行增删改操作。现实开发中，基本上不通过视图对表中的数据进行增删改操作。删除视图可以使用“DROP VIEW 视图名称”，删除视图不会影响基表的数据。 全局数据库名称orcl管理口令 ilzzr888]]></content>
      <categories>
        <category>实习</category>
      </categories>
      <tags>
        <tag>实习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实习第一周]]></title>
    <url>%2F2019%2F07%2F25%2F%E5%AE%9E%E4%B9%A0%E7%AC%AC%E4%B8%80%E5%91%A8%2F</url>
    <content type="text"><![CDATA[接触产品 erp nc65 中间件 UAP uap用友UAP平台是一体化平台，其中包括了开发平台、集成平台、动态建模平台、商业分析平台（用友BQ）、数据处理平台（用友AE）、云管理平台和运行平台等7个领域产品，这些平台产品涵盖了软件应用的全生命周期和IT服务管理过程，用于全面支撑平台化企业，可以为大中型企业与公共组织构建信息化平台提供核心工具与服务。账号密码 管理系统(用超级管理员打开)超级管理员root ilzzr888!! 系统管理员2 ilzzr888qwe（普通开启方式）集团管理员(失效日期设置，不对，还有奇怪)统一新增密码 ilzzr888用01 ilzzr888qwe职责里面没有分配功能一块 邮箱密码ilzzr888.. 用友云社区ilzzr888开发环境基础设置 窗口–&gt;首选项–&gt; java–&gt;jre 选用UAP自带的Runtime jre UAP-STUDIO–&gt;开发配置 UAP HOME设置，位置是uaphome 数据源等在配置基础环境sysconfig.bat脚本中就配置好，会自动导入 开发设置–&gt;客户端链接 端口号要与脚本设置一样。 oracle数据库nc65erp本地搭建UAP中间件搭建元数据建立与使用 新建实体组件 无业务组件[train.bill] 实体属性：创建实体时，访问器类型：当组件代码风格选择传统样式时，针对主子表或者多子表中主表对应的实体，访问器要设置为AggVO，即聚合VO访问器，其他的一律选择NCVO。 代码风格：自定义样式 向导生成单据属性 类型样式：SINGLE 单一样式，最终的类型就是原始数据类型。（与数据库对应，或者封装的类型。） REF:引用样式，用于实体，值对象，随后的类型即为参照。 主键一定设置为UFID类型。 字段名称即为生成数据库表列的名称。 对于设置为AggVO样式的实体，访问策略有设置为BodyOfAggVOAccessor. 参照设置：一个实体可以设置多个参照，但必须设置一个缺省参照。获取属性的参照时，如果没有设置则取属性对应实体的缺省参照。 树管理开发界面 与树卡档案不同的是，右边的信息只有一个字段与之关联，新增删除是对节点内容的修改。数据挂在节点下面。 树卡档案，树与卡片相联系，一个树节点就是一个卡片。增改卡片就是对树节点的修改。 整体开发流程： 元数据建模，节点注册配置，单击模板制作，输出模板制作，节点代码编写。 实现接口，记得配置接口属性映射（如何设置接口的属性。） 元数据发布：发布元数据，生成java源代码，生成建库sql脚本并执行。NC65学习记录 uap-studio新增模块后，需将模块信息注册到数据源。 生成元数据时，映射以及命名要规范。 orcl数据库安装管理口令： 创建模块之后，记得给登录系统管理员，给模块分配角色。]]></content>
      <categories>
        <category>实习</category>
      </categories>
      <tags>
        <tag>实习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sql语句]]></title>
    <url>%2F2019%2F07%2F23%2Fsql%E8%AF%AD%E5%8F%A5%2F</url>
    <content type="text"><![CDATA[数据库分页limit 2，5从第三条开始，每次5条。 UNION去重且排序UNION ALL不去重不排序NION用的比较多union all是直接连接，取到得是所有值，记录可能有重复 union 是取唯一值，记录没有重复 UNION 的语法如下：[SQL 语句 1]UNION[SQL 语句 2] UNION ALL 的语法如下：[SQL 语句 1]UNION ALL[SQL 语句 2]UNION和UNION ALL关键字都是将两个结果集合并为一个]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git]]></title>
    <url>%2F2019%2F07%2F16%2Fgit%2F</url>
    <content type="text"><![CDATA[git remote add &lt;主机名&gt; &lt;远程地址&gt; git remote -v 查看别称 git add . 添加所有 git reset 撤回所有add ,也可以针对特定文件。 git commit -m “提交说明” [git commit -a -m “ “] 组合命令，[-a] = git add . git push -u &lt;主机名&gt; master推送出错时，error: failed to push some refs to &#39;git@github.com:…..” 是因为没有将远程仓库同步，将远程仓库与本地代码合并。git pull –rebase origin mastergit pull origin master –allow-unrelated-histories git clone &lt;远程地址&gt; . 注意最后加 . 克隆当前目录 git diff #查看difference git reset]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git小记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ajax跨域]]></title>
    <url>%2F2019%2F07%2F05%2Fajax%E8%B7%A8%E5%9F%9F%2F</url>
    <content type="text"><![CDATA[为什么会发生跨域 浏览器限制（浏览器会对请求做校验，校验失败则是跨域。 ） 跨域 只要端口号，域名，协议..不同，浏览器都会将其标记为跨域 XHR(XMLHttpRequest)请求。如果不是XHR请求，浏览器也不会对其限制。解决思路 JSONP对json的补充，不是一个官方协议，但也是一个约定（前后台约定callback）。动态创建一个script标签，返回js代码（需要对后台代码做修改。）原理即是对发送的请求加一个callback参数，后台发现有callback参数，即知道是一个jsonp请求，就会将返回的数据将json改成JavaScript。JavaScript里的内容就是一个函数调用。dataType：”jsonp”,jsonp:”callback” 弊端： 服务器需要改动代码支持。 只支持get方法。 发送的不是XHR请求。（异步，各种事件的特性都无法使用） 支持跨域被调用方的角度。跨域请求是直接从浏览器发送过去的，在响应头中添加字段，告诉浏览器允许跨域。 服务器端实现 通过拦截器在响应头中添加字段 response.addHeader(“Access-Control-Allow-Origin”,”http://localhost:8081/*(允许所有方法)&quot;); 将这个字段设置为*，即为允许跨域，并不完善。 response.addHeader(“Access-Control-Allow-Methods”,”GET”); NGINX配置 APACHE配置 带cookie的跨域12345678910$.ajax(&#123; url : &apos;http://remote.domain.com/corsrequest&apos;, data : data, dataType: &apos;json&apos;, type : &apos;POST&apos;, xhrFields: &#123; withCredentials: true &#125;, crossDomain: true, contentType: &quot;application/json&quot;, 通过设置 withCredentials: true ，发送Ajax时，Request header中便会带上 Cookie 信息。通过设置document里添加cookie 隐藏跨域在调用方的角度, 通过一个代理http服务器，将从A域名发出的请求，将指定的url加入B域名里面，就不会判定为跨域。静态请求：请求与用户数据无关。动态请求：请求与用户数据有关。]]></content>
      <categories>
        <category>跨域</category>
      </categories>
      <tags>
        <tag>小识跨域</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实习记录]]></title>
    <url>%2F2019%2F07%2F02%2F%E5%AE%9E%E4%B9%A0%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[在此记录实习过程中遇到的难点以及解决方案。]]></content>
      <tags>
        <tag>实习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java容器]]></title>
    <url>%2F2019%2F07%2F01%2Fjava%E5%AE%B9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[数组与链表的适用场景数组 优点具有快速查找特性。O(1)。 缺点在内存中地址是连续的，当需要扩容时，将会重新新建数组空间，拷贝旧数组的内容到新数组。每当删除，新增时，需要移动数组内容的位置。时间复杂度为O(N)。空间利用率低。适合多查，少增改链表 优点数据在内存中并不是连续的，当需要增改时，只需要改变指针位置。每一个结点都有一个指针域和数据域。 缺点遍历查找时麻烦，需要从头遍历查找。 容器主要包括 Collection 和 Map 两种，Collection 存储着对象的集合，而 Map 存储着键值对（两个对象）的映射表。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql数据库]]></title>
    <url>%2F2019%2F06%2F18%2Fmysql%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[存储引擎mysql数据库引擎属于插件式存储引擎，基于业务可在不同的表上使用合适的存储引擎。在mysql5.5后，默认是InnoDB存储引擎。InnoDB：InnoDB是造成mysql灵活性的技术的直接产品。 支持事务处理，支持外键，行锁，当需要频繁的更新，删除操作时选择此引擎。MyISAM:不支持事务，支持表锁。读多写少可使用，查询效率更好。 更换存储引擎： Alter table XXX engine = InnoDB由于存储引擎不同，索引的结构也会相对不同。Myisam 的索引结构（非聚集索引）InnoDB表 （聚集索引） :即存数据又存索引 索引—–最常见的慢查询优化方式是一种优化查询的数据结构，mysql是基于B+树实现的，可以优化查询速度。 建索引原则：适合 ： 频繁作为where条件的字段。关联字段可以建索引，例如外键。 Order bt col, Group by col.不适合 ： where条件中用不到的字段，频繁更新的字段 ，数据值分布比较均匀的不适合建索引，例如男女 ， 真假值 ，表的数据量少]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[复习小结]]></title>
    <url>%2F2019%2F06%2F18%2F%E5%A4%8D%E4%B9%A0%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[学习网站https://www.runoob.com/ bootstrap布局http://www.ibootstrap.cn 牛客备战网站https://www.nowcoder.com/studypath/1 整理复习 常忘小结（三门基本课程：数据结构，计算机网络，操作系统） 计算机网络 http协议 TCP三次握手，四次挥手 TCP滑动窗口机制Linux 常用命令 文件权限控制mysql数据库 两个常用存储引擎 数据库优化（sql优化，索引，水平垂直分表）JVM 对象的创建 垃圾回收java小结 java集合类大纲（全盘掌握） 基本数据类型对象的创建（spring,int）常忘 知识点小结框架小结 对于IOC，AOP的理解 spring MVC工作流程java java 容器]]></content>
      <categories>
        <category>复习</category>
      </categories>
      <tags>
        <tag>整理小结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP协议]]></title>
    <url>%2F2019%2F06%2F18%2FHTTP%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[HTTP协议细讲 HTTP有哪些方法？ HTTP1.0定义了三种请求方法： GET, POST 和 HEAD方法 HTTP1.1新增了五种请求方法：OPTIONS, PUT, DELETE, TRACE 和 CONNECT 这些方法的具体作用是什么？ GET: 通常用于请求服务器发送某些资源(幂等、缓存) HEAD: 请求资源的头部信息, 并且这些头部与 HTTP GET 方法请求时返回的一致. 该请求方法的一个使用场景是在下载一个大文件前先获取其大小再决定是否要下载, 以此可以节约带宽资源 OPTIONS: 用于获取目的资源所支持的通信选项 POST: 发送数据给服务器 PUT: 用于新增资源或者使用请求中的有效负载替换目标资源的表现形式(幂等) DELETE: 用于删除指定的资源(幂等) PATCH: 用于对资源进行部分修改 CONNECT: HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器 TRACE: 回显服务器收到的请求，主要用于测试或诊断 GET和POST有什么区别？ 数据传输方式不同：GET请求通过URL传输数据，而POST的数据通过请求体传输。 安全性不同：POST的数据因为在请求主体内，所以有一定的安全性保证，而GET的数据在URL中，通过历史记录，缓存很容易查到数据信息。 数据类型不同：GET只允许 ASCII 字符，而POST无限制 GET无害： 刷新、后退等浏览器操作GET请求是无害的，POST可能重复提交表单 特性不同：GET是安全（这里的安全是指只读特性，就是使用这个方法不会引起服务器状态变化）且幂等（幂等的概念是指同一个请求方法执行多次和仅执行一次的效果完全相同），而POST是非安全非幂等 PUT和POST都是给服务器发送新增资源，有什么区别？PUT 和POST方法的区别是,PUT方法是幂等的：连续调用一次或者多次的效果相同（无副作用），而POST方法是非幂等的。 除此之外还有一个区别，通常情况下，PUT的URI指向是具体单一资源，而POST可以指向资源集合。 举个例子，我们在开发一个博客系统，当我们要创建一篇文章的时候往往用POST https://www.jianshu.com/articles，这个请求的语义是，在articles的资源集合下创建一篇新的文章，如果我们多次提交这个请求会创建多个文章，这是非幂等的。 而PUT https://www.jianshu.com/articles/820357430的语义是更新对应文章下的资源（比如修改作者名称等），这个URI指向的就是单一资源，而且是幂等的，比如你把『刘德华』修改成『蔡徐坤』，提交多少次都是修改成『蔡徐坤』 ps: 『POST表示创建资源，PUT表示更新资源』这种说法是错误的，两个都能创建资源，根本区别就在于幂等性 HTTP协议掌握考点 HTTP协议主要特点 HTTP报文的组成部分（请求报文，响应报文） get和post HTTP状态码主要特点 无连接 ：连接一次就会断开，不会继续保持连接 无状态客户端和服务器端是两种身份。第一次请求结束后，就断开了，第二次请求时，服务器端并没有记住之前的状态，也就是说，服务器端无法区分客户端是否为同一个人、同一个身份。有的时候，我们访问网站时，网站能记住我们的账号，这个是通过其他的手段（比如 session）做到的，并不是http协议能做到的。HTTP报文的组成部分 请求报文get/post请求方法 请求url http协议及版本号请求头 一堆键值对 请求体 name = tom &amp;password = 123(数据部分) 响应报文http协议/版本号 状态码及状态描述响应头响应体 返回信息记住组成部分，会考聊一聊HTTP的状态码有哪些？ 2XX 成功 200 OK，表示从客户端发来的请求在服务器端被正确处理 ✨ 201 Created 请求已经被实现，而且有一个新的资源已经依据请求的需要而建立 202 Accepted 请求已接受，但是还没执行，不保证完成请求 204 No content，表示请求成功，但响应报文不含实体的主体部分 206 Partial Content，进行范围请求 ✨ 3XX 重定向 301 moved permanently，永久性重定向，表示资源已被分配了新的 URL 302 found，临时性重定向，表示资源临时被分配了新的 URL ✨ 303 see other，表示资源存在着另一个 URL，应使用 GET 方法丁香获取资源 304 not modified，表示服务器允许访问资源，但因发生请求未满足条件的情况 307 temporary redirect，临时重定向，和302含义相同 4XX 客户端错误 400 bad request，请求报文存在语法错误 ✨ 401 unauthorized，表示发送的请求需要有通过 HTTP 认证的认证信息 ✨ 403 forbidden，表示对请求资源的访问被服务器拒绝 ✨ 404 not found，表示在服务器上没有找到请求的资源 ✨ 408 Request timeout, 客户端请求超时 409 Confict, 请求的资源可能引起冲突 5XX 服务器错误 500 internal sever error，表示服务器端在执行请求时发生了错误 ✨ 501 Not Implemented 请求超出服务器能力范围，例如服务器不支持当前请求所需要的某个功能，或者请求是服务器不支持的某个方法 503 service unavailable，表明服务器暂时处于超负载或正在停机维护，无法处理请求 505 http version not supported 服务器不支持，或者拒绝支持在请求中使用的 HTTP 版本 HTTP的keep-alive是干什么的？在早期的HTTP/1.0中，每次http请求都要创建一个连接，而创建连接的过程需要消耗资源和时间，为了减少资源消耗，缩短响应时间，就需要重用连接。在后来的HTTP/1.0中以及HTTP/1.1中，引入了重用连接的机制，就是在http请求头中加入Connection: keep-alive来告诉对方这个请求响应完成后不要关闭，下一次咱们还用这个请求继续交流。协议规定HTTP/1.0如果想要保持长连接，需要在请求头中加上Connection: keep-alive。 keep-alive的优点： 较少的CPU和内存的使用（由于同时打开的连接的减少了） 允许请求和应答的HTTP管线化 降低拥塞控制 （TCP连接减少了） 减少了后续请求的延迟（无需再进行握手） 报告错误无需关闭TCP连接 什么是长连接、短连接？在HTTP/1.0中默认使用短连接。也就是说，客户端和服务器每进行一次HTTP操作，就建立一次连接，任务结束就中断连接。当客户端浏览器访问的某个HTML或其他类型的Web页中包含有其他的Web资源（如JavaScript文件、图像文件、CSS文件等），每遇到这样一个Web资源，浏览器就会重新建立一个HTTP会话。 而从HTTP/1.1起，默认使用长连接，用以保持连接特性。使用长连接的HTTP协议，会在响应头加入这行代码： Connection:keep-alive在使用长连接的情况下，当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接不会关闭，客户端再次访问这个服务器时，会继续使用这一条已经建立的连接。Keep-Alive不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如Apache）中设定这个时间。实现长连接需要客户端和服务端都支持长连接。 HTTP协议的长连接和短连接，实质上是TCP协议的长连接和短连接。 HTTPS的工作原理 我们都知道HTTPS能够加密信息，以免敏感信息被第三方获取，所以很多银行网站或电子邮箱等等安全级别较高的服务都会采用HTTPS协议。 客户端在使用HTTPS方式与Web服务器通信时有以下几个步骤，如图所示。 （1）客户使用https的URL访问Web服务器，要求与Web服务器建立SSL连接。 （2）Web服务器收到客户端请求后，会将网站的证书信息（证书中包含公钥）传送一份给客户端。 （3）客户端的浏览器与Web服务器开始协商SSL连接的安全等级，也就是信息加密的等级。 （4）客户端的浏览器根据双方同意的安全等级，建立会话密钥，然后利用网站的公钥将会话密钥加密，并传送给网站。 （5）Web服务器利用自己的私钥解密出会话密钥。 （6）Web服务器利用会话密钥加密与客户端之间的通信。 HTTPS是如何保证安全的？过程比较复杂，我们得先理解两个概念 对称加密：即通信的双方都使用同一个秘钥进行加解密，比如特务接头的暗号，就属于对称加密 对称加密虽然很简单性能也好，但是无法解决首次把秘钥发给对方的问题，很容易被黑客拦截秘钥。 非对称加密： 私钥 + 公钥= 密钥对 即用私钥加密的数据,只有对应的公钥才能解密,用公钥加密的数据,只有对应的私钥才能解密 因为通信双方的手里都有一套自己的密钥对,通信之前双方会先把自己的公钥都先发给对方 然后对方再拿着这个公钥来加密数据响应给对方,等到到了对方那里,对方再用自己的私钥进行解密 非对称加密虽然安全性更高，但是带来的问题就是速度很慢，影响性能。 解决方案： 那么结合两种加密方式，将对称加密的密钥使用非对称加密的公钥进行加密，然后发送出去，接收方使用私钥进行解密得到对称加密的密钥，然后双方可以使用对称加密来进行沟通。 此时又带来一个问题，中间人问题： 如果此时在客户端和服务器之间存在一个中间人,这个中间人只需要把原本双方通信互发的公钥,换成自己的公钥,这样中间人就可以轻松解密通信双方所发送的所有数据。 所以这个时候需要一个安全的第三方颁发证书（CA），证明身份的身份，防止被中间人攻击。 证书中包括：签发者、证书用途、使用者公钥、使用者私钥、使用者的HASH算法、证书到期时间等 但是问题来了，如果中间人篡改了证书，那么身份证明是不是就无效了？这个证明就白买了，这个时候需要一个新的技术，数字签名。 数字签名就是用CA自带的HASH算法对证书的内容进行HASH得到一个摘要，再用CA的私钥加密，最终组成数字签名。 当别人把他的证书发过来的时候,我再用同样的Hash算法,再次生成消息摘要，然后用CA的公钥对数字签名解密,得到CA创建的消息摘要,两者一比,就知道中间有没有被人篡改了。 这个时候就能最大程度保证通信的安全了。 HTTP2相对于HTTP1.x有什么优势和特点？#二进制分帧帧：HTTP/2 数据通信的最小单位消息：指 HTTP/2 中逻辑上的 HTTP 消息。例如请求和响应等，消息由一个或多个帧组成。 流：存在于连接中的一个虚拟通道。流可以承载双向消息，每个流都有一个唯一的整数ID HTTP/2 采用二进制格式传输数据，而非 HTTP 1.x 的文本格式，二进制协议解析起来更高效。 #头部压缩HTTP/1.x会在请求和响应中中重复地携带不常改变的、冗长的头部数据，给网络带来额外的负担。 HTTP/2在客户端和服务器端使用“首部表”来跟踪和存储之前发送的键－值对，对于相同的数据，不再通过每次请求和响应发送 首部表在HTTP/2的连接存续期内始终存在，由客户端和服务器共同渐进地更新; 每个新的首部键－值对要么被追加到当前表的末尾，要么替换表中之前的值。 你可以理解为只发送差异数据，而不是全部发送，从而减少头部的信息量 #服务器推送服务端可以在发送页面HTML时主动推送其它资源，而不用等到浏览器解析到相应位置，发起请求再响应。例如服务端可以主动把JS和CSS文件推送给客户端，而不需要客户端解析HTML时再发送这些请求。 服务端可以主动推送，客户端也有权利选择是否接收。如果服务端推送的资源已经被浏览器缓存过，浏览器可以通过发送RST_STREAM帧来拒收。主动推送也遵守同源策略，服务器不会随便推送第三方资源给客户端。 #多路复用HTTP 1.x 中，如果想并发多个请求，必须使用多个 TCP 链接，且浏览器为了控制资源，还会对单个域名有 6-8个的TCP链接请求限制。 HTTP2中： 同域名下所有通信都在单个连接上完成。 单个连接可以承载任意数量的双向数据流。 数据流以消息的形式发送，而消息又由一个或多个帧组成，多个帧之间可以乱序发送，因为根据帧首部的流标识可以重新组装 拓展阅读：HTTP/2特性及其在实际应用中的表现 #HTTP的缓存的过程是怎样的？通常情况下的步骤是: 客户端向服务器发出请求，请求资源 服务器返回资源，并通过响应头决定缓存策略 客户端根据响应头的策略决定是否缓存资源（这里假设是），并将响应头与资源缓存下来 在客户端再次请求且命中资源的时候，此时客户端去检查上次缓存的缓存策略，根据策略的不同、是否过期等判断是直接读取本地缓存还是与服务器协商缓存]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo]]></title>
    <url>%2F2019%2F06%2F06%2Fhexo%2F</url>
    <content type="text"><![CDATA[hexo g 生成静态文件 generate hexo d 部署网站，需要预先生成静态文件 hexo clean 清楚缓存文件 hexo s 启动博客 hexo new “文章名称” 创建文章名称 hexo new page “tags” 新页面 组合命令 hexo d -g 部署上传 这是next主题优化网站 这是Typora官方网站]]></content>
      <categories>
        <category>hexo记录</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面经]]></title>
    <url>%2F2019%2F06%2F06%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[JavaGuide：JavaGuide，对于复习很有帮助，面试时候都会尽量刷一遍知识点。 云趣科技（凉）第一家面试的公司，准备不充分被刷。常规做笔试题（java基础应该掌握的东西）：数据库sql，多线程创建方式，适配器模式概述，代码重构的方法。面试：文件流操作，jsp隐藏属性，spring事务机制，jdbc如何实现，jdbc开启事务，jsp自定义标签，项目基本思路。总结：一定要尽早迈出第一步，尽早的查漏补缺，认识自己复习的到哪一步，有助于提升动力复习。 粤信科技（拿offer） 6.6面试距离第一次面试的公司，期间1个星期。重新整理复习的重点，掌握jvm，多线程，集合类，计网常用知识点等能够在面试中的加分项。笔试：java基础知识。面试：hr面和技术面自我介绍：也要准备好个稿，参考javaGuide。hr面：聊一下你在学校的生活，参加过什么活动。还有挑一个比较熟悉的项目讲给她听。这个感觉只要态度端正，要给面试官感觉到你尊重她，你在认真的听她讲话。这一部分暂不知道什么需要注意的。但排前一个的同学，就在这一面挂了，hr面完直接让他走了，没有技术面。技术面：全程都是根据你写的项目在问。所以一定要整理好自己项目的知识点。主要问题：挑一个你最熟悉的项目，说一说你是怎么设计数据库表的？你认为你这个项目中有哪个技术点让你印象深刻，你是怎么解决的？收到offer通知后，出错了。没能做保底，在hr跟我说需要实习到大四，嘴贱说初衷是打算实习3,4个月，然后换另一家公司。结果hr立马说那不行，收回入职邀请。她说我的心态已经变了，不会给考虑机会。555 用友科技（拿offer） 6.7没有笔试。面试：我看你简历上写了jvm,你说一下吧说一下java基础中，接口和抽象类的区别然后我自己在说知识点的时候，就提到自己的项目去了，最后两个项目都说了下业务逻辑，和一些基本的技术点。如果需要接触新知识，你是怎么学习的如果我要你在7天内完成任务，你完成不了，会怎么办你还要问我什么问题。（javaguide有提及，可以参考。）等了一周才发offer，公司大，环境好，但是工资较低，继续找 火烈鸟 6.11 凉透，难的发指。信心摧残。基础够格，就能过，无面经。]]></content>
      <categories>
        <category>面试汇总</category>
      </categories>
      <tags>
        <tag>面试总结</tag>
      </tags>
  </entry>
</search>
