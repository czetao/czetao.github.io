<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[mysql慢查询配置]]></title>
    <url>%2F2019%2F10%2F09%2Fmysql%E6%85%A2%E6%9F%A5%E8%AF%A2%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[第一步.开启 MySQL 慢查询方式一、修改配置文件Windows：配置文件为 my.ini，一般在 MySQL 安装目录下或者 c:\Windows 下。Linux：配置文件为 my.cnf ，一般在 /etc 下。在 my.ini 增加下面代码: 12345678910[mysqld] long_query_time=2 #5.5以前版本配置如下选项 log-slow-queries="mysql_slow_query.log" #5.5及以上版本配置如下选项 slow-query-log=On slow_query_log_file="mysql_slow_query.log" log-query-not-using-indexes 第一句：定义超过多少秒的查询算是慢查询，这里定义的是2秒第二句：定义慢查询日志的路径（注意如果是 Linux 或 Mac 系统要考虑权限问题）第三句：记录下没有使用索引的query 方式二、通过命令行开启慢查询上面的配置需要重启 mysql server 进程 mysqld 才会生效。但很多时候，尤其是产品生产环境，不希望每次修改都要重新启动mysql服务器，而是希望能在某些特定时间记录。MySQL5.1 之后为我们提供了灵活的运行时控制，使得你不必重新启动 mysql 服务器，也能选择性地记录或不记录某些 slow queries。 MySQL5.1中，提供了全局变量 slow_query_log、slow_query_log_file，可以灵活地控制enable/disable慢查询。同时可以通过 long_query_time 设置慢查询时间 注意:设置了全局变量 slow_query_log ,则变量 log_slow_queries 也会隐性地跟着改变 1mysql&gt;set global slow_query_log=ON 不幸的是,在 MySQL5.1 之前并没有提供类似的全局变量来灵活控制，但是我们可以通过将long_query_time 设置得足够大来避免记录某些查询语句。比如 12mysql&gt;set global long_query_time = 3600;mysql&gt;set global log_querise_not_using_indexes = ON; MySQL5.0 在不关闭服务的情况下，希望不记录日志的办法是让日志文件成为 /dev/null 的符号链接(symbolic link)。注意:你只需要在改变后运行 flush logs 以确定MySQL释放当前的日志文件描述符，重新把日志记录到/dev/null。 和 MySQL5.0 不同, MySQL5.1 后可以在运行时改变日记行为，将日志记录到数据库表中。只要将 MySQL 全局变量 log_output 设置为 table 即可。MySQL 会将日志分别记录到mysql.gengera_log 和 mysql.slow_log 两张表中。但是，推荐的做法是将日志记录在日记文件中。 1234mysql&gt; show variables like ‘log_output’\GVariable_name: log_outputValue: FILEmysql&gt;set global log_output=’table’; 缺陷与审记如果开启了 log_queries_not_using_indexes 选项，slow query 日志会充满过多的垃圾日志记录，这些快且高效的全表扫描查询(表小)会冲掉真正有用的slow queries记录。比如 select * from category 这样的查询也会被记录下来。通过microslow-patch补丁可使用更细的时间粒度，和记录所有执行过的sql语句。不过，使用这个补订不得不自己编译MySQL，出于稳定性考滤，我们推荐在开发测试环境，可以打上这个补丁，享受这个补丁带来的便利。在运营环境尽量不要这么做… 第五步.分析慢查询日志方式一.通过工具分析MySQL 自带了 mysqldumpslow 工具用来分析 slow query 日志，除此之外，还有一些好用的开源工具。比如MyProfi、mysql-log-filter，当然还有 mysqlsla。 以下是 mysqldumpslow 常用参数说明，详细内容可用 mysqldumpslow -help 查询。-s，表示按照何种方式排序，c、t、l、r分别是按照记录次数、时间、查询时间、返回的记录数来排序（从大到小），ac、at、al、ar表示相应的倒序。-t，是top n的意思，即为返回前面多少条数据。-g，后边可以写一个正则匹配模式，大小写不敏感。 接下来就是用MySQL 自带的慢查询工具 mysqldumpslow 分析了（该工具位于 MySQL 的bin 目录下），我这里的日志文件名字是host-slow.log。 列出记录次数最多的10个sql语句 1mysqldumpslow -s c -t 10 host-slow.log 列出返回记录集最多的10个sql语句 1mysqldumpslow -s r -t 10 host-slow.log explain6 keys MYSQL使用的索引，简单且重要 7 key_len MYSQL使用的索引长度 8 ref ref列显示使用哪个列或常数与key一起从表中选择行。 9 rows 显示MYSQL执行查询的行数，简单且重要，数值越大越不好，说明没有用好索引 在日常工作中，我们会有时会开慢查询去记录一些执行时间比较久的SQL语句，找出这些SQL语句并不意味着完事了，些时我们常常用到explain这个命令来查看一个这些SQL语句的执行计划，查看该SQL语句有没有使用上了索引，有没有做全表扫描，这都可以通过explain命令来查看。所以我们深入了解MySQL的基于开销的优化器，还可以获得很多可能被优化器考虑到的访问策略的细节，以及当运行SQL语句时哪种策略预计会被优化器采用。（QEP：sql生成一个执行计划query Execution plan） ;) 1234567mysql&gt; explain select * from servers;+----+-------------+---------+------+---------------+------+---------+------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------+------+---------------+------+---------+------+------+-------+| 1 | SIMPLE | servers | ALL | NULL | NULL | NULL | NULL | 1 | NULL |+----+-------------+---------+------+---------------+------+---------+------+------+-------+1 row in set (0.03 sec) ;) expain出来的信息有10列，分别是id、select_type、table、type、possible_keys、key、key_len、ref、rows、Extra,下面对这些字段出现的可能进行解释： 一、 id ​ 我的理解是SQL执行的顺序的标识,SQL从大到小的执行 \1. id相同时，执行顺序由上至下 \2. 如果是子查询，id的序号会递增，id值越大优先级越高，越先被执行 3.id如果相同，可以认为是一组，从上往下顺序执行；在所有组中，id值越大，优先级越高，越先执行 二、select_type ​ 示查询中每个select子句的类型 (1) SIMPLE(简单SELECT,不使用UNION或子查询等) (2) PRIMARY(查询中若包含任何复杂的子部分,最外层的select被标记为PRIMARY) (3) UNION(UNION中的第二个或后面的SELECT语句) (4) DEPENDENT UNION(UNION中的第二个或后面的SELECT语句，取决于外面的查询) (5) UNION RESULT(UNION的结果) (6) SUBQUERY(子查询中的第一个SELECT) (7) DEPENDENT SUBQUERY(子查询中的第一个SELECT，取决于外面的查询) (8) DERIVED(派生表的SELECT, FROM子句的子查询) (9) UNCACHEABLE SUBQUERY(一个子查询的结果不能被缓存，必须重新评估外链接的第一行) 三、table 显示这一行的数据是关于哪张表的，有时不是真实的表名字,看到的是derivedx(x是个数字,我的理解是第几步执行的结果) ;) 12345678mysql&gt; explain select * from (select * from ( select * from t1 where id=2602) a) b;+----+-------------+------------+--------+-------------------+---------+---------+------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+--------+-------------------+---------+---------+------+------+-------+| 1 | PRIMARY | &lt;derived2&gt; | system | NULL | NULL | NULL | NULL | 1 | || 2 | DERIVED | &lt;derived3&gt; | system | NULL | NULL | NULL | NULL | 1 | || 3 | DERIVED | t1 | const | PRIMARY,idx_t1_id | PRIMARY | 4 | | 1 | |+----+-------------+------------+--------+-------------------+---------+---------+------+------+-------+ ;) 四、type 表示MySQL在表中找到所需行的方式，又称“访问类型”。 常用的类型有： ALL, index, range, ref, eq_ref, const, system, NULL（从左到右，性能从差到好） ALL：Full Table Scan， MySQL将遍历全表以找到匹配的行 index: Full Index Scan，index与ALL区别为index类型只遍历索引树 range:只检索给定范围的行，使用一个索引来选择行 ref: 表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值 eq_ref: 类似ref，区别就在使用的索引是唯一索引，对于每个索引键值，表中只有一条记录匹配，简单来说，就是多表连接中使用primary key或者 unique key作为关联条件 const、system: 当MySQL对查询某部分进行优化，并转换为一个常量时，使用这些类型访问。如将主键置于where列表中，MySQL就能将该查询转换为一个常量,system是const类型的特例，当查询的表只有一行的情况下，使用system NULL: MySQL在优化过程中分解语句，执行时甚至不用访问表或索引，例如从一个索引列里选取最小值可以通过单独索引查找完成。 五、possible_keys 指出MySQL能使用哪个索引在表中找到记录，查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询使用 该列完全独立于EXPLAIN输出所示的表的次序。这意味着在possible_keys中的某些键实际上不能按生成的表次序使用。如果该列是NULL，则没有相关的索引。在这种情况下，可以通过检查WHERE子句看是否它引用某些列或适合索引的列来提高你的查询性能。如果是这样，创造一个适当的索引并且再次用EXPLAIN检查查询 六、Key key列显示MySQL实际决定使用的键（索引） 如果没有选择索引，键是NULL。要想强制MySQL使用或忽视possible_keys列中的索引，在查询中使用FORCE INDEX、USE INDEX或者IGNORE INDEX。 七、key_len 表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度（key_len显示的值为索引字段的最大可能长度，并非实际使用长度，即key_len是根据表定义计算而得，不是通过表内检索出的） 不损失精确性的情况下，长度越短越好 八、ref 表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值 九、rows 表示MySQL根据表统计信息及索引选用情况，估算的找到所需的记录所需要读取的行数 十、Extra 该列包含MySQL解决查询的详细信息,有以下几种情况： Using where:列数据是从仅仅使用了索引中的信息而没有读取实际的行动的表返回的，这发生在对表的全部的请求列都是同一个索引的部分的时候，表示mysql服务器将在存储引擎检索行后再进行过滤 Using temporary：表示MySQL需要使用临时表来存储结果集，常见于排序和分组查询 Using filesort：MySQL中无法利用索引完成的排序操作称为“文件排序” Using join buffer：改值强调了在获取连接条件时没有使用索引，并且需要连接缓冲区来存储中间结果。如果出现了这个值，那应该注意，根据查询的具体情况可能需要添加索引来改进能。 Impossible where：这个值强调了where语句会导致没有符合条件的行。 Select tables optimized away：这个值意味着仅通过使用索引，优化器可能仅从聚合函数结果中返回一行 总结：**• EXPLAIN不会告诉你关于触发器、存储过程的信息或用户自定义函数对查询的影响情况• EXPLAIN不考虑各种Cache• EXPLAIN不能显示MySQL在执行查询时所作的优化工作• 部分统计信息是估算的，并非精确值• EXPALIN只能解释SELECT操作，其他操作要重写为SELECT后查看执行计划。**]]></content>
      <tags>
        <tag>项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx解答]]></title>
    <url>%2F2019%2F10%2F08%2Fnginx%E8%A7%A3%E7%AD%94%2F</url>
    <content type="text"><![CDATA[1. 概念 正向代理是一个位于客户端和目标服务器之间的代理服务器(中间服务器)。为了从原始服务器取得内容，客户端向代理服务器发送一个请求，并且指定目标服务器，之后代理向目标服务器转交并且将获得的内容返回给客户端。正向代理的情况下客户端必须要进行一些特别的设置才能使用。 反向代理正好相反。对于客户端来说，反向代理就好像目标服务器。并且客户端不需要进行任何设置。客户端向反向代理发送请求，接着反向代理判断请求走向何处，并将请求转交给客户端，使得这些内容就好似他自己一样，一次客户端并不会感知到反向代理后面的服务，也因此不需要客户端做任何设置，只需要把反向代理服务器当成真正的服务器就好了。 2. 区别 正向代理需要你主动设置代理服务器ip或者域名进行访问，由设置的服务器ip或者域名去获取访问内容并返回；而反向代理不需要你做任何设置，直接访问服务器真实ip或者域名，但是服务器内部会自动根据访问内容进行跳转及内容返回，你不知道它最终访问的是哪些机器。 正向代理是代理客户端，为客户端收发请求，使真实客户端对服务器不可见；而反向代理是代理服务器端，为服务器收发请求，使真实服务器对客户端不可见。 2、安装tomcat2个,现在我们模拟的话服务器就采用tomcat来模拟。 安装tomcat的过程就不介绍了，在http://blog.csdn.net/u013144287/article/details/78499485过程中有介绍，（1）创建一个tomcat目录mkdir -p /usr/local/tomcats在此目录下安装两个tomcat如图所示： （2）修改tomcat2的端口号，vi ./tomcat2/conf/server.xml 修改此3处端口号，分别在原来基础上加1，然后wq保存，启动两台tomcat 3、需求nginx的安装可以参考http://blog.csdn.net/u013144287/article/details/78408001此篇文章通过访问不同的域名访问运行在tomcat不同端口的服务器，中间使用nginx反向代理服务器windows上访问需要修改hosts文件进行配置如下：8080.zcinfo.com 访问运行8080端口的tomcat 8082.zcinfo.com 访问运行8081端口的tomcat 如图所示：hosts目录是：C:\Windows\System32\drivers\etc 4、Nginx的配置在Nginx的配置文件里面加入如下配置 upstream tomcatserver1 { server 192.168.3.43:8080; } upstream tomcatserver2 { server 192.168.3.43:8082; } server { listen 80; server_name 8080.zcinfo.com; #charset koi8-r; #access_log logs/host.access.log main; location / { proxy_pass http://tomcatserver1; index index.html index.htm; } } server { listen 80; server_name 8082.zcinfo.com; #charset koi8-r; #access_log logs/host.access.log main; location / { proxy_pass http://tomcatserver2; index index.html index.htm; } } 重启nginxps：如果在同一个域名下有多台服务器提供服务，此时需要nginx负载均衡。5、测试 至此恭喜您，nginx反向代理成功二、负载均衡1、什么是负载均衡？负载均衡建立在现有网络结构之上，它提供了一种廉价有效透明的方法扩展网络设备和服务器的带宽、增加吞吐量、加强网络数据处理能力、提高网络的灵活性和可用性。 负载均衡，英文名称为Load Balance，其意思就是分摊到多个操作单元上进行执行，例如Web服务器、FTP服务器、企业关键应用服务器和其它关键任务服务器等，从而共同完成工作任务。 2、需求 nginx作为负载均衡服务器，用户请求先到达nginx，再由nginx根据负载配置将请求转发至tomcat服务器。 nginx负载均衡服务器：192.168.3.43 tomcat1服务器：192.168.3.43:8080 tomcat2服务器：192.168.3.43:8081 3、nginx的配置 upstream tomcatserver1 { server 192.168.3.43:8080; server 192.168.3.43:8082; #多加了此台服务器 } upstream tomcatserver2 { server 192.168.3.43:8082; } server { listen 80; server_name 8080.zcinfo.com; #charset koi8-r; #access_log logs/host.access.log main; location / { proxy_pass http://tomcatserver1; index index.html index.htm; } } server { listen 80; server_name 8082.zcinfo.com; #charset koi8-r; #access_log logs/host.access.log main; location / { proxy_pass http://tomcatserver2; index index.html index.htm; } } 如果两台服务器性能差不多这样设置重启nginx就行了，但是现在假如两台服务器性能不一样，还需要设置性能权重，让性能高服务器做更多事情。只需要加入weight=?就行了，如下： upstream tomcatserver1 { server 192.168.3.43:8080 weight=2; server 192.168.3.43:8082 weight=1; } upstream tomcatserver2 { server 192.168.3.43:8082; } server { listen 80; server_name 8080.zcinfo.com; #charset koi8-r; #access_log logs/host.access.log main; location / { proxy_pass http://tomcatserver1; index index.html index.htm; } } server { listen 80; server_name 8082.zcinfo.com; #charset koi8-r; #access_log logs/host.access.log main; location / { proxy_pass http://tomcatserver2; index index.html index.htm; } } 重新启动nginx,会发现8080出现了两次，8082出现一次这样轮循。ps:关于nginx负载均衡的一些参数介绍例子 节点说明：在http节点里添加: #定义负载均衡设备的 Ip及设备状态upstream myServer { server 127.0.0.1:9090 down; server 127.0.0.1:8080 weight=2; server 127.0.0.1:6060; server 127.0.0.1:7070 backup; } 在需要使用负载的Server节点下添加 proxy_pass http://myServer; upstream 每个设备的状态: down 表示单前的server暂时不参与负载weight 默认为1.weight越大，负载的权重就越大。max_fails ：允许请求失败的次数默认为1.当超过最大次数时，返回proxy_next_upstream 模块定义的错误fail_timeout:max_fails 次失败后，暂停的时间。backup： 其它所有的非backup机器down或者忙的时候，请求backup机器。所以这台机器压力会最轻。 4、效果 ————————————————版权声明：本文为CSDN博主「演员赵诗绎」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/u013144287/article/details/78551398]]></content>
      <tags>
        <tag>项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式锁实现对比]]></title>
    <url>%2F2019%2F10%2F07%2F%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%AE%9E%E7%8E%B0%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[分布式情况下，怎么解决订单号生成不重复 使用分布式锁 提前生成好，订单号，存放在redis取。获取订单号，直接从redis中取。 使用分布式锁生成订单号技术1.使用数据库实现分布式锁 缺点:性能差、线程出现异常时，容易出现死锁 2.使用redis实现分布式锁 缺点:锁的失效时间难控制、容易产生死锁、非阻塞式、不可重入 3.**使用zookeeper实现分布式锁** 实现相对简单、可靠性强、使用临时节点，失效时间容易控制 什么是分布式锁分布式锁一般用在分布式系统或者多个应用中，用来控制同一任务是否执行或者任务的执行顺序。在项目中，部署了多个tomcat应用，在执行定时任务时就会遇到同一任务可能执行多次的情况，我们可以借助分布式锁，保证在同一时间只有一个tomcat应用执行了定时任务 使用Zookeeper实现分布式锁Zookeeper实现分布式锁原理使用zookeeper创建临时序列节点来实现分布式锁，适用于顺序执行的程序，大体思路就是创建临时序列节点，找出最小的序列节点，获取分布式锁，程序执行完成之后此序列节点消失，通过watch来监控节点的变化，从剩下的节点的找到最小的序列节点，获取分布式锁，执行相应处理，依次类推…… 实现步骤: 多个Jvm同时在Zookeeper上创建同一个相同的节点( /Lock) zk节点唯一的！ 不能重复！节点类型为临时节点， jvm1创建成功时候，jvm2和jvm3创建节点时候会报错，该节点已经存在。这时候 jvm2和jvm3进行等待。 jvm1的程序现在执行完毕，执行释放锁。关闭当前会话。临时节点不复存在了并且事件通知Watcher，jvm2和jvm3继续创建。 ps：zk强制关闭时候，通知会有延迟。但是close（）方法关闭时候，延迟小 如果程序一直不处理完，可能导致思索（其他的一直等待）。设置有效期~ 直接close（）掉 其实连接也是有有效期设置的 大家可以找下相关资料看下哦]]></content>
      <tags>
        <tag>分布式锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[幂等性保证机制]]></title>
    <url>%2F2019%2F10%2F06%2F%E5%B9%82%E7%AD%89%E6%80%A7%E4%BF%9D%E8%AF%81%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[幂等性概念在编程中.一个幂等操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同。指可以使用相同参数重复执行，并能获得相同结果，不用担心重复执行会对系统造成改变。 幂等性场景 查询操作：查询一次和查询多次，在数据不变的情况下，查询结果是一样的。select是天然的幂等操作； 删除操作：删除操作也是幂等的，删除一次和多次删除都是把数据删除。(注意可能返回结果不一样，删除的数据不存在，返回0，删除的数据多条，返回结果多个) ； 修改操作：多次修改，因为主键约束，修改一次和修改多次，如果数据没变，操作相同。 保证幂等性措施唯一索引防止新增脏数据。比如：支付宝的资金账户，支付宝也有用户账户，每个用户只能有一个资金账户，怎么防止给用户创建资金账户多个，那么给资金账户表中的用户ID加唯一索引，所以一个用户新增成功一个资金账户记录。要点：唯一索引或唯一组合索引来防止新增数据存在脏数据（当表存在唯一索引，并发时新增报错时，再查询一次就可以了，数据应该已经存在了，返回结果即可）；需要合适的业务唯一字段 token机制防止页面重复提交。原理上通过session token来实现的(也可以通过redis来实现)。当客户端请求页面时，服务器会生成一个随机数Token，并且将Token放置到session当中，然后将Token发给客户端（一般通过构造hidden表单）。下次客户端提交请求时，Token会随着表单一起提交到服务器端。 服务器端第一次验证相同过后，会将session中的Token值更新下，若用户重复提交，第二次的验证判断将失败，因为用户提交的表单中的Token没变，但服务器端session中Token已经改变了。 悲观锁获取数据的时候加锁获取。select * from table_xxx where id=’xxx’ for update;注意：id字段一定是主键或者唯一索引，不然是锁表，会死人的；悲观锁使用时一般伴随事务一起使用，数据锁定时间可能会很长，根据实际情况选用； 乐观锁只是在更新数据那一刻锁表，其他时间不锁表，所以相对于悲观锁，效率更高。乐观锁的实现方式多种多样可以通过version或者其他状态条件： 通过版本号实现update table_xxx set name=#{name},version=version+1 where version=#{version}； 通过条件限制 update table_xxx set avai_amount=avai_amount-#subAmount# whereavai_amount-#subAmount# &gt;= 0要求：quality-#subQuality# &gt;= 0，这个情景适合不用版本号，只更新是做数据安全校验，适合库存模型，扣份额和回滚份额，性能更高； 分布式锁如果是分布式系统，构建全局唯一索引比较困难，例如唯一性的字段没法确定，这时候可以引入分布式锁，通过第三方的系统(redis或zookeeper)，在业务系统插入数据或者更新数据，获取分布式锁，然后做操作，之后释放锁，这样其实是把多线程并发的锁的思路，引入多多个系统，也就是分布式系统中得解决思路。要点：某个长流程处理过程要求不能并发执行，可以在流程执行之前根据某个标志(用户ID+后缀等)获取分布式锁，其他流程执行时获取锁就会失败，也就是同一时间该流程只能有一个能执行成功，执行完成后，释放分布式锁(分布式锁要第三方系统提供)； select + insert并发不高的后台系统，或者一些任务JOB，为了支持幂等，支持重复执行，简单的处理方法是，先查询下一些关键数据，判断是否已经执行过，在进行业务处理，就可以了。注意：核心高并发流程不要用这种方法； 状态机幂等在设计单据相关的业务，或者是任务相关的业务，肯定会涉及到状态机(状态变更图)，就是业务单据上面有个状态，状态在不同的情况下会发生变更，一般情况下存在有限状态机，这时候，如果状态机已经处于下一个状态，这时候来了一个上一个状态的变更，理论上是不能够变更的，这样的话，保证了有限状态机的幂等。注意：订单等单据类业务，存在很长的状态流转，一定要深刻理解状态机，对业务系统设计能力提高有很大帮助 对外提供接口的api如何保证幂等如银联提供的付款接口：需要接入商户提交付款请求时附带：source来源，seq序列号；source+seq在数据库里面做唯一索引，防止多次付款(并发时，只能处理一个请求) 。重点：对外提供接口为了支持幂等调用，接口有两个字段必须传，一个是来源source，一个是来源方序列号seq，这个两个字段在提供方系统里面做联合唯一索引，这样当第三方调用时，先在本方系统里面查询一下，是否已经处理过，返回相应处理结果；没有处理过，进行相应处理，返回结果。注意，为了幂等友好，一定要先查询一下，是否处理过该笔业务，不查询直接插入业务系统，会报错，但实际已经处理了。]]></content>
      <tags>
        <tag>场景</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解AQS]]></title>
    <url>%2F2019%2F10%2F06%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3AQS%2F</url>
    <content type="text"><![CDATA[同步队列当共享资源被某个线程占有，其他请求该资源的线程将会阻塞，从而进入同步队列。就数据结构而言，队列的实现方式无外乎两者一是通过数组的形式，另外一种则是链表的形式。AQS中的同步队列则是通过链式方式进行实现。同步队列是带头结点的链式存储结构。 在AQS有一个静态内部类Node 1volatile int waitStatus //节点状态 volatile Node prev //当前节点/线程的前驱节点 volatile Node next; //当前节点/线程的后继节点 volatile Thread thread;//加入同步队列的线程引用 Node nextWaiter;//等待队列中的下一个节点 节点的状态有以下这些 1int CANCELLED = 1//节点从同步队列中取消 int SIGNAL = -1//后继节点的线程处于等待状态，如果当前节点释放同步状态会通知后继节点，使得后继节点的线程能够运行； int CONDITION = -2//当前节点进入等待队列中 int PROPAGATE = -3//表示下一次共享式同步状态获取将会无条件传播下去 int INITIAL = 0;//初始状态` AQS中有两个重要的成员变量，通过头尾指针来管理同步队列，同时实现包括获取锁失败的线程进行入队，释放锁时对同步队列中的线程进行通知 1private transient volatile Node head;private transient volatile Node tail; 调用lock()方法是获取独占式锁，获取失败就将当前线程加入同步队列，成功则线程执行。而lock()方法实际上会调用AQS的acquire()方法，源码如下 1//先看同步状态是否获取成功，如果成功则方法结束返回 //若失败则先调用addWaiter()方法再调用acquireQueued()方法 public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125; acquire根据当前获得同步状态成功与否做了两件事情：1. 成功，则方法结束返回，2. 失败，则先调用addWaiter()然后在调用acquireQueued()方法。 addWaiter()源码1private Node addWaiter(Node mode) &#123; // 1. 将当前线程构建成Node类型 Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure // 2. 当前尾节点是否为null？ Node pred = tail; if (pred != null) &#123; // 2.2 将当前节点尾插入的方式插入同步队列中 node.prev = pred; if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; // 2.1. 当前同步队列尾节点为null，说明当前线程是第一个加入同步队列进行等待的线程 enq(node); return node;&#125; 程序的逻辑主要分为两个部分：1. 当前同步队列的尾节点为null，调用方法enq()插入;2. 当前队列的尾节点不为null，则采用尾插入（compareAndSetTail（）方法）的方式入队。另外还会有另外一个问题：如果 if (compareAndSetTail(pred, node))为false怎么办？会继续执行到enq()方法，同时很明显compareAndSetTail是一个CAS操作，通常来说如果CAS操作失败会继续自旋（死循环,在enq中自旋）进行重试。 enq()源码1. 处理当前同步队列尾节点为null时进行入队操作；2. 如果CAS尾插入节点失败后负责自旋进行尝试。 1private Node enq(final Node node) &#123; for (;;) &#123; Node t = tail; if (t == null) &#123; // Must initialize //1. 构造头结点 if (compareAndSetHead(new Node())) tail = head; &#125; else &#123; // 2. 尾插入，CAS操作失败自旋尝试 node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125;&#125; 获取独占式锁失败的线程包装成Node然后插入同步队列的过程，线程）会做什么事情了来保证自己能够有机会获得独占式锁？ acquireQueued()源码1final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; // 1. 获得当前节点的先驱节点 final Node p = node.predecessor(); // 2. 当前节点能否获取独占式锁 // 2.1 如果当前节点的先驱节点是头结点并且成功获取同步状态，即可以获得独占式锁 // 链表是带头节点的表示头节点的下一个结点为第一个结点 if (p == head &amp;&amp; tryAcquire(arg)) &#123; //队列头指针用指向当前节点 setHead(node); //释放前驱节点 p.next = null; // help GC failed = false; //没有被中断，返回false，让selfInterrupt();不执行,让线程不被中断 return interrupted; &#125; // 2.2 获取锁失败，线程进入等待状态等待获取独占式锁 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 如果先驱节点是头结点的并且成功获得同步状态的时候（if (p == head &amp;&amp; tryAcquire(arg))），当前节点所指向的线程能够获取锁。反之，获取锁失败进入等待状态。 acquireQueued()在自旋过程中主要完成了两件事情： 如果当前节点的前驱节点是头节点，并且能够获得同步状态的话，当前线程能够获得锁该方法执行结束退出； 获取锁失败的话，先将当前结点的前驱结点状态设置成SIGNAL，然后调用LookSupport.park方法使得当前线程阻塞。 shouldParkAfterFailedAcquire()源码1private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; int ws = pred.waitStatus; if (ws == Node.SIGNAL) // 有个规定，就是当前结点的前驱结点状态一定要是-1，才能将当前结点park， // 相当于你告诉你前面的人排到你的时候叫我，我睡个觉 // 至于为什么前驱结点要是signal,等到release的时候有用 return true; if (ws &gt; 0) &#123; //ws大于0只有一个取消状态 //做个循环将取消结点去掉 do &#123; node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0); pred.next = node; &#125; else &#123; //CAS将前驱结点的ws设置成-1，自旋在aquareQueued里 compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; return false;&#125; parkAndCheckInterrupt()源码1private final boolean parkAndCheckInterrupt() &#123; //使得该线程阻塞 LockSupport.park(this); return Thread.interrupted();&#125; 独占锁的释放release()源码1public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false;&#125; 如果同步状态释放成功（tryRelease返回true）则会执行if块中的代码，当head指向的头结点不为null，并且该节点的状态值不为0的话才会执行unparkSuccessor()方法。 unparkSuccessor()源码1private void unparkSuccessor(Node node) &#123; /* * If status is negative (i.e., possibly needing signal) try * to clear in anticipation of signalling. It is OK if this * fails or if status is changed by waiting thread. */ // 如果头结点为负数，可能为-1，尝试将其ws设置为0，失败也没关系 int ws = node.waitStatus; if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); /* * Thread to unpark is held in successor, which is normally * just the next node. But if cancelled or apparently null, * traverse backwards from tail to find the actual * non-cancelled successor. */ //向后遍历找到头节点的后继节点，不能为取消结点 Node s = node.next; if (s == null || s.waitStatus &gt; 0) &#123; s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; if (s != null) //后继节点不为null时唤醒该线程 LockSupport.unpark(s.thread);&#125; 首先获取头节点的后继节点，当后继节点的时候会调用LookSupport.unpark()方法，该方法会唤醒该节点的后继节点所包装的线程。因此，每一次锁释放后就会唤醒队列中该节点的后继节点所引用的线程，从而进一步可以佐证获得锁的过程是一个FIFO（先进先出）的过程。 总结线程获取锁失败，线程被封装成Node进行入队操作，核心方法在于addWaiter()和enq()，同时enq()完成对同步队列的头结点初始化工作以及CAS操作失败的重试; 线程获取锁是一个自旋的过程，当且仅当 当前节点的前驱节点是头结点并且成功获得同步状态时，节点出队即该节点引用的线程获得锁，否则，当不满足条件时就会调用LookSupport.park()方法使得线程阻塞； 释放锁的时候会唤醒后继节点； 在获取同步状态时，AQS维护一个同步队列，获取同步状态失败的线程会加入到队列中进行自旋；移除队列（或停止自旋）的条件是前驱节点是头结点并且成功获得了同步状态。在释放同步状态时，同步器会调用unparkSuccessor()方法唤醒后继节点。 可中断式获取锁唯一的区别是当parkAndCheckInterrupt返回true时即线程阻塞时该线程被中断，代码抛出被中断异常。而上面只是interrupted = true; 超时等待式获取锁和上面类似，每次自旋都会重新计算超时时间 共享锁共享锁的原理和独占锁差不多，区别在于aquire（arg）大于0就能获取锁，每次都是arg– 释放大概是CAS释放多个线程，保证一定的顺序性]]></content>
      <tags>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis底层数据结构]]></title>
    <url>%2F2019%2F10%2F06%2Fredis%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[通俗易懂的Redis数据结构基础教程 Redis有5个基本数据结构，string、list、hash、set和zset。它们是日常开发中使用频率非常高应用最为广泛的数据结构，把这5个数据结构都吃透了，你就掌握了Redis应用知识的一半了。 string 首先我们从string谈起。string表示的是一个可变的字节数组，我们初始化字符串的内容、可以拿到字符串的长度，可以获取string的子串，可以覆盖string的子串内容，可以追加子串。 Redis的字符串是动态字符串，是可以修改的字符串，内部结构实现上类似于Java的ArrayList，采用预分配冗余空间的方式来减少内存的频繁分配，如图中所示，内部为当前字符串实际分配的空间capacity一般要高于实际字符串长度len。当字符串长度小于1M时，扩容都是加倍现有的空间，如果超过1M，扩容时一次只会多扩1M的空间。需要注意的是字符串最大长度为512M。 初始化字符串 需要提供「变量名称」和「变量的内容」 123&gt; set ireader beijing.zhangyue.keji.gufen.youxian.gongsiOK复制代码 获取字符串的内容 提供「变量名称」 123&gt; get ireader&quot;beijing.zhangyue.keji.gufen.youxian.gongsi&quot;复制代码 获取字符串的长度 提供「变量名称」 123&gt; strlen ireader(integer) 42复制代码 获取子串 提供「变量名称」以及开始和结束位置[start, end] 123&gt; getrange ireader 28 34&quot;youxian&quot;复制代码 覆盖子串 提供「变量名称」以及开始位置和目标子串 12345&gt; setrange ireader 28 wooxian(integer) 42 # 返回长度&gt; get ireader&quot;beijing.zhangyue.keji.gufen.wooxian.gongsi&quot;复制代码 追加子串 12345&gt; append ireader .hao(integer) 46 # 返回长度&gt; get ireader&quot;beijing.zhangyue.keji.gufen.wooxian.gongsi.hao&quot;复制代码 遗憾的是字符串没有提供字串插入方法和子串删除方法。 计数器 如果字符串的内容是一个整数，那么还可以将字符串当成计数器来使用。 1234567891011121314151617&gt; set ireader 42OK&gt; get ireader&quot;42&quot;&gt; incrby ireader 100(integer) 142&gt; get ireader&quot;142&quot;&gt; decrby ireader 100(integer) 42&gt; get ireader&quot;42&quot;&gt; incr ireader # 等价于incrby ireader 1(integer) 43&gt; decr ireader # 等价于decrby ireader 1(integer) 42复制代码 计数器是有范围的，它不能超过Long.Max，不能低于Long.MIN 123456789&gt; set ireader 9223372036854775807OK&gt; incr ireader(error) ERR increment or decrement would overflow&gt; set ireader -9223372036854775808OK&gt; decr ireader(error) ERR increment or decrement would overflow复制代码 过期和删除 字符串可以使用del指令进行主动删除，可以使用expire指令设置过期时间，到点会自动删除，这属于被动删除。可以使用ttl指令获取字符串的寿命。 123456789&gt; expire ireader 60(integer) 1 # 1表示设置成功，0表示变量ireader不存在&gt; ttl ireader(integer) 50 # 还有50秒的寿命，返回-2表示变量不存在，-1表示没有设置过期时间&gt; del ireader(integer) 1 # 删除成功返回1&gt; get ireader(nil) # 变量ireader没有了复制代码 list Redis将列表数据结构命名为list而不是array，是因为列表的存储结构用的是链表而不是数组，而且链表还是双向链表。因为它是链表，所以随机定位性能较弱，首尾插入删除性能较优。如果list的列表长度很长，使用时我们一定要关注链表相关操作的时间复杂度。 负下标 链表元素的位置使用自然数0,1,2,....n-1表示，还可以使用负数-1,-2,...-n来表示，-1表示「倒数第一」，-2表示「倒数第二」，那么-n就表示第一个元素，对应的下标为0。 队列／堆栈 链表可以从表头和表尾追加和移除元素，结合使用rpush/rpop/lpush/lpop四条指令，可以将链表作为队列或堆栈使用，左向右向进行都可以 123456789101112131415161718192021222324252627282930# 右进左出&gt; rpush ireader go(integer) 1&gt; rpush ireader java python(integer) 3&gt; lpop ireader&quot;go&quot;&gt; lpop ireader&quot;java&quot;&gt; lpop ireader&quot;python&quot;# 左进右出&gt; lpush ireader go java python(integer) 3&gt; rpop ireader&quot;go&quot;...# 右进右出&gt; rpush ireader go java python(integer) 3&gt; rpop ireader &quot;python&quot;...# 左进左出&gt; lpush ireader go java python(integer) 3&gt; lpop ireader&quot;python&quot;...复制代码 在日常应用中，列表常用来作为异步队列来使用。 长度 使用llen指令获取链表长度 12345&gt; rpush ireader go java python(integer) 3&gt; llen ireader(integer) 3复制代码 随机读 可以使用lindex指令访问指定位置的元素，使用lrange指令来获取链表子元素列表，提供start和end下标参数 12345678910111213&gt; rpush ireader go java python(integer) 3&gt; lindex ireader 1&quot;java&quot;&gt; lrange ireader 0 21) &quot;go&quot;2) &quot;java&quot;3) &quot;python&quot;&gt; lrange ireader 0 -1 # -1表示倒数第一1) &quot;go&quot;2) &quot;java&quot;3) &quot;python&quot;复制代码 使用lrange获取全部元素时，需要提供end_index，如果没有负下标，就需要首先通过llen指令获取长度，才可以得出end_index的值，有了负下标，使用-1代替end_index就可以达到相同的效果。 修改元素 使用lset指令在指定位置修改元素。 123456789&gt; rpush ireader go java python(integer) 3&gt; lset ireader 1 javascriptOK&gt; lrange ireader 0 -11) &quot;go&quot;2) &quot;javascript&quot;3) &quot;python&quot;复制代码 插入元素 使用linsert指令在列表的中间位置插入元素，有经验的程序员都知道在插入元素时，我们经常搞不清楚是在指定位置的前面插入还是后面插入，所以antirez在linsert指令里增加了方向参数before/after来显示指示前置和后置插入。不过让人意想不到的是linsert指令并不是通过指定位置来插入，而是通过指定具体的值。这是因为在分布式环境下，列表的元素总是频繁变动的，意味着上一时刻计算的元素下标在下一时刻可能就不是你所期望的下标了。 12345678910&gt; rpush ireader go java python(integer) 3&gt; linsert ireader before java ruby(integer) 4&gt; lrange ireader 0 -11) &quot;go&quot;2) &quot;ruby&quot;3) &quot;java&quot;4) &quot;python&quot;复制代码 到目前位置，我还没有在实际应用中发现插入指定的应用场景。 删除元素 列表的删除操作也不是通过指定下标来确定元素的，你需要指定删除的最大个数以及元素的值 12345678&gt; rpush ireader go java python(integer) 3&gt; lrem ireader 1 java(integer) 1&gt; lrange ireader 0 -11) &quot;go&quot;2) &quot;python&quot;复制代码 定长列表 在实际应用场景中，我们有时候会遇到「定长列表」的需求。比如要以走马灯的形式实时显示中奖用户名列表，因为中奖用户实在太多，能显示的数量一般不超过100条，那么这里就会使用到定长列表。维持定长列表的指令是ltrim，需要提供两个参数start和end，表示需要保留列表的下标范围，范围之外的所有元素都将被移除。 123456789&gt; rpush ireader go java python javascript ruby erlang rust cpp(integer) 8&gt; ltrim ireader -3 -1OK&gt; lrange ireader 0 -11) &quot;erlang&quot;2) &quot;rust&quot;3) &quot;cpp&quot;复制代码 如果指定参数的end对应的真实下标小于start，其效果等价于del指令，因为这样的参数表示需要需要保留列表元素的下标范围为空。 快速列表 如果再深入一点，你会发现Redis底层存储的还不是一个简单的linkedlist，而是称之为快速链表quicklist的一个结构。首先在列表元素较少的情况下会使用一块连续的内存存储，这个结构是ziplist，也即是压缩列表。它将所有的元素紧挨着一起存储，分配的是一块连续的内存。当数据量比较多的时候才会改成quicklist。因为普通的链表需要的附加指针空间太大，会比较浪费空间。比如这个列表里存的只是int类型的数据，结构上还需要两个额外的指针prev和next。所以Redis将链表和ziplist结合起来组成了quicklist。也就是将多个ziplist使用双向指针串起来使用。这样既满足了快速的插入删除性能，又不会出现太大的空间冗余。 hash 哈希等价于Java语言的HashMap或者是Python语言的dict，在实现结构上它使用二维结构，第一维是数组，第二维是链表，hash的内容key和value存放在链表中，数组里存放的是链表的头指针。通过key查找元素时，先计算key的hashcode，然后用hashcode对数组的长度进行取模定位到链表的表头，再对链表进行遍历获取到相应的value值，链表的作用就是用来将产生了「hash碰撞」的元素串起来。Java语言开发者会感到非常熟悉，因为这样的结构和HashMap是没有区别的。哈希的第一维数组的长度也是2^n。 增加元素 可以使用hset一次增加一个键值对，也可以使用hmset一次增加多个键值对 12345&gt; hset ireader go fast(integer) 1&gt; hmset ireader java fast python slowOK复制代码 获取元素 可以通过hget定位具体key对应的value，可以通过hmget获取多个key对应的value，可以使用hgetall获取所有的键值对，可以使用hkeys和hvals分别获取所有的key列表和value列表。这些操作和Java语言的Map接口是类似的。 1234567891011121314151617181920212223&gt; hmset ireader go fast java fast python slowOK&gt; hget ireader go&quot;fast&quot;&gt; hmget ireader go python1) &quot;fast&quot;2) &quot;slow&quot;&gt; hgetall ireader1) &quot;go&quot;2) &quot;fast&quot;3) &quot;java&quot;4) &quot;fast&quot;5) &quot;python&quot;6) &quot;slow&quot;&gt; hkeys ireader1) &quot;go&quot;2) &quot;java&quot;3) &quot;python&quot;&gt; hvals ireader1) &quot;fast&quot;2) &quot;fast&quot;3) &quot;slow&quot;复制代码 删除元素 可以使用hdel删除指定key，hdel支持同时删除多个key 1234567&gt; hmset ireader go fast java fast python slowOK&gt; hdel ireader go(integer) 1&gt; hdel ireader java python(integer) 2复制代码 判断元素是否存在 通常我们使用hget获得key对应的value是否为空就直到对应的元素是否存在了，不过如果value的字符串长度特别大，通过这种方式来判断元素存在与否就略显浪费，这时可以使用hexists指令。 12345&gt; hmset ireader go fast java fast python slowOK&gt; hexists ireader go(integer) 1复制代码 计数器 hash结构还可以当成计数器来使用，对于内部的每一个key都可以作为独立的计数器。如果value值不是整数，调用hincrby指令会出错。 123456789101112131415161718&gt; hincrby ireader go 1(integer) 1&gt; hincrby ireader python 4(integer) 4&gt; hincrby ireader java 4(integer) 4&gt; hgetall ireader1) &quot;go&quot;2) &quot;1&quot;3) &quot;python&quot;4) &quot;4&quot;5) &quot;java&quot;6) &quot;4&quot;&gt; hset ireader rust good(integer) 1&gt; hincrby ireader rust 1(error) ERR hash value is not an integer复制代码 扩容 当hash内部的元素比较拥挤时(hash碰撞比较频繁)，就需要进行扩容。扩容需要申请新的两倍大小的数组，然后将所有的键值对重新分配到新的数组下标对应的链表中(rehash)。如果hash结构很大，比如有上百万个键值对，那么一次完整rehash的过程就会耗时很长。这对于单线程的Redis里来说有点压力山大。所以Redis采用了渐进式rehash的方案。它会同时保留两个新旧hash结构，在后续的定时任务以及hash结构的读写指令中将旧结构的元素逐渐迁移到新的结构中。这样就可以避免因扩容导致的线程卡顿现象。 缩容 Redis的hash结构不但有扩容还有缩容，从这一点出发，它要比Java的HashMap要厉害一些，Java的HashMap只有扩容。缩容的原理和扩容是一致的，只不过新的数组大小要比旧数组小一倍。 setJava程序员都知道HashSet的内部实现使用的是HashMap，只不过所有的value都指向同一个对象。Redis的set结构也是一样，它的内部也使用hash结构，所有的value都指向同一个内部值。 增加元素 可以一次增加多个元素 123&gt; sadd ireader go java python(integer) 3复制代码 读取元素 使用smembers列出所有元素，使用scard获取集合长度，使用srandmember获取随机count个元素，如果不提供count参数，默认为1 1234567891011&gt; sadd ireader go java python(integer) 3&gt; smembers ireader1) &quot;java&quot;2) &quot;python&quot;3) &quot;go&quot;&gt; scard ireader(integer) 3&gt; srandmember ireader&quot;java&quot;复制代码 删除元素 使用srem删除一到多个元素，使用spop删除随机一个元素 1234567&gt; sadd ireader go java python rust erlang(integer) 5&gt; srem ireader go java(integer) 2&gt; spop ireader&quot;erlang&quot;复制代码 判断元素是否存在 使用sismember指令，只能接收单个元素 1234567&gt; sadd ireader go java python rust erlang(integer) 5&gt; sismember ireader rust(integer) 1&gt; sismember ireader javascript(integer) 0复制代码 sortedset SortedSet(zset)是Redis提供的一个非常特别的数据结构，一方面它等价于Java的数据结构Map&lt;String, Double&gt;，可以给每一个元素value赋予一个权重score，另一方面它又类似于TreeSet，内部的元素会按照权重score进行排序，可以得到每个元素的名次，还可以通过score的范围来获取元素的列表。 zset底层实现使用了两个数据结构，第一个是hash，第二个是跳跃列表，hash的作用就是关联元素value和权重score，保障元素value的唯一性，可以通过元素value找到相应的score值。跳跃列表的目的在于给元素value排序，根据score的范围获取元素列表。 增加元素 通过zadd指令可以增加一到多个value/score对，score放在前面 12345&gt; zadd ireader 4.0 python(integer) 1&gt; zadd ireader 4.0 java 1.0 go(integer) 2复制代码 长度 通过指令zcard可以得到zset的元素个数 123&gt; zcard ireader(integer) 3复制代码 删除元素 通过指令zrem可以删除zset中的元素，可以一次删除多个 123&gt; zrem ireader go python(integer) 2复制代码 计数器 同hash结构一样，zset也可以作为计数器使用。 12345&gt; zadd ireader 4.0 python 4.0 java 1.0 go(integer) 3&gt; zincrby ireader 1.0 python&quot;5&quot;复制代码 获取排名和分数 通过zscore指令获取指定元素的权重，通过zrank指令获取指定元素的正向排名，通过zrevrank指令获取指定元素的反向排名[倒数第一名]。正向是由小到大，负向是由大到小。 1234567891011&gt; zscore ireader python&quot;5&quot;&gt; zrank ireader go # 分数低的排名考前，rank值小(integer) 0&gt; zrank ireader java(integer) 1&gt; zrank ireader python(integer) 2&gt; zrevrank ireader python(integer) 0复制代码 根据排名范围获取元素列表 通过zrange指令指定排名范围参数获取对应的元素列表，携带withscores参数可以一并获取元素的权重。通过zrevrange指令按负向排名获取元素列表[倒数]。正向是由小到大，负向是由大到小。 12345678910111213141516171819&gt; zrange ireader 0 -1 # 获取所有元素1) &quot;go&quot;2) &quot;java&quot;3) &quot;python&quot;&gt; zrange ireader 0 -1 withscores1) &quot;go&quot;2) &quot;1&quot;3) &quot;java&quot;4) &quot;4&quot;5) &quot;python&quot;6) &quot;5&quot;&gt; zrevrange ireader 0 -1 withscores1) &quot;python&quot;2) &quot;5&quot;3) &quot;java&quot;4) &quot;4&quot;5) &quot;go&quot;6) &quot;1&quot;复制代码 根据score范围获取列表 通过zrangebyscore指令指定score范围获取对应的元素列表。通过zrevrangebyscore指令获取倒排元素列表。正向是由小到大，负向是由大到小。参数-inf表示负无穷，+inf表示正无穷。 12345678910111213141516171819&gt; zrangebyscore ireader 0 51) &quot;go&quot;2) &quot;java&quot;3) &quot;python&quot;&gt; zrangebyscore ireader -inf +inf withscores1) &quot;go&quot;2) &quot;1&quot;3) &quot;java&quot;4) &quot;4&quot;5) &quot;python&quot;6) &quot;5&quot;&gt; zrevrangebyscore ireader +inf -inf withscores # 注意正负反过来了1) &quot;python&quot;2) &quot;5&quot;3) &quot;java&quot;4) &quot;4&quot;5) &quot;go&quot;6) &quot;1&quot;复制代码 根据范围移除元素列表 可以通过排名范围，也可以通过score范围来一次性移除多个元素 123456789&gt; zremrangebyrank ireader 0 1(integer) 2 # 删掉了2个元素&gt; zadd ireader 4.0 java 1.0 go(integer) 2&gt; zremrangebyscore ireader -inf 4(integer) 2&gt; zrange ireader 0 -11) &quot;python&quot;复制代码 跳跃列表 zset内部的排序功能是通过「跳跃列表」数据结构来实现的，它的结构非常特殊，也比较复杂。这一块的内容深度读者要有心理准备。 因为zset要支持随机的插入和删除，所以它不好使用数组来表示。我们先看一个普通的链表结构。 我们需要这个链表按照score值进行排序。这意味着当有新元素需要插入时，需要定位到特定位置的插入点，这样才可以继续保证链表是有序的。通常我们会通过二分查找来找到插入点，但是二分查找的对象必须是数组，只有数组才可以支持快速位置定位，链表做不到，那该怎么办？ 想想一个创业公司，刚开始只有几个人，团队成员之间人人平等，都是联合创始人。随着公司的成长，人数渐渐变多，团队沟通成本随之增加。这时候就会引入组长制，对团队进行划分。每个团队会有一个组长。开会的时候分团队进行，多个组长之间还会有自己的会议安排。公司规模进一步扩展，需要再增加一个层级——部门，每个部门会从组长列表中推选出一个代表来作为部长。部长们之间还会有自己的高层会议安排。 跳跃列表就是类似于这种层级制，最下面一层所有的元素都会串起来。然后每隔几个元素挑选出一个代表来，再将这几个代表使用另外一级指针串起来。然后在这些代表里再挑出二级代表，再串起来。最终就形成了金字塔结构。 想想你老家在世界地图中的位置：亚洲–&gt;中国-&gt;安徽省-&gt;安庆市-&gt;枞阳县-&gt;汤沟镇-&gt;田间村-&gt;xxxx号，也是这样一个类似的结构。 「跳跃列表」之所以「跳跃」，是因为内部的元素可能「身兼数职」，比如上图中间的这个元素，同时处于L0、L1和L2层，可以快速在不同层次之间进行「跳跃」。 定位插入点时，先在顶层进行定位，然后下潜到下一级定位，一直下潜到最底层找到合适的位置，将新元素插进去。你也许会问那新插入的元素如何才有机会「身兼数职」呢？ 跳跃列表采取一个随机策略来决定新元素可以兼职到第几层，首先L0层肯定是100%了，L1层只有50%的概率，L2层只有25%的概率，L3层只有12.5%的概率，一直随机到最顶层L31层。绝大多数元素都过不了几层，只有极少数元素可以深入到顶层。列表中的元素越多，能够深入的层次就越深，能进入到顶层的概率就会越大。 这还挺公平的，能不能进入中央不是靠拼爹，而是看运气。 转载 掘金 老钱 原文链接：https://juejin.im/post/5b53ee7e5188251aaa2d2e16#heading-0]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CountDownLatch底层实现]]></title>
    <url>%2F2019%2F10%2F06%2FCountDownLatch%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[一、CountDownLatch 概述1.1 什么是 CountDLatch 闭锁（CountDownLatch）是 java.util.concurrent 包下的一种同步工具类。闭锁可以用来确保某些活动直到其他活动都完成后才执行。 闭锁相当于一扇门：在闭锁到达结束状态之前，这扇门一直是关闭的，并且没有任何线程能通过，当达到结束状态时，这扇门会打开，并允许所有的线程通过。 1.2 CountDownLatch 的应用场景 确保某个计算在其需要的所有资源都被初始化之后才执行确保某个服务在其依赖的所有其他服务都已经启动之后才启动等待直到每个操作的所有参与者都就绪再执行（比如打麻将时需要等待四个玩家就绪）1.3 CountDownLatch 简单应用 我们知道 4 个人玩纸牌游戏一定会先等所有玩家就绪后才会发牌，下面我们就来用闭锁简单的模拟一下。1234567891011121314151617public class CountDownLatchTest &#123; /** * 初始化需要等待的 4 个事件 */ private static CountDownLatch latch = new CountDownLatch(4); public static void main(String[] args) throws InterruptedException &#123; // 创建 4 个线程分别代表 4 个玩家 new Thread(() -&gt; &#123; System.out.println("玩家 1 已就绪"); latch.countDown(); &#125;).start(); new Thread(() -&gt; &#123; System.out.println("玩家 2 已就绪"); latch.countDown(); &#125;).start(); new Thread(() -&gt; &#123; System.out.println("玩家 3 已就绪"); latch.countDown(); &#125;).start(); new Thread(() -&gt; &#123; System.out.println("玩家 4 已就绪"); latch.countDown(); &#125;).start(); // 所有玩家就绪前一直阻塞 latch.await(); System.out.println("所有玩家已就绪，请发牌"); &#125; 下面是控制台输出： 二、CountDownLatch 原理分析CountDownLatch 底层是基于 AQS 实现的，如果不懂 AQS 原理的小伙伴需要先了解下 AQS 再来看这篇文章。 2.1 API 相关方法 构造函数： 12345public CountDownLatch(int count) &#123; if (count &lt; 0) throw new IllegalArgumentException("count &lt; 0"); // 初始化 count 值 this.sync = new Sync(count);&#125; CountDownLatch 内部有一个 Sync 同步对象，这个对象是一个内部类实现了 AQS，下面我们会具体来看方法实现。 await 方法： 12345public void await() throws InterruptedException &#123; // 共享式检查是否中断，如果中断抛出异常 // 调用 tryAcquireShared 方法尝试获取同步状态，当闭锁内的线程执行完毕后尝试获取成功，直接返回 sync.acquireSharedInterruptibly(1);&#125; countDown 方法： 1234public void countDown() &#123; // 调用 releaseShared 每次使同步状态值减 1 sync.releaseShared(1);&#125; 通过上面的 API 我们应该能知道其大概的原理了，在 CountDownLatch 初始化的时候会有一个初始的同步状态值，这个同步状态值可以理解为放行前的所要执行的线程数，每次调用 countDown 方法时就把同步状态值减 1，await 方法会自旋检查同步状态值是否为 0，当不为 0 时会阻塞线程，当为 0 时会直接返回，该方法是支持相应 中断的，当线程中断时会抛出异常。因此该方法可以理解为一扇门，只有当指定数量的线程执行完后，才会执行后续的代码。 上面我们已经理解了大概的流程，下面来看下具体的实现代码。 2.2 Sync 同步类 123456789101112131415161718192021222324252627282930private static final class Sync extends AbstractQueuedSynchronizer &#123; // 初始化闭锁 count 值 Sync(int count) &#123; setState(count); &#125; int getCount() &#123; return getState(); &#125; // 通过共享方式尝试获取锁 protected int tryAcquireShared(int acquires) &#123; return (getState() == 0) ? 1 : -1; &#125; // 通过共享方式尝试释放锁 // 因为该方法是线程共享的，因此需要通过 CAS 操作保证线程安全 protected boolean tryReleaseShared(int releases) &#123; for (;;) &#123; int c = getState(); // 同步状态值在上一次置 0 时已经放行，因此返回 false if (c == 0) return false; // 同步状态值 - 1 int nextc = c-1; // 为 0 时返回 true if (compareAndSetState(c, nextc)) return nextc == 0; &#125; &#125;&#125; ————————————————版权声明：本文为CSDN博主「留兰香丶」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/codejas/article/details/88407572]]></content>
      <tags>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MVCC]]></title>
    <url>%2F2019%2F10%2F06%2FMVCC%2F</url>
    <content type="text"><![CDATA[原文链接：https://blog.csdn.net/whoamiyang/article/details/51901888 MVCC简介1.1 什么是MVCCMVCC是一种多版本并发控制机制。 1.2 MVCC是为了解决什么问题?大多数的MYSQL事务型存储引擎,如,InnoDB，Falcon以及PBXT都不使用一种简单的行锁机制.事实上,他们都和MVCC–多版本并发控制来一起使用.大家都应该知道,锁机制可以控制并发操作,但是其系统开销较大,而MVCC可以在大多数情况下代替行级锁,使用MVCC,能降低其系统开销.1.3 MVCC实现MVCC是通过保存数据在某个时间点的快照来实现的. 不同存储引擎的MVCC. 不同存储引擎的MVCC实现是不同的,典型的有乐观并发控制和悲观并发控制. 2.MVCC 具体实现分析下面,我们通过InnoDB的MVCC实现来分析MVCC使怎样进行并发控制的.InnoDB的MVCC,是通过在每行记录后面保存两个隐藏的列来实现的,这两个列，分别保存了这个行的创建时间，一个保存的是行的删除时间。这里存储的并不是实际的时间值,而是系统版本号(可以理解为事务的ID)，没开始一个新的事务，系统版本号就会自动递增，事务开始时刻的系统版本号会作为事务的ID.下面看一下在REPEATABLE READ隔离级别下,MVCC具体是如何操作的. 2.1简单的小例子create table yang(id int primary key auto_increment,name varchar(20)); 假设系统的版本号从1开始. INSERTInnoDB为新插入的每一行保存当前系统版本号作为版本号.第一个事务ID为1； start transaction;insert into yang values(NULL,’yang’) ;insert into yang values(NULL,’long’);insert into yang values(NULL,’fei’);commit;12345对应在数据中的表如下(后面两列是隐藏列,我们通过查询语句并看不到) id name 创建时间(事务ID) 删除时间(事务ID)1 yang 1 undefined2 long 1 undefined3 fei 1 undefinedSELECTInnoDB会根据以下两个条件检查每行记录:a.InnoDB只会查找版本早于当前事务版本的数据行(也就是,行的系统版本号小于或等于事务的系统版本号)，这样可以确保事务读取的行，要么是在事务开始前已经存在的，要么是事务自身插入或者修改过的.b.行的删除版本要么未定义,要么大于当前事务版本号,这可以确保事务读取到的行，在事务开始之前未被删除.只有a,b同时满足的记录，才能返回作为查询结果. DELETEInnoDB会为删除的每一行保存当前系统的版本号(事务的ID)作为删除标识.看下面的具体例子分析:第二个事务,ID为2; start transaction;select from yang; //(1)select from yang; //(2)commit;1234假设1假设在执行这个事务ID为2的过程中,刚执行到(1),这时,有另一个事务ID为3往这个表里插入了一条数据;第三个事务ID为3; start transaction;insert into yang values(NULL,’tian’);commit;123这时表中的数据如下: id name 创建时间(事务ID) 删除时间(事务ID)1 yang 1 undefined2 long 1 undefined3 fei 1 undefined4 tian 3 undefined然后接着执行事务2中的(2),由于id=4的数据的创建时间(事务ID为3),执行当前事务的ID为2,而InnoDB只会查找事务ID小于等于当前事务ID的数据行,所以id=4的数据行并不会在执行事务2中的(2)被检索出来,在事务2中的两条select 语句检索出来的数据都只会下表: id name 创建时间(事务ID) 删除时间(事务ID)1 yang 1 undefined2 long 1 undefined3 fei 1 undefined假设2假设在执行这个事务ID为2的过程中,刚执行到(1),假设事务执行完事务3后，接着又执行了事务4;第四个事务: start transaction;delete from yang where id=1;commit;123此时数据库中的表如下: id name 创建时间(事务ID) 删除时间(事务ID)1 yang 1 42 long 1 undefined3 fei 1 undefined4 tian 3 undefined接着执行事务ID为2的事务(2),根据SELECT 检索条件可以知道,它会检索创建时间(创建事务的ID)小于当前事务ID的行和删除时间(删除事务的ID)大于当前事务的行,而id=4的行上面已经说过,而id=1的行由于删除时间(删除事务的ID)大于当前事务的ID,所以事务2的(2)select * from yang也会把id=1的数据检索出来.所以,事务2中的两条select 语句检索出来的数据都如下: id name 创建时间(事务ID) 删除时间(事务ID)1 yang 1 42 long 1 undefined3 fei 1 undefinedUPDATEInnoDB执行UPDATE，实际上是新插入了一行记录，并保存其创建时间为当前事务的ID，同时保存当前事务ID到要UPDATE的行的删除时间. 假设3假设在执行完事务2的(1)后又执行,其它用户执行了事务3,4,这时，又有一个用户对这张表执行了UPDATE操作:第5个事务: start transaction;update yang set name=’Long’ where id=2;commit;123根据update的更新原则:会生成新的一行,并在原来要修改的列的删除时间列上添加本事务ID,得到表如下: id name 创建时间(事务ID) 删除时间(事务ID)1 yang 1 42 long 1 53 fei 1 undefined4 tian 3 undefined2 Long 5 undefined继续执行事务2的(2),根据select 语句的检索条件,得到下表: id name 创建时间(事务ID) 删除时间(事务ID)1 yang 1 42 long 1 53 fei 1 undefined还是和事务2中(1)select 得到相同的结果.————————————————版权声明：本文为CSDN博主「杨龙飞的博客」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/whoamiyang/article/details/51901888]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Synchronize的可重入性]]></title>
    <url>%2F2019%2F10%2F04%2FSynchronize%E7%9A%84%E5%8F%AF%E9%87%8D%E5%85%A5%E6%80%A7%2F</url>
    <content type="text"><![CDATA[说一说自己对于 synchronized 关键字的了解synchronized关键字解决的是多个线程之间访问资源的同步性，synchronized关键字可以保证被它修饰的方法或者代码块在任意时刻只能有一个线程执行。 另外，在 Java 早期版本中，synchronized属于重量级锁，效率低下，因为监视器锁（monitor）是依赖于底层的操作系统的 Mutex Lock 来实现的，Java 的线程是映射到操作系统的原生线程之上的。如果要挂起或者唤醒一个线程，都需要操作系统帮忙完成，而操作系统实现线程之间的切换时需要从用户态转换到内核态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，这也是为什么早期的 synchronized 效率低的原因。庆幸的是在 Java 6 之后 Java 官方对从 JVM 层面对synchronized 较大优化，所以现在的 synchronized 锁效率也优化得很不错了。JDK1.6对锁的实现引入了大量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。 synchronize的可重入性在java 内部，同一个线程在调用自己类中其他synchronize方法/块或调用父类的synchronize方法/块都不会阻碍改线程的执行，就是说同一线程对同一个对象锁是可重入的，而且同一个线程可以获取同一把锁多次，也就是可以多次重入。 1234567891011121314151617181920212223public class Child extends Father &#123; public static void main(String[] args) &#123; Child child = new Child(); child.doSomething(); &#125; public synchronized void doSomething() &#123; System.out.println("child.doSomething()"); doAnotherThing(); // 调用自己类中其他的synchronized方法 &#125; private synchronized void doAnotherThing() &#123; super.doSomething(); // 调用父类的synchronized方法 System.out.println("child.doAnotherThing()"); &#125;&#125;class Father &#123; public synchronized void doSomething() &#123; System.out.println("father.doSomething()"); &#125;&#125; 12345运行结果：child.doSomething() father.doSomething() child.doAnotherThing() 这里的对象锁只有一个,就是child对象的锁,当执行child.doSomething时，该线程获得child对象的锁，在doSomething方法内执行doAnotherThing时再次请求child对象的锁，因为synchronized是重入锁，所以可以得到该锁，继续在doAnotherThing里执行父类的doSomething方法时第三次请求child对象的锁，同理可得到，如果不是重入锁的话，那这后面这两次请求锁将会被一直阻塞，从而导致死锁。 synchonized可重入锁的实现每个锁会关联一个线程持有者和一个计数器。当计数器为0时表示该锁没有被任何线程持有，那么任何线程都可能获得该锁而调用相应方法，当一个线程请求成功后，JVM会记下持有锁的线程，并将计数器计为1，此时其他线程请求该锁，则必须等待。而该持有锁的线程如果再次请求这个锁，就可以再次拿到这个锁，同时计数器会递增。当线程退出一个synchronize方法/块时,计数器会递减，如果计数器为0则释放该锁。 偏向锁、轻量级锁和重量级锁synchronized的偏向锁、轻量级锁以及重量级锁是通过Java对象头实现的。博主在Java对象大小内幕浅析中提到了Java对象的内存布局分为：对象头、实例数据和对齐填充，而对象头又可以分为”Mark Word”和类型指针klass。”Mark Word”是关键，默认情况下，其存储对象的HashCode、分代年龄和锁标记位。 这里说的都是以HotSpot虚拟机为基准的。首先来看一下”Mark Word”的内容： 状态 存储内容 标志位 无锁 对象的hashCode、对象分代年龄、是否是偏向锁（0） 01 轻量级 指向栈中锁记录的指针 00 重量级 指向互斥量（重量级锁）的指针 10 GC标记 （空） 11 偏向锁 偏向线程ID、偏向时间戳、对象分代年龄、是否是偏向锁（1） 01 偏向锁是JDK6中引入的一项锁优化，它的目的是消除数据在无竞争情况下的同步原语，进一步提高程序的运行性能。 偏向锁会偏向于第一个获得它的线程，如果在接下来的执行过程中，该锁没有被其他的线程获取，则持有偏向锁的线程将永远不需要同步。大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得，为了让线程获得锁的代价更低而引入了偏向锁。 当锁对象第一次被线程获取的时候，线程使用CAS操作把这个锁的线程ID记录在对象Mark Word之中，同时置偏向标志位1。以后该线程在进入和退出同步块时不需要进行CAS操作来加锁和解锁，只需要简单地测试一下对象头的Mark Word里是否存储着指向当前线程的偏向锁。如果测试成功，表示线程已经获得了锁。 如果线程使用CAS操作时失败则表示该锁对象上存在竞争并且这个时候另外一个线程获得偏向锁的所有权。当到达全局安全点（safepoint，这个时间点上没有正在执行的字节码）时获得偏向锁的线程被挂起，膨胀为轻量级锁（涉及Monitor Record，Lock Record相关操作，这里不展开），同时被撤销偏向锁的线程继续往下执行同步代码。 则当有另外一个线程去尝试获取这个锁时，偏向模式就宣告结束。 线程在执行同步块之前，JVM会先在当前线程的栈帧中创建用于存储锁记录(Lock Record)的空间，并将对象头中的Mard Word复制到锁记录中，官方称为Displaced Mark Word。然后线程尝试使用CAS将对象头中的Mark Word替换为指向锁记录的指针。如果成功，当前线程获得锁，如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁。如果自旋失败则锁会膨胀成重量级锁。如果自旋成功则依然处于轻量级锁的状态。 轻量级锁的解锁过程也是通过CAS操作来进行的，如果对象的Mark Word仍然指向线程的锁记录，那就用CAS操作把对象当前的Mark Word和线程中赋值的Displaced Mark Word替换回来，如果替换成功，整个同步过程就完成了，如果替换失败，就说明有其他线程尝试过获取该锁，那就要在释放锁的同时，唤醒被挂起的线程。 轻量级锁提升程序同步性能的依据是：对于绝大部分的锁，在整个同步周期内都是不存在竞争的（区别于偏向锁）。这是一个经验数据。如果没有竞争，轻量级锁使用CAS操作避免了使用互斥量的开销，但如果存在锁竞争，除了互斥量的开销外，还额外发生了CAS操作，因此在有竞争的情况下，轻量级锁比传统的重量级锁更慢。 整个synchronized锁流程如下： 检测Mark Word里面是不是当前线程的ID，如果是，表示当前线程处于偏向锁 如果不是，则使用CAS将当前线程的ID替换Mard Word，如果成功则表示当前线程获得偏向锁，置偏向标志位1 如果失败，则说明发生竞争，撤销偏向锁，进而升级为轻量级锁。 当前线程使用CAS将对象头的Mark Word替换为锁记录指针，如果成功，当前线程获得锁 如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁。 如果自旋成功则依然处于轻量级状态。 如果自旋失败，则升级为重量级锁。————————————————版权声明：本文为CSDN博主「朱小厮」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/u013256816/article/details/51204385]]></content>
      <tags>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模拟面试3]]></title>
    <url>%2F2019%2F10%2F03%2F%E6%A8%A1%E6%8B%9F%E9%9D%A2%E8%AF%953%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[缓存雪崩解决方案]]></title>
    <url>%2F2019%2F10%2F03%2F%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[相对来说，考虑的比较完善的一套方案，分为事前，事中，事后三个层次去思考怎么来应对缓存雪崩的场景 1、事前解决方案 发生缓存雪崩之前，事情之前，怎么去避免redis彻底挂掉 redis本身的高可用性，复制，主从架构，操作主节点，读写，数据同步到从节点，一旦主节点挂掉，从节点跟上 双机房部署，一套redis cluster，部分机器在一个机房，另一部分机器在另外一个机房 还有一种部署方式，两套redis cluster，两套redis cluster之间做一个数据的同步，redis集群是可以搭建成树状的结构的 一旦说单个机房出了故障，至少说另外一个机房还能有些redis实例提供服务 2、事中解决方案 redis cluster已经彻底崩溃了，已经开始大量的访问无法访问到redis了 （1）ehcache本地缓存 所做的多级缓存架构的作用上了，ehcache的缓存，应对零散的redis中数据被清除掉的现象，另外一个主要是预防redis彻底崩溃 多台机器上部署的缓存服务实例的内存中，还有一套ehcache的缓存 ehcache的缓存还能支撑一阵 （2）对redis访问的资源隔离 （3）对源服务访问的限流以及资源隔离 3、事后解决方案 （1）redis数据可以恢复，做了备份，redis数据备份和恢复，redis重新启动起来 （2）redis数据彻底丢失了，或者数据过旧，快速缓存预热，redis重新启动起来 redis对外提供服务 缓存服务里，熔断策略，自动可以恢复，half-open，发现redis可以访问了，自动恢复了，自动就继续去访问redis了 基于hystrix的高可用服务这块技术之后，先讲解缓存服务如何设计成高可用的架构 缓存架构应对高并发下的缓存雪崩的解决方案，基于hystrix去做缓存服务的保护 要带着大家去实现的有什么东西？事前和事后不用了吧 事中，ehcache本身也做好了 基于hystrix对redis的访问进行保护，对源服务的访问进行保护，讲解hystrix的时候，也说过对源服务的访问怎么怎么进行这种高可用的保护 但是站的角度不同，源服务如果自己本身不知道什么原因出了故障，我们怎么去保护，调用商品服务的接口大量的报错、超时 限流，资源隔离，降级]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程池学习及优化]]></title>
    <url>%2F2019%2F10%2F03%2F%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AD%A6%E4%B9%A0%E5%8F%8A%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[多线程并发生产环境里面，一个是线程池的大小怎么设置，timeout时长怎么 不合理的话，问题还是很大的 在生产环境中部署一个短路器，一开始需要将一些关键配置设置的大一些，比如timeout超时时长，线程池大小，或信号量容量 然后逐渐优化这些配置，直到在一个生产系统中运作良好 （1）一开始先不要设置timeout超时时长，默认就是1000ms，也就是1s（2）一开始也不要设置线程池大小，默认就是10（3）直接部署hystrix到生产环境，如果运行的很良好，那么就让它这样运行好了（4）让hystrix应用，24小时运行在生产环境中（5）依赖标准的监控和报警机制来捕获到系统的异常运行情况（6）在24小时之后，看一下调用延迟的占比，以及流量，来计算出让短路器生效的最小的配置数字（7）直接对hystrix配置进行热修改，然后继续在hystrix dashboard上监控（8）看看修改配置后的系统表现有没有改善 下面是根据系统表现优化和调整线程池大小，队列大小，信号量容量，以及timeout超时时间的经验 假设对一个依赖服务的高峰调用QPS是每秒30次 一开始如果默认的线程池大小是10 我们想的是，理想情况下，每秒的高峰访问次数 99%的访问延时 + buffer = 30 0.2 + 4 = 10线程，10个线程每秒处理30次访问应该足够了，每个线程处理3次访问 此时，我们合理的timeout设置应该为300ms，也就是99.5%的访问延时，计算方法是，因为判断每次访问延时最多在250ms（TP99如果是200ms的话），再加一次重试时间50ms，就是300ms，感觉也应该足够了 因为如果timeout设置的太多了，比如400ms，比如如果实际上，在高峰期，还有网络情况较差的时候，可能每次调用要耗费350ms，也就是达到了最长的访问时长 那么每个线程处理2个请求，就会执行700ms，然后处理第三个请求的时候，就超过1秒钟了，此时会导致线程池全部被占满，都在处理请求 这个时候下一秒的30个请求再进来了，那么就会导致线程池已满，拒绝请求的情况，就会调用fallback降级机制 因此对于短路器来说，timeout超时一般应该设置成TP99.5，比如设置成300ms，那么可以确保说，10个线程，每个线程处理3个访问，每个访问最多就允许执行300ms，过时就timeout了 这样才能保证说每个线程都在1s内执行完，才不会导致线程池被占满，然后后续的请求过来大量的reject 对于线程池大小来说，一般应该控制在10个左右，20个以内，最少5个，不要太多，也不要太少 大家可能会想，每秒的高峰访问次数是30次，如果是300次，甚至是3000次，30000次呢？？？ 30000 * 0.2 = 6000 + buffer = 6100，一个服务器内一个线程池给6000个线程把 如果你一个依赖服务占据的线程数量太多的话，会导致其他的依赖服务对应的线程池里没有资源可以用了 6000 / 20 = 300台虚拟机也是ok的 虚拟机，4个cpu core，4G内存，虚拟机，300台 物理机，十几个cpu core，几十个G的内存，5~8个虚拟机，300个虚拟机 = 50台物理机 你要真的说是，你的公司服务的用户量，或者数据量，或者请求量，真要是到了每秒几万的QPS， 3万QPS，60 * 3 = 180万访问量，1800，1亿8千，1亿，10个小时，10亿的访问量，app，系统 几十台服务器去支撑，我觉得很正常 QPS每秒在几千都算多的了 为什么要用线程池?线程池提供了一种限制和管理资源（包括执行一个任务）。 每个线程池还维护一些基本统计信息，例如已完成任务 的数量。 这里借用《Java并发编程的艺术》提到的来说一下使用线程池的好处： 降低资源消耗。 通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。 当任务到达时，任务可以不需要的等到线程创建就能立即执行。 提高线程的可管理性。 线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性， 使用线程池可以进行统一的分配，调优和监控。 线程池的核心原理和实现方法，7大参数讲一下 corePoolSize：表示核心线程池的大小。当提交一个任务时，如果当前核心线程池的线程个数没有达到corePoolSize，则会创建新的线程来执行所提交的任务，即使当前核心线程池有空闲的线程。如果当前核心线程池的线程个数已经达到了corePoolSize，则不再重新创建线程。如果调用了prestartCoreThread()或者 prestartAllCoreThreads()，线程池创建的时候所有的核心线程都会被创建并且启动。 maximumPoolSize：表示线程池能创建线程的最大个数。如果当阻塞队列已满时，并且当前线程池线程个数没有超过maximumPoolSize的话，就会创建新的线程来执行任务。 keepAliveTime：空闲线程存活时间。如果当前线程池的线程个数已经超过了corePoolSize，并且线程空闲时间超过了keepAliveTime的话，就会将这些空闲线程销毁，这样可以尽可能降低系统资源消耗。 unit：时间单位。为keepAliveTime指定时间单位。 workQueue：阻塞队列。用于保存任务的阻塞队列，关于阻塞队列可以看这篇文章。可以使用ArrayBlockingQueue, LinkedBlockingQueue, SynchronousQueue, PriorityBlockingQueue。 threadFactory：创建线程的工程类。可以通过指定线程工厂为每个创建出来的线程设置更有意义的名字，如果出现并发问题，也方便查找问题原因。 handler：饱和策略。当线程池的阻塞队列已满和指定的线程都已经开启，说明当前线程池已经处于饱和状态了，那么就需要采用一种策略来处理这种情况。采用的策略有这几种： AbortPolicy： 直接拒绝所提交的任务，并抛出RejectedExecutionException异常；CallerRunsPolicy：只用调用者所在的线程来执行任务；DiscardPolicy：不处理直接丢弃掉任务；DiscardOldestPolicy：丢弃掉阻塞队列中存放时间最久的任务，执行当前任务 4. 线程池的关闭关闭线程池，可以通过shutdown和shutdownNow这两个方法。它们的原理都是遍历线程池中所有的线程，然后依次中断线程。shutdown和shutdownNow还是有不一样的地方： shutdownNow首先将线程池的状态设置为STOP,然后尝试停止所有的正在执行和未执行任务的线程，并返回等待执行任务的列表； shutdown只是将线程池的状态设置为SHUTDOWN状态，然后中断所有没有正在执行任务的线程 可以看出shutdown方法会将正在执行的任务继续执行完，而shutdownNow会直接中断正在执行的任务。调用了这两个方法的任意一个，isShutdown方法都会返回true，当所有的线程都关闭成功，才表示线程池成功关闭，这时调用isTerminated方法才会返回true。 作者：你听___链接：https://juejin.im/post/5aeec0106fb9a07ab379574f来源：掘金著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 原理 线程池判断核心线程池是否都在执行任务。如果不是，则创建一个新的工作线程执行任务，如果都在执行任务，则进入下一个流程 线程池判断工作队列是否已经满了，如果不是，则将新提交的任务存储在这个工作队列中。如果工作队列满了，则进入下一个流程。 线程池判断线程池中的线程是否都处于工作状态，如果没有，则创建一个新的工作线程来执行任务，如果满了，则交给饱和策略来处理这个任务。 提交线程有execute和submit方法，execute方法用于没有返回值的任务，submit处理有返回的callable]]></content>
      <tags>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper分布式锁]]></title>
    <url>%2F2019%2F10%2F03%2Fzookeeper%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%2F</url>
    <content type="text"><![CDATA[一、什么是分布式锁？要介绍分布式锁，首先要提到与分布式锁相对应的是线程锁、进程锁。 线程锁：主要用来给方法、代码块加锁。当某个方法或代码使用锁，在同一时刻仅有一个线程执行该方法或该代码段。线程锁只在同一JVM中有效果，因为线程锁的实现在根本上是依靠线程之间共享内存实现的，比如synchronized是共享对象头，显示锁Lock是共享某个变量（state）。 进程锁：为了控制同一操作系统中多个进程访问某个共享资源，因为进程具有独立性，各个进程无法访问其他进程的资源，因此无法通过synchronized等线程锁实现进程锁。 分布式锁：当多个进程不在同一个系统中，用分布式锁控制多个进程对资源的访问。 二、分布式锁的使用场景。线程间并发问题和进程间并发问题都是可以通过分布式锁解决的，但是强烈不建议这样做！因为采用分布式锁解决这些小问题是非常消耗资源的！分布式锁应该用来解决分布式情况下的多进程并发问题才是最合适的。 有这样一个情境，线程A和线程B都共享某个变量X。 如果是单机情况下（单JVM），线程之间共享内存，只要使用线程锁就可以解决并发问题。 如果是分布式情况下（多JVM），线程A和线程B很可能不是在同一JVM中，这样线程锁就无法起到作用了，这时候就要用到分布式锁来解决。 缓存重建问题及解决整个三级缓存的架构已经走通了 我们还遇到一个问题，就是说，如果缓存服务在本地的ehcache中都读取不到数据，那就恩坑爹了 这个时候就意味着，需要重新到源头的服务中去拉去数据，拉取到数据之后，赶紧先给nginx的请求返回，同时将数据写入ehcache和redis中 分布式重建缓存的并发冲突问题 重建缓存：比如我们这里，数据在所有的缓存中都不存在了（LRU算法弄掉了），就需要重新查询数据写入缓存，重建缓存分布式的重建缓存，在不同的机器上，不同的服务实例中，去做上面的事情，就会出现多个机器分布式重建去读取相同的数据，然后写入缓存中 分布式重建缓存的并发冲突问题。。。。。。 1、流量均匀分布到所有缓存服务实例上 应用层nginx，是将请求流量均匀地打到各个缓存服务实例中的，可能咱们的eshop-cache那个服务，可能会部署多实例在不同的机器上 2、应用层nginx的hash，固定商品id，走固定的缓存服务实例 分发层的nginx的lua脚本，是怎么写的，怎么玩儿的，搞一堆应用层nginx的地址列表，对每个商品id做一个hash，然后对应用nginx数量取模 将每个商品的请求固定分发到同一个应用层nginx上面去 在应用层nginx里，发现自己本地lua shared dict缓存中没有数据的时候，就采取一样的方式，对product id取模，然后将请求固定分发到同一个缓存服务实例中去 这样的话，就不会出现说多个缓存服务实例分布式的去更新那个缓存了 留个作业，大家去做吧，这个东西，之前已经讲解果了，lua脚本几乎都是一模一样的，我们就不去做了，节省点时间 3、源信息服务发送的变更消息，需要按照商品id去分区，固定的商品变更走固定的kafka分区，也就是固定的一个缓存服务实例获取到 缓存服务，是监听kafka topic的，一个缓存服务实例，作为一个kafka consumer，就消费topic中的一个partition 所以你有多个缓存服务实例的话，每个缓存服务实例就消费一个kafka partition 所以这里，一般来说，你的源头信息服务，在发送消息到kafka topic的时候，都需要按照product id去分区 也就时说，同一个product id变更的消息一定是到同一个kafka partition中去的，也就是说同一个product id的变更消息，一定是同一个缓存服务实例消费到的 我们也不去做了，其实很简单，kafka producer api，里面send message的时候，多加一个参数就可以了，product id传递进去，就可以了 4、问题是，自己写的简易的hash分发，与kafka的分区，可能并不一致！！！ 我们自己写的简易的hash分发策略，是按照crc32去取hash值，然后再取模的 关键你又不知道你的kafka producer的hash策略是什么，很可能说跟我们的策略是不一样的 拿就可能导致说，数据变更的消息所到的缓存服务实例，跟我们的应用层nginx分发到的那个缓存服务实例也许就不在一台机器上了 这样的话，在高并发，极端的情况下，可能就会出现冲突 5、分布式的缓存重建并发冲突问题发生了。。。 6、基于zookeeper分布式锁的解决方案 分布式锁，如果你有多个机器在访问同一个共享资源，那么这个时候，如果你需要加个锁，让多个分布式的机器在访问共享资源的时候串行起来 那么这个时候，那个锁，多个不同机器上的服务共享的锁，就是分布式锁 分布式锁当然有很多种不同的实现方案，redis分布式锁，zookeeper分布式锁 zk，做分布式协调这一块，还是很流行的，大数据应用里面，hadoop，storm，都是基于zk去做分布式协调 zk分布式锁的解决并发冲突的方案 （1）变更缓存重建以及空缓存请求重建，更新redis之前，都需要先获取对应商品id的分布式锁（2）拿到分布式锁之后，需要根据时间版本去比较一下，如果自己的版本新于redis中的版本，那么就更新，否则就不更新（3）如果拿不到分布式锁，那么就等待，不断轮询等待，直到自己获取到分布式的锁 分布锁实现一zk分布式锁的代码封装 zookeeper java client api去封装连接zk，以及获取分布式锁，还有释放分布式锁的代码 先简单介绍一下zk分布式锁的原理 我们通过去创建zk的一个临时node，来模拟给摸一个商品id加锁 zk会给你保证说，只会创建一个临时node，其他请求过来如果再要创建临时node，就会报错，NodeExistsException 那么所以说，我们的所谓上锁，其实就是去创建某个product id对应的一个临时node 如果临时node创建成功了，那么说明我们成功加锁了，此时就可以去执行对redis立面数据的操作 如果临时node创建失败了，说明有人已经在拿到锁了，在操作reids中的数据，那么就不断的等待，直到自己可以获取到锁为止 基于zk client api，去封装上面的这个代码逻辑 释放一个分布式锁，去删除掉那个临时node就可以了，就代表释放了一个锁，那么此时其他的机器就可以成功创建临时node，获取到锁 即使是用zk去实现一个分布式锁，也有很多种做法，有复杂的，也有简单的 应该说，我演示的这种分布式锁的做法，是非常简单的一种，但是很实用，大部分情况下，用这种简单的分布式锁都能搞定 1234连接zookeeper客户端实例代码逻辑：使用countdownlauch来控制多线程的并发。CountDownLatch connectedSemaphore = new CountDownLatch(1);zookeeper在连接客户端的时候，是一个异步的过程，在连接的下一步中通过connectedSemaphore.await()阻塞线程。封装一个监听器，监听zookeeper连接，当连接成功的时候，connectedSemaphore.countdown(),释放线程。 12分布式加锁封装逻辑：通过商品id创建一个zookeeper临时节点，如果创建失败，说明此时有人已经创建了。会抛出异常，那么在catch中进入死循环，不断的尝试创建临时节点，直到创建成功，跳出死循环。 12分布式释放锁的逻辑：释放一个分布式锁，只要删除这个商品id创建的临时节点就可以。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148&lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.5&lt;/version&gt;&lt;/dependency&gt;/** * ZooKeeperSession * @author Administrator * */public class ZooKeeperSession &#123; private static CountDownLatch connectedSemaphore = new CountDownLatch(1); private ZooKeeper zookeeper; public ZooKeeperSession() &#123; // 去连接zookeeper server，创建会话的时候，是异步去进行的 // 所以要给一个监听器，说告诉我们什么时候才是真正完成了跟zk server的连接 try &#123; this.zookeeper = new ZooKeeper( &quot;192.168.31.187:2181,192.168.31.19:2181,192.168.31.227:2181&quot;, 50000, new ZooKeeperWatcher()); // 给一个状态CONNECTING，连接中 System.out.println(zookeeper.getState()); try &#123; // CountDownLatch // java多线程并发同步的一个工具类 // 会传递进去一些数字，比如说1,2 ，3 都可以 // 然后await()，如果数字不是0，那么久卡住，等待 // 其他的线程可以调用coutnDown()，减1 // 如果数字减到0，那么之前所有在await的线程，都会逃出阻塞的状态 // 继续向下运行 connectedSemaphore.await(); &#125; catch(InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;ZooKeeper session established......&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 获取分布式锁 * @param productId */ public void acquireDistributedLock(Long productId) &#123; String path = &quot;/product-lock-&quot; + productId; try &#123; zookeeper.create(path, &quot;&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); System.out.println(&quot;success to acquire lock for product[id=&quot; + productId + &quot;]&quot;); &#125; catch (Exception e) &#123; // 如果那个商品对应的锁的node，已经存在了，就是已经被别人加锁了，那么就这里就会报错 // NodeExistsException int count = 0; while(true) &#123; try &#123; Thread.sleep(20); zookeeper.create(path, &quot;&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); &#125; catch (Exception e2) &#123; e2.printStackTrace(); count++; continue; &#125; System.out.println(&quot;success to acquire lock for product[id=&quot; + productId + &quot;] after &quot; + count + &quot; times try......&quot;); break; &#125; &#125; &#125; /** * 释放掉一个分布式锁 * @param productId */ public void releaseDistributedLock(Long productId) &#123; String path = &quot;/product-lock-&quot; + productId; try &#123; zookeeper.delete(path, -1); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 建立zk session的watcher * @author Administrator * */ private class ZooKeeperWatcher implements Watcher &#123; public void process(WatchedEvent event) &#123; System.out.println(&quot;Receive watched event: &quot; + event.getState()); if(KeeperState.SyncConnected == event.getState()) &#123; connectedSemaphore.countDown(); &#125; &#125; &#125; /** * 封装单例的静态内部类 * @author Administrator * */ private static class Singleton &#123; private static ZooKeeperSession instance; static &#123; instance = new ZooKeeperSession(); &#125; public static ZooKeeperSession getInstance() &#123; return instance; &#125; &#125; /** * 获取单例 * @return */ public static ZooKeeperSession getInstance() &#123; return Singleton.getInstance(); &#125; /** * 初始化单例的便捷方法 */ public static void init() &#123; getInstance(); &#125; &#125; 获取分布式锁写缓存的业务逻辑业务代码 1、主动更新 监听kafka消息队列，获取到一个商品变更的消息之后，去哪个源服务中调用接口拉取数据，更新到ehcache和redis中 先获取分布式锁，然后才能更新redis，同时更新时要比较时间版本 1先获取分布式锁，从redis中拿取相同key的商品id的缓存数据，比较时间版本，如果时间更新，则覆盖缓存，反之，不更新缓存。 2、被动重建 直接读取源头数据，直接返回给nginx，同时推送一条消息到一个队列，后台线程异步消费 后台现成负责先获取分布式锁，然后才能更新redis，同时要比较时间版本]]></content>
      <tags>
        <tag>项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模拟第二天]]></title>
    <url>%2F2019%2F10%2F02%2F%E6%A8%A1%E6%8B%9F%E7%AC%AC%E4%BA%8C%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[HTTPS的工作原理（1）客户使用https的URL访问Web服务器，要求与Web服务器建立SSL连接。 （2）Web服务器收到客户端请求后，会将网站的证书信息（证书中包含公钥）传送一份给客户端。 （3）客户端的浏览器与Web服务器开始协商SSL连接的安全等级，也就是信息加密的等级。 （4）客户端的浏览器根据双方同意的安全等级，建立会话密钥，然后利用网站的公钥将会话密钥加密，并传送给网站。 （5）Web服务器利用自己的私钥解密出会话密钥。 （6）Web服务器利用会话密钥加密与客户端之间的通信。 Synchronized和ReentrantLock的区别① 两者都是可重入锁两者都是可重入锁。“可重入锁”概念是：自己可以再次获取自己的内部锁。比如一个线程获得了某个对象的锁，此时这个对象锁还没有释放，当其再次想要获取这个对象的锁的时候还是可以获取的，如果不可锁重入的话，就会造成死锁。同一个线程每次获取锁，锁的计数器都自增1，所以要等到锁的计数器下降为0时才能释放锁。 ②synchronized依赖于JVM而ReenTrantLock依赖于APIsynchronized 是依赖于 JVM 实现的，前面我们也讲到了 虚拟机团队在 JDK1.6 为 synchronized 关键字进行了很多优化，但是这些优化都是在虚拟机层面实现的，并没有直接暴露给我们。ReenTrantLock 是 JDK 层面实现的（也就是 API 层面，需要 lock() 和 unlock 方法配合 try/fifinally 语句块来完成），所以我们可以通过查看它的源代码，来看它是如何实现的。 ③ReenTrantLock比synchronized增加了一些高级功能相比synchronized，ReenTrantLock增加了一些高级功能。主要来说主要有三点： ①等待可中断ReenTrantLock提供了一种能够中断等待锁的线程的机制，通过lock.lockInterruptibly()来实现这个机制。也就是说正在等待的线程可以选择放弃等待，改为处理其他事情。 ②可实现公平锁ReenTrantLock可以指定是公平锁还是非公平锁。而synchronized只能是非公平锁。所谓的公平锁就是先等待的线程先获得锁。ReenTrantLock默认情况是非公平的，可以通过 ReenTrantLock类的ReentrantLock(boolean fair) 构造方法来制定是否是公平的。 ③可实现选择性通知（锁可以绑定多个条件）synchronized关键字与wait()和notify/notifyAll()方法相结合可以实现等待/通知机制，ReentrantLock类当然也 可以实现，但是需要借助于Condition接口与newCondition() 方法。 Condition是JDK1.5之后才有的，它具有很好的灵活性，比如可以实现多路通知功能也就是在一个Lock对象中可以创建多个Condition实例（即对象监视器），线程对象可以注册在指定的Condition中，从而可以有选择性的进行线程通知，在调度线程上更加灵活。 在使用notify/notifyAll()方法进行通知时，被通知的线程是由JVM选择的，用ReentrantLock类结合Condition实例可以实现”选择性通知”，这个功能非常重要，而且是Condition接口默认提供的。而 synchronized关键字就相当于整个Lock对象中只有一个Condition实例，所有的线程都注册在它一个身上。如果执行notifyAll()方法的话就会通知所有处于等待状态的线程这样会造成很大的效率问题，而Condition实例的 signalAll()方法 只会唤醒注册在该Condition实例中的所有等待线程。 聚簇索引和非聚簇索引（二级索引）myisam索引：因为myisam的索引和数据是分开存储存储的，myisam通过key_buffer把索引先缓存到内存中，当需要访问数据时（通过索引访问数据），在内存中直接搜索索引，然后通过索引找到磁盘相应数据，这也就是为什么索引不在key buffer命中时，速度慢的原因 innodb索引：innodb的数据和索引放在一起，当找到索引也就找到了数据 1234MySQL的BTree索引使用的是B树中的B+Tree，但对于主要的两种存储引擎的实现方式是不同的。MyISAM: B+Tree叶节点的data域存放的是数据记录的地址。在索引检索的时候，首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其 data 域的值，然后以 data 域的值为地址读取相应的数据记录。这被称为“非聚簇索引”。InnoDB: 其数据文件本身就是索引文件。相比MyISAM，索引文件和数据文件是分离的，其表数据文件本身就是按B+Tree组织的一个索引结构，树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。这被称为“聚簇索引（或聚集索引）”。而其余的索引都作为辅助索引，辅助索引的data域存储相应记录主键的值而不是地址，这也是和MyISAM不同的地方。在根据主索引搜索时，直接找到key所在的节点即可取出数据；在根据辅助索引查找时，则需要先取出主键的值，再走一遍主索引。 因此，在设计表的时候，不建议使用过长的字段作为主键，也不建议使用非单调的字段作为主键，这样会造成主索引频繁分裂。 PS：整理自《Java工程师修炼之道》 mysql的聚簇索引是指innodb引擎的特性，mysiam并没有，如果需要该索引，只要将索引指定为主键（primary key）就可以了。 比如： 12345678create table blog_user( user_Name char(15) not null check(user_Name !=''), user_Password char(15) not null, user_emial varchar(20) not null unique, primary key(user_Name) )engine=innodb default charset=utf8 auto_increment=1; 其中的 primary key(user_Name) 这个就是聚簇索引索引了；聚簇索引的叶节点就是数据节点，而非聚簇索引的叶节点仍然是索引节点，并保留一个链接指向对应数据块。 聚簇索引主键的插入速度要比非聚簇索引主键的插入速度慢很多。相比之下，聚簇索引适合排序，非聚簇索引（也叫二级索引）不适合用在排序的场合。因为聚簇索引本身已经是按照物理顺序放置的，排序很快。非聚簇索引则没有按序存放，需要额外消耗资源来排序。当你需要取出一定范围内的数据时，用聚簇索引也比用非聚簇索引好。另外，二级索引需要两次索引查找，而不是一次才能取到数据，因为存储引擎第一次需要通过二级索引找到索引的叶子节点，从而找到数据的主键，然后在聚簇索引中用主键再次查找索引，再找到数据。 innodb索引分类：聚簇索引(clustered index) 1) 有主键时，根据主键创建聚簇索引 2) 没有主键时，会用一个唯一且不为空的索引列做为主键，成为此表的聚簇索引 3) 如果以上两个都不满足那innodb自己创建一个虚拟的聚集索引辅助索引(secondary index) 非聚簇索引都是辅助索引，像复合索引、前缀索引、唯一索引————————————————版权声明：本文为CSDN博主「大树叶」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/bigtree_3721/article/details/51335479]]></content>
      <tags>
        <tag>模拟练习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper+kafka+nginx]]></title>
    <url>%2F2019%2F10%2F02%2Fzookeeper-kafka%2F</url>
    <content type="text"><![CDATA[zookeeper 与 kafka集群的搭建多级缓存的架构 主要是用来解决什么样的数据的缓存的更新的啊？？？ 时效性不高的数据，比如一些商品的基本信息，如果发生了变更，假设在5分钟之后再更新到页面中，供用户观察到，也是ok的 时效性要求不高的数据，那么我们采取的是异步更新缓存的策略 时效性要求很高的数据，库存，采取的是数据库+缓存双写的技术方案，也解决了双写的一致性的问题 缓存数据生产服务，监听一个消息队列，然后数据源服务（商品信息管理服务）发生了数据变更之后，就将数据变更的消息推送到消息队列中 缓存数据生产服务可以去消费到这个数据变更的消息，然后根据消息的指示提取一些参数，然后调用对应的数据源服务的接口，拉去数据，这个时候一般是从mysql库中拉去的 消息队列是什么东西？采取打的就是kafka 我工作的时候，很多项目是跟大数据相关的，当然也有很多是纯java系统的架构，最近用kafka用得比较多 kafka比较简单易用，讲课来说，很方便 解释一下，我们当然是不可能对课程中涉及的各种技术都深入浅出的讲解的了，kafka，花上20个小时给你讲解一下，不可能的 所以说呢，在这里，一些技术的组合，用什么都ok 笑傲江湖中的风清扬，手中无剑胜有剑，还有任何东西都可以当做兵器，哪怕是一根草也可以 搞技术，kafka和activemq肯定有区别，但是说，在有些场景下，其实可能没有那么大的区分度，kafka和activemq其实是一样的 生产者+消费者的场景，kafka+activemq都ok 涉及的这种架构，对时效性要求高和时效性要求低的数据，分别采取什么技术方案？数据库+缓存双写一致性？异步+多级缓存架构？大缓存的维度化拆分？ 你要关注的，是一些架构上的东西和思想，而不是具体的什么mq的使用 activemq的课程，书籍，资料 kafka集群，zookeeper集群，先搭建zookeeper集群，再搭建kafka集群 kafka另外一个原因：kafka，本来就要搭建zookeeper，zookeeper这个东西，后面我们还要用呢，缓存的分布式并发更新的问题，分布式锁解决 zookeeper + kafka的集群，都是三节点 java高级工程师的思想，在干活儿，在思考，jvm，宏观的思考，通盘去考虑整个架构，还有未来的技术规划，业务的发展方向，架构的演进方向和路线 把课程里讲解的各种技术方案组合成、修改成你需要的适合你的业务的缓存架构 1、zookeeper集群搭建 将课程提供的zookeeper-3.4.5.tar.gz使用WinSCP拷贝到/usr/local目录下。对zookeeper-3.4.5.tar.gz进行解压缩：tar -zxvf zookeeper-3.4.5.tar.gz。对zookeeper目录进行重命名：mv zookeeper-3.4.5 zk 配置zookeeper相关的环境变量vi ~/.bashrcexport ZOOKEEPER_HOME=/usr/local/zkexport PATH=$PATH:$ZOOKEEPER_HOME/bin source ~/.bashrc cd zk/confcp zoo_sample.cfg zoo.cfg vi zoo.cfg修改：dataDir=/usr/local/zk/data新增：server.0=eshop-cache01:2888:3888server.1=eshop-cache02:2888:3888server.2=eshop-cache03:2888:3888 cd zkmkdir datacd data vi myid0 在另外两个节点上按照上述步骤配置ZooKeeper，使用scp将zk和.bashrc拷贝到eshop-cache02和eshop-cache03上即可。唯一的区别是标识号分别设置为1和2。 分别在三台机器上执行：zkServer.sh start。检查ZooKeeper状态：zkServer.sh status，应该是一个leader，两个followerjps：检查三个节点是否都有QuromPeerMain进程 2、kafka集群搭建 scala，我就不想多说了，就是一门编程语言，现在比较火，很多比如大数据领域里面的spark（计算引擎）就是用scala编写的 将课程提供的scala-2.11.4.tgz使用WinSCP拷贝到/usr/local目录下。对scala-2.11.4.tgz进行解压缩：tar -zxvf scala-2.11.4.tgz。对scala目录进行重命名：mv scala-2.11.4 scala 配置scala相关的环境变量vi ~/.bashrcexport SCALA_HOME=/usr/local/scalaexport PATH=$SCALA_HOME/binsource ~/.bashrc 查看scala是否安装成功：scala -version 按照上述步骤在其他机器上都安装好scala。使用scp将scala和.bashrc拷贝到另外两台机器上即可。 将课程提供的kafka_2.9.2-0.8.1.tgz使用WinSCP拷贝到/usr/local目录下。对kafka_2.9.2-0.8.1.tgz进行解压缩：tar -zxvf kafka_2.9.2-0.8.1.tgz。对kafka目录进行改名：mv kafka_2.9.2-0.8.1 kafka 配置kafkavi /usr/local/kafka/config/server.propertiesbroker.id：依次增长的整数，0、1、2，集群中Broker的唯一idzookeeper.connect=192.168.31.187:2181,192.168.31.19:2181,192.168.31.227:2181 安装slf4j将课程提供的slf4j-1.7.6.zip上传到/usr/local目录下unzip slf4j-1.7.6.zip把slf4j中的slf4j-nop-1.7.6.jar复制到kafka的libs目录下面 解决kafka Unrecognized VM option ‘UseCompressedOops’问题 vi /usr/local/kafka/bin/kafka-run-class.sh if [ -z “$KAFKA_JVM_PERFORMANCE_OPTS” ]; then KAFKA_JVM_PERFORMANCE_OPTS=”-server -XX:+UseCompressedOops -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSClassUnloadingEnabled -XX:+CMSScavengeBeforeRemark -XX:+DisableExplicitGC -Djava.awt.headless=true”fi 去掉-XX:+UseCompressedOops即可 按照上述步骤在另外两台机器分别安装kafka。用scp把kafka拷贝到其他机器即可。唯一区别的，就是server.properties中的broker.id，要设置为1和2 在三台机器上的kafka目录下，分别执行以下命令：nohup bin/kafka-server-start.sh config/server.properties &amp; 使用jps检查启动是否成功 使用基本命令检查kafka是否搭建成功 bin/kafka-topics.sh –zookeeper 192.168.229.7:2181,192.168.229.8:2181,192.168.229.9:2181 –topic test –replication-factor 1 –partitions 1 –create bin/kafka-console-producer.sh –broker-list 192.168.229.7:9092,192.168.229.8:9092,192.168.229.9:9092 –topic test bin/kafka-console-consumer.sh –bootstrap-server 192.168.229.7:9092,192.168.229.8:9092,192.168.229.9:9092 –topic test –from-beginning 基于“分发层+应用层”双层nginx架构提升缓存命中率1、缓存命中率低 缓存数据生产服务那一层已经搞定了，相当于三层缓存架构中的本地堆缓存+redis分布式缓存都搞定了 就要来做三级缓存中的nginx那一层的缓存了 如果一般来说，你默认会部署多个nginx，在里面都会放一些缓存，就默认情况下，此时缓存命中率是比较低的 2、如何提升缓存命中率 分发层+应用层，双层nginx 分发层nginx，负责流量分发的逻辑和策略，这个里面它可以根据你自己定义的一些规则，比如根据productId去进行hash，然后对后端的nginx数量取模 将某一个商品的访问的请求，就固定路由到一个nginx后端服务器上去，保证说只会从redis中获取一次缓存数据，后面全都是走nginx本地缓存了 后端的nginx服务器，就称之为应用服务器; 最前端的nginx服务器，被称之为分发服务器 看似很简单，其实很有用，在实际的生产环境中，可以大幅度提升你的nginx本地缓存这一层的命中率，大幅度减少redis后端的压力，提升性能]]></content>
      <tags>
        <tag>项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模拟第一天]]></title>
    <url>%2F2019%2F10%2F02%2F%E6%A8%A1%E6%8B%9F%E7%AC%AC%E4%B8%80%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[java创建对象有几种方式 使用new关键字：这是我们最常见的也是最简单的创建对象的方式，通过这种方式我们还可以调用任意的够赞函数（无参的和有参的）。比如：Student student = new Student(); 使用Class类的newInstance方法：我们也可以使用Class类的newInstance方法创建对象，这个newInstance方法调用无参的构造器创建对象，如：Student student2 = (Student)Class.forName(“根路径.Student”).newInstance(); 或者：Student stu = Student.class.newInstance(); 使用Constructor类的newInstance方法：本方法和Class类的newInstance方法很像，java.lang.relect.Constructor类里也有一个newInstance方法可以创建对象。我们可以通过这个newInstance方法调用有参数的和私有的构造函数。如： Constructor constructor = Student.class.getInstance(); Student stu = constructor.newInstance(); 这两种newInstance的方法就是大家所说的反射，事实上Class的newInstance方法内部调用Constructor的newInstance方法。这也是众多框架Spring、Hibernate、Struts等使用后者的原因。 使用Clone的方法：无论何时我们调用一个对象的clone方法，JVM就会创建一个新的对象，将前面的对象的内容全部拷贝进去，用clone方法创建对象并不会调用任何构造函数。要使用clone方法，我们必须先实现Cloneable接口并实现其定义的clone方法。如：Student stu2 = stu.clone();这也是原型模式的应用。 使用反序列化：当我们序列化和反序列化一个对象，JVM会给我们创建一个单独的对象，在反序列化时，JVM创建对象并不会调用任何构造函数。为了反序列化一个对象，我们需要让我们的类实现Serializable接口。如：ObjectInputStream in = new ObjectInputStream (new FileInputStream(“data.obj”)); Student stu3 = (Student)in.readObject(); 浅拷贝与深拷贝的区别？怎么实现深拷贝浅拷贝：创建一个新对象，然后将当前对象的非静态字段复制到该新对象，如果字段是值类型的，那么对该字段执行复制；如果该字段是引用类型的话，则复制引用但不复制引用的对象。因此，原始对象及其副本引用同一个对象。 深拷贝：创建一个新对象，然后将当前对象的非静态字段复制到该新对象，无论该字段是值类型的还是引用类型，都复制独立的一份。当你修改其中一个对象的任何内容时，都不会影响另一个对象的内容。 使用clone方法的对象需要是实现cloneable接口 ①、让每个引用类型属性内部都重写clone() 方法②、利用序列化https://blog.csdn.net/baiye_xing/article/details/71788741 SQL注入原理，mybatis怎么实现sql注入https://czetao.github.io/2019/10/01/SQL%E6%B3%A8%E5%85%A5/ threadlocal使用需要注意什么问题？threadlocal–&gt;threadlocalmap —&gt;Entry&lt;threadlocal,value&gt; https://blog.csdn.net/LHQJ1992/article/details/52451136https://blog.csdn.net/qq_33404395/article/details/82356344 ThreadLocal的实现是这样的：每个Thread 维护一个 ThreadLocalMap 映射表，这个映射表的 key 是 ThreadLocal实例本身，value 是真正需要存储的 Object。也就是说 ThreadLocal 本身并不存储值，它只是作为一个 key 来让线程从 ThreadLocalMap 获取 value。值得注意的是图中的虚线，表示 ThreadLocalMap 是使用 ThreadLocal 的弱引用作为 Key 的，弱引用的对象在 GC 时会被回收。 ThreadLocalMap使用ThreadLocal的弱引用作为key，如果一个ThreadLocal没有外部强引用来引用它，那么系统 GC 的时候，这个ThreadLocal势必会被回收，这样一来，ThreadLocalMap中就会出现key为null的Entry，就没有办法访问这些key为null的Entry的value，如果当前线程再迟迟不结束的话，这些key为null的Entry的value就会一直存在一条强引用链：Thread Ref -&gt; Thread -&gt; ThreaLocalMap -&gt; Entry -&gt; value永远无法回收，造成内存泄漏。————————————————版权声明：本文为CSDN博主「WangCw的夏天」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/qq_33404395/article/details/82356344 为何HashMap的数组长度一定是2的次幂？hashMap的数组长度一定保持2的次幂，比如16的二进制表示为 10000，那么length-1就是15，二进制为01111，同理扩容后的数组长度为32，二进制表示为100000，length-1为31，二进制表示为011111。从下图可以我们也能看到这样会保证低位全为1，而扩容后只有一位差异，也就是多出了最左位的1，这样在通过 h&amp;(length-1)的时候，只要h对应的最左边的那一个差异位为0，就能保证得到的新的数组索引和老数组索引一致(大大减少了之前已经散列良好的老数组的数据位置重新调换)，个人理解。 还有，数组长度保持2的次幂，length-1的低位都为1，会使得获得的数组索引index更加均匀，比如： 我们看到，上面的&amp;运算，高位是不会对结果产生影响的（hash函数采用各种位运算可能也是为了使得低位更加散列），我们只关注低位bit，如果低位全部为1，那么对于h低位部分来说，任何一位的变化都会对结果产生影响，也就是说，要得到index=21这个存储位置，h的低位只有这一种组合。这也是数组长度设计为必须为2的次幂的原因。 当length-1时，低位都是111，可以保证key低位的唯一性 如果不是2的次幂，也就是低位不是全为1此时，要使得index=21，h的低位部分不再具有唯一性了，哈希冲突的几率会变的更大，同时，index对应的这个bit位无论如何不会等于1了，而对应的那些数组位置也就被白白浪费了。 https://www.cnblogs.com/chengxiao/p/6059914.html#t3]]></content>
      <tags>
        <tag>模拟练习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis详解、]]></title>
    <url>%2F2019%2F10%2F01%2Fredis%E8%AF%A6%E8%A7%A3%E3%80%81%2F</url>
    <content type="text"><![CDATA[要掌握的很好的，就是redis架构 安装一个虚拟机集群 安装redis集群 配置持久化 RDB 快照手动设置检查点（etc/redis/redis.conf 里面可以修改save） 高并发，高可用，海量数据，备份，随时可以恢复， redis架构，每秒钟几十万的访问量QPS，99.99%的高可用性，TB级的海量数据，备份和恢复，缓存架构就成功了一半。最最简单的模式，无非就是存取redis，存数据，取数据。解决各种各样的高并发下缓存面临的难题，缓存架构中不断引入各种解决方案和技术，解决高并发的问题。 搭建redis集群，从0开始，一步一步搭建一个4个结点的Centos集群。安装4台虚拟机 在hosts文件下,配置好所有的机器的ip地址到hostname的映射关系 使用ssh配置每台机器之间免密登录 make install 要把redis作为一个系统的daemon进程去运行，每次系统启动，redis进程一起启动 如果没有持久化的话，redis遇到灾难性故障的时候，就会丢失所有的数据 如果通过持久化将数据搞一份儿在磁盘上去，然后定期比如说同步和备份到一些云存储服务上去，那么就可以保证数据不丢失全部，还是可以恢复一部分数据回来的。redis如果单单把数据放在内存中，是没有任何方法应对一些灾难性的工作的，redis 在启动会自动从磁盘中恢复数据到内存中。 redis 持久化RDB,AOF利弊比较比如你redis整个挂了，然后redis就不可用了，你要做的事情是让redis变得可用，尽快变得可用 重启redis，尽快让它对外提供服务，但是就像上一讲说，如果你没做数据备份，这个时候redis启动了，也不可用啊，数据都没了 很可能说，大量的请求过来，缓存全部无法命中，在redis里根本找不到数据，这个时候就死定了，缓存雪崩问题，所有请求，没有在redis命中，就会去mysql数据库这种数据源头中去找，一下子mysql承接高并发，然后就挂了 mysql挂掉，你都没法去找数据恢复到redis里面去，redis的数据从哪儿来？从mysql来。。。 具体的完整的缓存雪崩的场景，还有企业级的解决方案，到后面讲 如果你把redis的持久化做好，备份和恢复方案做到企业级的程度，那么即使你的redis故障了，也可以通过备份数据，快速恢复，一旦恢复立即对外提供服务 redis的持久化，跟高可用，是有关系的，企业级redis架构中去讲解 redis持久化：RDB，AOF 1、RDB和AOF两种持久化机制的介绍 RDB持久化机制，对redis中的数据执行周期性的持久化 AOF机制对每条写入命令作为日志，以append-only的模式写入一个日志文件中，在redis重启的时候，可以通过回放AOF日志中的写入指令来重新构建整个数据集 如果我们想要redis仅仅作为纯内存的缓存来用，那么可以禁止RDB和AOF所有的持久化机制 通过RDB或AOF，都可以将redis内存中的数据给持久化到磁盘上面来，然后可以将这些数据备份到别的地方去，比如说阿里云，云服务 如果redis挂了，服务器上的内存和磁盘上的数据都丢了，可以从云服务上拷贝回来之前的数据，放到指定的目录中，然后重新启动redis，redis就会自动根据持久化数据文件中的数据，去恢复内存中的数据，继续对外提供服务 如果同时使用RDB和AOF两种持久化机制，那么在redis重启的时候，会使用AOF来重新构建数据，因为AOF中的数据更加完整 2、RDB持久化机制的优点 （1）RDB会生成多个数据文件，每个数据文件都代表了某一个时刻中redis的数据，这种多个数据文件的方式，非常适合做冷备，可以将这种完整的数据文件发送到一些远程的安全存储上去，比如说Amazon的S3云服务上去，在国内可以是阿里云的ODPS分布式存储上，以预定好的备份策略来定期备份redis中的数据 （2）RDB对redis对外提供的读写服务，影响非常小，可以让redis保持高性能，因为redis主进程只需要fork一个子进程，让子进程执行磁盘IO操作来进行RDB持久化即可 （3）相对于AOF持久化机制来说，直接基于RDB数据文件来重启和恢复redis进程，更加快速 3、RDB持久化机制的缺点 （1）如果想要在redis故障时，尽可能少的丢失数据，那么RDB没有AOF好。一般来说，RDB数据快照文件，都是每隔5分钟，或者更长时间生成一次，这个时候就得接受一旦redis进程宕机，那么会丢失最近5分钟的数据 （2）RDB每次在fork子进程来执行RDB快照数据文件生成的时候，如果数据文件特别大，可能会导致对客户端提供的服务暂停数毫秒，或者甚至数秒 4、AOF持久化机制的优点 （1）AOF可以更好的保护数据不丢失，一般AOF会每隔1秒，通过一个后台线程执行一次fsync操作，最多丢失1秒钟的数据 （2）AOF日志文件以append-only模式写入，所以没有任何磁盘寻址的开销，写入性能非常高，而且文件不容易破损，即使文件尾部破损，也很容易修复 （3）AOF日志文件即使过大的时候，出现后台重写操作，也不会影响客户端的读写。因为在rewrite log的时候，会对其中的指导进行压缩，创建出一份需要恢复数据的最小日志出来。再创建新日志文件的时候，老的日志文件还是照常写入。当新的merge后的日志文件ready的时候，再交换新老日志文件即可。 （4）AOF日志文件的命令通过非常可读的方式进行记录，这个特性非常适合做灾难性的误删除的紧急恢复。比如某人不小心用flushall命令清空了所有数据，只要这个时候后台rewrite还没有发生，那么就可以立即拷贝AOF文件，将最后一条flushall命令给删了，然后再将该AOF文件放回去，就可以通过恢复机制，自动恢复所有数据 5、AOF持久化机制的缺点 （1）对于同一份数据来说，AOF日志文件通常比RDB数据快照文件更大 （2）AOF开启后，支持的写QPS会比RDB支持的写QPS低，因为AOF一般会配置成每秒fsync一次日志文件，当然，每秒一次fsync，性能也还是很高的 （3）以前AOF发生过bug，就是通过AOF记录的日志，进行数据恢复的时候，没有恢复一模一样的数据出来。所以说，类似AOF这种较为复杂的基于命令日志/merge/回放的方式，比基于RDB每次持久化一份完整的数据快照文件的方式，更加脆弱一些，容易有bug。不过AOF就是为了避免rewrite过程导致的bug，因此每次rewrite并不是基于旧的指令日志进行merge的，而是基于当时内存中的数据进行指令的重新构建，这样健壮性会好很多。 6、RDB和AOF到底该如何选择 （1）不要仅仅使用RDB，因为那样会导致你丢失很多数据 （2）也不要仅仅使用AOF，因为那样有两个问题，第一，你通过AOF做冷备，没有RDB做冷备，来的恢复速度更快; 第二，RDB每次简单粗暴生成数据快照，更加健壮，可以避免AOF这种复杂的备份和恢复机制的bug （3）综合使用AOF和RDB两种持久化机制，用AOF来保证数据不丢失，作为数据恢复的第一选择; 用RDB来做不同程度的冷备，在AOF文件都丢失或损坏不可用的时候，还可以使用RDB来进行快速的数据恢复 RDB持久化详解1、如何配置RDB持久化机制2、RDB持久化机制的工作流程3、基于RDB持久化机制的数据恢复实验 1、如何配置RDB持久化机制 redis.conf文件，也就是/etc/redis/6379.conf，去配置持久化 save 60 1000 每隔60s，如果有超过1000个key发生了变更，那么就生成一个新的dump.rdb文件，就是当前redis内存中完整的数据快照，这个操作也被称之为snapshotting，快照 也可以手动调用save或者bgsave命令，同步或异步执行rdb快照生成 save可以设置多个，就是多个snapshotting检查点，每到一个检查点，就会去check一下，是否有指定的key数量发生了变更，如果有，就生成一个新的dump.rdb文件 2、RDB持久化机制的工作流程 （1）redis根据配置自己尝试去生成rdb快照文件（2）fork一个子进程出来（3）子进程尝试将数据dump到临时的rdb快照文件中（4）完成rdb快照文件的生成之后，就替换之前的旧的快照文件 dump.rdb，每次生成一个新的快照，都会覆盖之前的老快照 3、基于RDB持久化机制的数据恢复实验 （1）在redis中保存几条数据，立即停掉redis进程，然后重启redis，看看刚才插入的数据还在不在 数据还在，为什么？ 带出来一个知识点，通过redis-cli SHUTDOWN这种方式去停掉redis，其实是一种安全退出的模式，redis在退出的时候会将内存中的数据立即生成一份完整的rdb快照 /var/redis/6379/dump.rdb （2）在redis中再保存几条新的数据，用kill -9粗暴杀死redis进程，模拟redis故障异常退出，导致内存数据丢失的场景 这次就发现，redis进程异常被杀掉，数据没有进dump文件，几条最新的数据就丢失了 （2）手动设置一个save检查点，save 5 1（3）写入几条数据，等待5秒钟，会发现自动进行了一次dump rdb快照，在dump.rdb中发现了数据（4）异常停掉redis进程，再重新启动redis，看刚才插入的数据还在 rdb的手动配置检查点，以及rdb快照的生成，包括数据的丢失和恢复，全都演示过了 AOF持久化详解1、AOF持久化的配置2、AOF持久化的数据恢复实验3、AOF rewrite4、AOF破损文件的修复5、AOF和RDB同时工作 1、AOF持久化的配置 AOF持久化，默认是关闭的，默认是打开RDB持久化 appendonly yes，可以打开AOF持久化机制，在生产环境里面，一般来说AOF都是要打开的，除非你说随便丢个几分钟的数据也无所谓。如果开启AOF，就算没有AOF文件，redis在重启时，也会创建一个新的空的AOF文件恢复数据，在这个时候就需要先关闭AOF，拷贝dump.rdb恢复redis先，接着应该直接在命令行热修改redis配置，打开AOF。此时磁盘上的配置文件还是no（关闭）的，还需要在磁盘上将AOF打开。 AOF append-only ，顺序写入，如果AOF文件破损，那么用redis-check-aof fix修复文件打开AOF持久化机制之后，redis每次接收到一条写命令，就会写入日志文件中，当然是先写入os cache的，然后每隔一定时间再fsync一下 而且即使AOF和RDB都开启了，redis重启的时候，也是优先通过AOF进行数据恢复的，因为aof数据比较完整 可以配置AOF的fsync策略，有三种策略可以选择，一种是每次写入一条数据就执行一次fsync; 一种是每隔一秒执行一次fsync; 一种是不主动执行fsync always: 每次写入一条数据，立即将这个数据对应的写日志fsync到磁盘上去，性能非常非常差，吞吐量很低; 确保说redis里的数据一条都不丢，那就只能这样了 mysql -&gt; 内存策略，大量磁盘，QPS到多少，一两k。QPS，每秒钟的请求数量redis -&gt; 内存，磁盘持久化，QPS到多少，单机，一般来说，上万QPS没问题 everysec: 每秒将os cache中的数据fsync到磁盘，这个最常用的，生产环境一般都这么配置，性能很高，QPS还是可以上万的 no: 仅仅redis负责将数据写入os cache就撒手不管了，然后后面os自己会时不时有自己的策略将数据刷入磁盘，不可控了 2、AOF持久化的数据恢复实验 （1）先仅仅打开RDB，写入一些数据，然后kill -9杀掉redis进程，接着重启redis，发现数据没了，因为RDB快照还没生成（2）打开AOF的开关，启用AOF持久化（3）写入一些数据，观察AOF文件中的日志内容 其实你在appendonly.aof文件中，可以看到刚写的日志，它们其实就是先写入os cache的，然后1秒后才fsync到磁盘中，只有fsync到磁盘中了，才是安全的，要不然光是在os cache中，机器只要重启，就什么都没了 （4）kill -9杀掉redis进程，重新启动redis进程，发现数据被恢复回来了，就是从AOF文件中恢复回来的 redis进程启动的时候，直接就会从appendonly.aof中加载所有的日志，把内存中的数据恢复回来 3、AOF rewrite redis中的数据其实有限的，很多数据可能会自动过期，可能会被用户删除，可能会被redis用缓存清除的算法清理掉 redis中的数据会不断淘汰掉旧的，就一部分常用的数据会被自动保留在redis内存中 所以可能很多之前的已经被清理掉的数据，对应的写日志还停留在AOF中，AOF日志文件就一个，会不断的膨胀，到很大很大 所以AOF会自动在后台每隔一定时间做rewrite操作，比如日志里已经存放了针对100w数据的写日志了; redis内存只剩下10万; 基于内存中当前的10万数据构建一套最新的日志，到AOF中; 覆盖之前的老日志; 确保AOF日志文件不会过大，保持跟redis内存数据量一致 redis 2.4之前，还需要手动，开发一些脚本，crontab，通过BGREWRITEAOF命令去执行AOF rewrite，但是redis 2.4之后，会自动进行rewrite操作 在redis.conf中，可以配置rewrite策略 auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb 比如说上一次AOF rewrite之后，是128mb 然后就会接着128mb继续写AOF的日志，如果发现增长的比例，超过了之前的100%，256mb，就可能会去触发一次rewrite 但是此时还要去跟min-size，64mb去比较，256mb &gt; 64mb，才会去触发rewrite （1）redis fork一个子进程（2）子进程基于当前内存中的数据，构建日志，开始往一个新的临时的AOF文件中写入日志（3）redis主进程，接收到client新的写操作之后，在内存中写入日志，同时新的日志也继续写入旧的AOF文件（4）子进程写完新的日志文件之后，redis主进程将内存中的新日志再次追加到新的AOF文件中（5）用新的日志文件替换掉旧的日志文件 4、AOF破损文件的修复 如果redis在append数据到AOF文件时，机器宕机了，可能会导致AOF文件破损 用redis-check-aof –fix命令来修复破损的AOF文件 5、AOF和RDB同时工作 （1）如果RDB在执行snapshotting操作，那么redis不会执行AOF rewrite; 如果redis再执行AOF rewrite，那么就不会执行RDB snapshotting（2）如果RDB在执行snapshotting，此时用户执行BGREWRITEAOF命令，那么等RDB快照生成之后，才会去执行AOF rewrite（3）同时有RDB snapshot文件和AOF日志文件，那么redis重启的时候，会优先使用AOF进行数据恢复，因为其中的日志更完整 6、最后一个小实验，让大家对redis的数据恢复有更加深刻的体会 （1）在有rdb的dump和aof的appendonly的同时，rdb里也有部分数据，aof里也有部分数据，这个时候其实会发现，rdb的数据不会恢复到内存中（2）我们模拟让aof破损，然后fix，有一条数据会被fix删除（3）再次用fix得aof文件去重启redis，发现数据只剩下一条了 数据恢复完全是依赖于底层的磁盘的持久化的，主要rdb和aof上都没有数据，那就没了 9.29redis在企业级数据备份方案以及数据恢复负灾演练到这里为止，其实还是停留在简单学习知识的程度，学会了redis的持久化的原理和操作，但是在企业中，持久化到底是怎么去用得呢？ 企业级的数据备份和各种灾难下的数据恢复，是怎么做得呢？ 1、企业级的持久化的配置策略 在企业中，RDB的生成策略，用默认的也差不多 save 60 10000：如果你希望尽可能确保说，RDB最多丢1分钟的数据，那么尽量就是每隔1分钟都生成一个快照，低峰期，数据量很少，也没必要 10000-&gt;生成RDB，1000-&gt;RDB，这个根据你自己的应用和业务的数据量，你自己去决定 AOF一定要打开，fsync，everysec auto-aof-rewrite-percentage 100: 就是当前AOF大小膨胀到超过上次100%，上次的两倍auto-aof-rewrite-min-size 64mb: 根据你的数据量来定，16mb，32mb 2、企业级的数据备份方案 RDB非常适合做冷备，每次生成之后，就不会再有修改了 数据备份方案 （1）写crontab定时调度脚本去做数据备份（2）每小时都copy一份rdb的备份，到一个目录中去，仅仅保留最近48小时的备份（3）每天都保留一份当日的rdb的备份，到一个目录中去，仅仅保留最近1个月的备份（4）每次copy备份的时候，都把太旧的备份给删了（5）每天晚上将当前服务器上所有的数据备份，发送一份到远程的云服务上去 /usr/local/redis 每小时copy一次备份，删除48小时前的数据 crontab -e 0 sh /usr/local/redis/copy/redis_rdb_copy_hourly.sh redis_rdb_copy_hourly.sh #!/bin/sh cur_date=date +%Y%m%d%krm -rf /usr/local/redis/snapshotting/$cur_datemkdir /usr/local/redis/snapshotting/$cur_datecp /var/redis/6379/dump.rdb /usr/local/redis/snapshotting/$cur_date del_date=date -d -48hour +%Y%m%d%krm -rf /usr/local/redis/snapshotting/$del_date 每天copy一次备份 crontab -e 0 0 * sh /usr/local/redis/copy/redis_rdb_copy_daily.sh redis_rdb_copy_daily.sh #!/bin/sh cur_date=date +%Y%m%drm -rf /usr/local/redis/snapshotting/$cur_datemkdir /usr/local/redis/snapshotting/$cur_datecp /var/redis/6379/dump.rdb /usr/local/redis/snapshotting/$cur_date del_date=date -d -1month +%Y%m%drm -rf /usr/local/redis/snapshotting/$del_date 每天一次将所有数据上传一次到远程的云服务器上去 3、数据恢复方案 （1）如果是redis进程挂掉，那么重启redis进程即可，直接基于AOF日志文件恢复数据 不演示了，在AOF数据恢复那一块，演示了，fsync everysec，最多就丢一秒的数 （2）如果是redis进程所在机器挂掉，那么重启机器后，尝试重启redis进程，尝试直接基于AOF日志文件进行数据恢复 AOF没有破损，也是可以直接基于AOF恢复的 AOF append-only，顺序写入，如果AOF文件破损，那么用redis-check-aof fix （3）如果redis当前最新的AOF和RDB文件出现了丢失/损坏，那么可以尝试基于该机器上当前的某个最新的RDB数据副本进行数据恢复 当前最新的AOF和RDB文件都出现了丢失/损坏到无法恢复，一般不是机器的故障，人为 大数据系统，hadoop，有人不小心就把hadoop中存储的大量的数据文件对应的目录，rm -rf一下，我朋友的一个小公司，运维不太靠谱，权限也弄的不太好 /var/redis/6379下的文件给删除了 找到RDB最新的一份备份，小时级的备份可以了，小时级的肯定是最新的，copy到redis里面去，就可以恢复到某一个小时的数据 容灾演练 我跟大家解释一下，我其实上课，为什么大量的讲师可能讲课就是纯PPT，或者是各种复制粘贴，都不是现场讲解和写代码演示的 很容易出错，为了避免出错，一般就会那样玩儿 吐槽，念PPT，效果很差 真实的，备课，讲课不可避免，会出现一些问题，但是我觉得还好，真实 appendonly.aof + dump.rdb，优先用appendonly.aof去恢复数据，但是我们发现redis自动生成的appendonly.aof是没有数据的 然后我们自己的dump.rdb是有数据的，但是明显没用我们的数据 redis启动的时候，自动重新基于内存的数据，生成了一份最新的rdb快照，直接用空的数据，覆盖掉了我们有数据的，拷贝过去的那份dump.rdb 你停止redis之后，其实应该先删除appendonly.aof，然后将我们的dump.rdb拷贝过去，然后再重启redis 很简单，就是虽然你删除了appendonly.aof，但是因为打开了aof持久化，redis就一定会优先基于aof去恢复，即使文件不在，那就创建一个新的空的aof文件 停止redis，暂时在配置中关闭aof，然后拷贝一份rdb过来，再重启redis，数据能不能恢复过来，可以恢复过来 脑子一热，再关掉redis，手动修改配置文件，打开aof，再重启redis，数据又没了，空的aof文件，所有数据又没了 在数据安全丢失的情况下，基于rdb冷备，如何完美的恢复数据，同时还保持aof和rdb的双开 停止redis，关闭aof，拷贝rdb备份，重启redis，确认数据恢复，直接在命令行热修改redis配置，打开aof，这个redis就会将内存中的数据对应的日志，写入aof文件中 此时aof和rdb两份数据文件的数据就同步了 redis config set热修改配置参数，可能配置文件中的实际的参数没有被持久化的修改，再次停止redis，手动修改配置文件，打开aof的命令，再次重启redis （4）如果当前机器上的所有RDB文件全部损坏，那么从远程的云服务上拉取最新的RDB快照回来恢复数据 （5）如果是发现有重大的数据错误，比如某个小时上线的程序一下子将数据全部污染了，数据全错了，那么可以选择某个更早的时间点，对数据进行恢复 举个例子，12点上线了代码，发现代码有bug，导致代码生成的所有的缓存数据，写入redis，全部错了 找到一份11点的rdb的冷备，然后按照上面的步骤，去恢复到11点的数据，不就可以了吗 redis通过主从架构实现读写分离，完成10万+QPS1、redis高并发跟整个系统的高并发之间的关系 redis，你要搞高并发的话，不可避免，要把底层的缓存搞得很好 mysql，高并发，做到了，那么也是通过一系列复杂的分库分表，订单系统，事务要求的，QPS到几万，比较高了 要做一些电商的商品详情页，真正的超高并发，QPS上十万，甚至是百万，一秒钟百万的请求量 光是redis是不够的，但是redis是整个大型的缓存架构中，支撑高并发的架构里面，非常重要的一个环节 首先，你的底层的缓存中间件，缓存系统，必须能够支撑的起我们说的那种高并发，其次，再经过良好的整体的缓存架构的设计（多级缓存架构、热点缓存），支撑真正的上十万，甚至上百万的高并发 2、redis不能支撑高并发的瓶颈在哪里？ 单机 3、如果redis要支撑超过10万+的并发，那应该怎么做？ 单机的redis几乎不太可能说QPS超过10万+，除非一些特殊情况，比如你的机器性能特别好，配置特别高，物理机，维护做的特别好，而且你的整体的操作不是太复杂 单机在几万 读写分离，一般来说，对缓存，一般都是用来支撑读高并发的，写的请求是比较少的，可能写请求也就一秒钟几千，一两千 大量的请求都是读，一秒钟二十万次读 读写分离 主从架构 -&gt; 读写分离 -&gt; 支撑10万+读QPS的架构 4、接下来要讲解的一个topic redis replication redis主从架构 -&gt; 读写分离架构 -&gt; 可支持水平扩展的读高并发架构 redis replication（主从架构）基本原理课程大纲 1、图解redis replication基本原理2、redis replication的核心机制3、master持久化对于主从架构的安全保障的意义 redis replication -&gt; 主从架构 -&gt; 读写分离 -&gt; 水平扩容支撑读高并发 redis replication的最最基本的原理，铺垫 1、图解redis replication基本原理 2、redis replication的核心机制 （1）redis采用异步方式复制数据到slave节点，不过redis 2.8开始，slave node会周期性地确认自己每次复制的数据量（2）一个master node是可以配置多个slave node的（3）slave node也可以连接其他的slave node（4）slave node做复制的时候，是不会block master node的正常工作的（5）slave node在做复制的时候，也不会block对自己的查询操作，它会用旧的数据集来提供服务; 但是复制完成的时候，需要删除旧数据集，加载新数据集，这个时候就会暂停对外服务了（6）slave node主要用来进行横向扩容，做读写分离，扩容的slave node可以提高读的吞吐量 slave，高可用性，有很大的关系 3、master持久化对于主从架构的安全保障的意义 如果采用了主从架构，那么建议必须开启master node的持久化！ 不建议用slave node作为master node的数据热备，因为那样的话，如果你关掉master的持久化，可能在master宕机重启的时候数据是空的，然后可能一经过复制，salve node数据也丢了 master -&gt; RDB和AOF都关闭了 -&gt; 全部在内存中 master宕机，重启，是没有本地数据可以恢复的，然后就会直接认为自己IDE数据是空的 master就会将空的数据集同步到slave上去，所有slave的数据全部清空 100%的数据丢失 master节点，必须要使用持久化机制 第二个，master的各种备份方案，要不要做，万一说本地的所有文件丢失了; 从备份中挑选一份rdb去恢复master; 这样才能确保master启动的时候，是有数据的 即使采用了后续讲解的高可用机制，slave node可以自动接管master node，但是也可能sentinal还没有检测到master failure，master node就自动重启了，还是可能导致上面的所有slave node数据清空故障 9.30redis主从复制原理细讲1、复制的完整流程 （1）slave node启动，仅仅保存master node的信息，包括master node的host和ip，但是复制流程没开始 master host和ip是从哪儿来的，redis.conf里面的slaveof配置的 （2）slave node内部有个定时任务，每秒检查是否有新的master node要连接和复制，如果发现，就跟master node建立socket网络连接（3）slave node发送ping命令给master node（4）口令认证，如果master设置了requirepass，那么salve node必须发送masterauth的口令过去进行认证（5）master node第一次执行全量复制，将所有数据发给slave node（6）master node后续持续将写命令，异步复制给slave node 2、数据同步相关的核心机制 指的就是第一次slave连接msater的时候，执行的全量复制，那个过程里面你的一些细节的机制 （1）master和slave都会维护一个offset master会在自身不断累加offset，slave也会在自身不断累加offsetslave每秒都会上报自己的offset给master，同时master也会保存每个slave的offset 这个倒不是说特定就用在全量复制的，主要是master和slave都要知道各自的数据的offset，才能知道互相之间的数据不一致的情况 （2）backlog master node有一个backlog，默认是1MB大小master node给slave node复制数据时，也会将数据在backlog中同步写一份backlog主要是用来做全量复制中断候的增量复制的 （3）master run id info server，可以看到master run id如果根据host+ip定位master node，是不靠谱的，如果master node重启或者数据出现了变化，那么slave node应该根据不同的run id区分，run id不同就做全量复制如果需要不更改run id重启redis，可以使用redis-cli debug reload命令 （4）psync 从节点使用psync从master node进行复制，psync runid offsetmaster node会根据自身的情况返回响应信息，可能是FULLRESYNC runid offset触发全量复制，可能是CONTINUE触发增量复制 3、全量复制 （1）master执行bgsave，在本地生成一份rdb快照文件（2）master node将rdb快照文件发送给salve node，如果rdb复制时间超过60秒（repl-timeout），那么slave node就会认为复制失败，可以适当调节大这个参数（3）对于千兆网卡的机器，一般每秒传输100MB，6G文件，很可能超过60s（4）master node在生成rdb时，会将所有新的写命令缓存在内存中，在salve node保存了rdb之后，再将新的写命令复制给salve node（5）client-output-buffer-limit slave 256MB 64MB 60，如果在复制期间，内存缓冲区持续消耗超过64MB，或者一次性超过256MB，那么停止复制，复制失败（6）slave node接收到rdb之后，清空自己的旧数据，然后重新加载rdb到自己的内存中，同时基于旧的数据版本对外提供服务（7）如果slave node开启了AOF，那么会立即执行BGREWRITEAOF，重写AOF rdb生成、rdb通过网络拷贝、slave旧数据的清理、slave aof rewrite，很耗费时间 如果复制的数据量在4G~6G之间，那么很可能全量复制时间消耗到1分半到2分钟 4、增量复制 （1）如果全量复制过程中，master-slave网络连接断掉，那么salve重新连接master时，会触发增量复制（2）master直接从自己的backlog中获取部分丢失的数据，发送给slave node，默认backlog就是1MB（3）msater就是根据slave发送的psync中的offset来从backlog中获取数据的 5、heartbeat 主从节点互相都会发送heartbeat信息 master默认每隔10秒发送一次heartbeat，salve node每隔1秒发送一个heartbeat 6、异步复制 master每次接收到写命令之后，现在内部写入数据，然后异步发送给slave node 搭建redis主从节点之前几讲都是在铺垫各种redis replication的原理，和知识，主从，读写分离，画图 知道了这些东西，关键是怎么搭建呢？？？ 一主一从，往主节点去写，在从节点去读，可以读到，主从架构就搭建成功了 1、启用复制，部署slave node wget http://downloads.sourceforge.net/tcl/tcl8.6.1-src.tar.gztar -xzvf tcl8.6.1-src.tar.gzcd /usr/local/tcl8.6.1/unix/./configuremake &amp;&amp; make install 使用redis-3.2.8.tar.gz（截止2017年4月的最新稳定版）tar -zxvf redis-3.2.8.tar.gzcd redis-3.2.8make &amp;&amp; make test &amp;&amp; make install （1）redis utils目录下，有个redis_init_script脚本（2）将redis_init_script脚本拷贝到linux的/etc/init.d目录中，将redis_init_script重命名为redis_6379，6379是我们希望这个redis实例监听的端口号（3）修改redis_6379脚本的第6行的REDISPORT，设置为相同的端口号（默认就是6379）（4）创建两个目录：/etc/redis（存放redis的配置文件），/var/redis/6379（存放redis的持久化文件）（5）修改redis配置文件（默认在根目录下，redis.conf），拷贝到/etc/redis目录中，修改名称为6379.conf （6）修改redis.conf中的部分配置为生产环境 daemonize yes 让redis以daemon进程运行pidfile /var/run/redis_6379.pid 设置redis的pid文件位置port 6379 设置redis的监听端口号dir /var/redis/6379 设置持久化文件的存储位置 (设置持久化文件的位置) （7）让redis跟随系统启动自动启动 在redis_6379脚本中，最上面，加入两行注释 chkconfig: 2345 90 10description: Redis is a persistent key-value databasechkconfig redis_6379 on（随着系统启动+） 在slave node上配置（redis.conf文件中只要修改slaveof相关配置）： slaveof 192.168.1.1（主节点） 6379（redis端口号），即可也可以使用slaveof命令 则配置成自己是从节点2、强制读写分离 基于主从复制架构，实现读写分离 redis slave node只读，默认开启，slave-read-only 开启了只读的redis slave node，会拒绝所有的写操作，这样可以强制搭建成读写分离的架构 3、集群安全认证 master上启用安全认证，requirepassmaster连接口令，masterauth 4、读写分离架构的测试 先启动主节点，eshop-cache01上的redis实例再启动从节点，eshop-cache02上的redis实例 刚才我调试了一下，redis slave node一直说没法连接到主节点的6379的端口 在搭建生产环境的集群的时候，不要忘记修改一个配置，bind bind 127.0.0.1 -&gt; 本地的开发调试的模式，就只能127.0.0.1本地才能访问到6379的端口 每个redis.conf中的bind 127.0.0.1 -&gt; bind自己的ip地址在每个节点上都: iptables -A INPUT -ptcp –dport 6379 -j ACCEPT redis-cli -h ipaddrinfo replication 在主上写，在从上读ps -ef | grep redis查看redis进程是否已经启动 哨兵架构基础知识讲解1、哨兵的介绍 sentinal，中文名是哨兵 哨兵是redis集群架构中非常重要的一个组件，主要功能如下 （1）集群监控，负责监控redis master和slave进程是否正常工作（2）消息通知，如果某个redis实例有故障，那么哨兵负责发送消息作为报警通知给管理员（3）故障转移，如果master node挂掉了，会自动转移到slave node上（4）配置中心，如果故障转移发生了，通知client客户端新的master地址 哨兵本身也是分布式的，作为一个哨兵集群去运行，互相协同工作 （1）故障转移时，判断一个master node是宕机了，需要大部分的哨兵都同意才行，涉及到了分布式选举的问题（2）即使部分哨兵节点挂掉了，哨兵集群还是能正常工作的，因为如果一个作为高可用机制重要组成部分的故障转移系统本身是单点的，那就很坑爹了 目前采用的是sentinal 2版本，sentinal 2相对于sentinal 1来说，重写了很多代码，主要是让故障转移的机制和算法变得更加健壮和简单 2、哨兵的核心知识 （1）哨兵至少需要3个实例，来保证自己的健壮性（2）哨兵 + redis主从的部署架构，是不会保证数据零丢失的，只能保证redis集群的高可用性（3）对于哨兵 + redis主从这种复杂的部署架构，尽量在测试环境和生产环境，都进行充足的测试和演练 3、为什么redis哨兵集群只有2个节点无法正常工作？ 哨兵集群必须部署2个以上节点 如果哨兵集群仅仅部署了个2个哨兵实例，quorum=1 +—-+ +—-+| M1 |———| R1 || S1 | | S2 |+—-+ +—-+ Configuration: quorum = 1 master宕机，s1和s2中只要有1个哨兵认为master宕机就可以还行切换，同时s1和s2中会选举出一个哨兵来执行故障转移 同时这个时候，需要majority，也就是大多数哨兵都是运行的，2个哨兵的majority就是2（2的majority=2，3的majority=2，5的majority=3，4的majority=2），2个哨兵都运行着，就可以允许执行故障转移 但是如果整个M1和S1运行的机器宕机了，那么哨兵只有1个了，此时就没有majority来允许执行故障转移，虽然另外一台机器还有一个R1，但是故障转移不会执行 4、经典的3节点哨兵集群 12345+----+| M1 || S1 |+----+ | +—-+ | +—-+| R2 |—-+—-| R3 || S2 | | S3 |+—-+ +—-+ Configuration: quorum = 2，majority = 2 如果M1所在机器宕机了，那么三个哨兵还剩下2个，S2和S3可以一致认为master宕机，然后选举出一个来执行故障转移 同时3个哨兵的majority是2，所以还剩下的2个哨兵运行着，就可以允许执行故障转移 redis哨兵主备切换的数据丢失问题：异步复制，集群脑裂课程大纲 1、两种数据丢失的情况2、解决异步复制和脑裂导致的数据丢失 1、两种数据丢失的情况 主备切换的过程，可能会导致数据丢失 （1）异步复制导致的数据丢失 因为master -&gt; slave的复制是异步的，所以可能有部分数据还没复制到slave，master就宕机了，此时这些部分数据就丢失了 （2）脑裂导致的数据丢失 脑裂，也就是说，某个master所在机器突然脱离了正常的网络，跟其他slave机器不能连接，但是实际上master还运行着 此时哨兵可能就会认为master宕机了，然后开启选举，将其他slave切换成了master 这个时候，集群里就会有两个master，也就是所谓的脑裂 此时虽然某个slave被切换成了master，但是可能client还没来得及切换到新的master，还继续写向旧master的数据可能也丢失了 因此旧master再次恢复的时候，会被作为一个slave挂到新的master上去，自己的数据会清空，重新从新的master复制数据 2、解决异步复制和脑裂导致的数据丢失 min-slaves-to-write 1min-slaves-max-lag 10 要求至少有1个slave，数据复制和同步的延迟不能超过10秒 如果说一旦所有的slave，数据复制和同步的延迟都超过了10秒钟，那么这个时候，master就不会再接收任何请求了 上面两个配置可以减少异步复制和脑裂导致的数据丢失 （1）减少异步复制的数据丢失 有了min-slaves-max-lag这个配置，就可以确保说，一旦slave复制数据和ack延时太长，就认为可能master宕机后损失的数据太多了，那么就拒绝写请求，这样可以把master宕机时由于部分数据未同步到slave导致的数据丢失降低的可控范围内 （2）减少脑裂的数据丢失 如果一个master出现了脑裂，跟其他slave丢了连接，那么上面两个配置可以确保说，如果不能继续给指定数量的slave发送数据，而且slave超过10秒没有给自己ack消息，那么就直接拒绝客户端的写请求 这样脑裂后的旧master就不会接受client的新数据，也就避免了数据丢失 上面的配置就确保了，如果跟任何一个slave丢了连接，在10秒后发现没有slave给自己ack，那么就拒绝新的写请求 因此在脑裂场景下，最多就丢失10秒的数据 redis哨兵的多个核心底层原理的深入解析1、sdown和odown转换机制 sdown和odown两种失败状态 sdown是主观宕机，就一个哨兵如果自己觉得一个master宕机了，那么就是主观宕机 odown是客观宕机，如果quorum数量的哨兵都觉得一个master宕机了，那么就是客观宕机 sdown达成的条件很简单，如果一个哨兵ping一个master，超过了is-master-down-after-milliseconds指定的毫秒数之后，就主观认为master宕机 sdown到odown转换的条件很简单，如果一个哨兵在指定时间内，收到了quorum指定数量的其他哨兵也认为那个master是sdown了，那么就认为是odown了，客观认为master宕机 2、哨兵集群的自动发现机制 哨兵互相之间的发现，是通过redis的pub/sub系统实现的，每个哨兵都会往sentinel:hello这个channel里发送一个消息，这时候所有其他哨兵都可以消费到这个消息，并感知到其他的哨兵的存在 每隔两秒钟，每个哨兵都会往自己监控的某个master+slaves对应的sentinel:hello channel里发送一个消息，内容是自己的host、ip和runid还有对这个master的监控配置 每个哨兵也会去监听自己监控的每个master+slaves对应的sentinel:hello channel，然后去感知到同样在监听这个master+slaves的其他哨兵的存在 每个哨兵还会跟其他哨兵交换对master的监控配置，互相进行监控配置的同步 3、slave配置的自动纠正 哨兵会负责自动纠正slave的一些配置，比如slave如果要成为潜在的master候选人，哨兵会确保slave在复制现有master的数据; 如果slave连接到了一个错误的master上，比如故障转移之后，那么哨兵会确保它们连接到正确的master上 4、slave-&gt;master选举算法 如果一个master被认为odown了，而且majority哨兵都允许了主备切换，那么某个哨兵就会执行主备切换操作，此时首先要选举一个slave来 会考虑slave的一些信息 （1）跟master断开连接的时长（2）slave优先级（3）复制offset（4）run id 如果一个slave跟master断开连接已经超过了down-after-milliseconds的10倍，外加master宕机的时长，那么slave就被认为不适合选举为master (down-after-milliseconds * 10) + milliseconds_since_master_is_in_SDOWN_state 接下来会对slave进行排序 （1）按照slave优先级进行排序，slave priority越低，优先级就越高(slave priority是在redis.conf文件中自己设置的)（2）如果slave priority相同，那么看replica offset，哪个slave复制了越多的数据，offset越靠后，优先级就越高（3）如果上面两个条件都相同，那么选择一个run id比较小的那个slave 5、quorum和majority 每次一个哨兵要做主备切换，首先需要quorum数量的哨兵认为odown，然后选举出一个哨兵来做切换，这个哨兵还得得到majority哨兵的授权，才能正式执行切换 如果quorum &lt; majority，比如5个哨兵，majority就是3，quorum设置为2，那么就3个哨兵授权就可以执行切换 但是如果quorum &gt;= majority，那么必须quorum数量的哨兵都授权，比如5个哨兵，quorum是5，那么必须5个哨兵都同意授权，才能执行切换 6、configuration epoch 哨兵会对一套redis master+slave进行监控，有相应的监控的配置 执行切换的那个哨兵，会从要切换到的新master（salve-&gt;master）那里得到一个configuration epoch，这就是一个version号，每次切换的version号都必须是唯一的 如果第一个选举出的哨兵切换失败了，那么其他哨兵，会等待failover-timeout时间，然后接替继续执行切换，此时会重新获取一个新的configuration epoch，作为新的version号 7、configuraiton传播 哨兵完成切换之后，会在自己本地更新生成最新的master配置，然后同步给其他的哨兵，就是通过之前说的pub/sub消息机制 这里之前的version号就很重要了，因为各种消息都是通过一个channel去发布和监听的，所以一个哨兵完成一次新的切换之后，新的master配置是跟着新的version号的 其他的哨兵都是根据版本号的大小来更新自己的master配置的 redis哨兵集群的实战配置quorum的解释如下： （1）至少多少个哨兵要一致同意，master进程挂掉了，或者slave进程挂掉了，或者要启动一个故障转移操作（2）quorum是用来识别故障的，真正执行故障转移的时候，还是要在哨兵集群执行选举，选举一个哨兵进程出来执行故障转移操作（3）假设有5个哨兵，quorum设置了2，那么如果5个哨兵中的2个都认为master挂掉了; 2个哨兵中的一个就会做一个选举，选举一个哨兵出来，执行故障转移; 如果5个哨兵中有3个哨兵都是运行的，那么故障转移就会被允许执行 动手实操，练习如何操作部署哨兵集群，如何基于哨兵进行故障转移，还有一些企业级的配置方案 1、哨兵的配置文件(存放在redis安装目录下)sentinel.conf 最小的配置 每一个哨兵都可以去监控多个maser-slaves的主从架构 因为可能你的公司里，为不同的项目，部署了多个master-slaves的redis主从集群 相同的一套哨兵集群，就可以去监控不同的多个redis主从集群 你自己给每个redis主从集群分配一个逻辑的名称 sentinel monitor mymaster 127.0.0.1 6379 2sentinel down-after-milliseconds mymaster 60000sentinel failover-timeout mymaster 180000sentinel parallel-syncs mymaster 1 sentinel monitor resque 192.168.1.3 6380 4sentinel down-after-milliseconds resque 10000sentinel failover-timeout resque 180000sentinel parallel-syncs resque 5 sentinel monitor mymaster 127.0.0.1 6379 类似这种配置，来指定对一个master的监控，给监控的master指定的一个名称，因为后面分布式集群架构里会讲解，可以配置多个master做数据拆分 sentinel down-after-milliseconds mymaster 60000sentinel failover-timeout mymaster 180000sentinel parallel-syncs mymaster 1 上面的三个配置，都是针对某个监控的master配置的，给其指定上面分配的名称即可 上面这段配置，就监控了两个master node 这是最小的哨兵配置，如果发生了master-slave故障转移，或者新的哨兵进程加入哨兵集群，那么哨兵会自动更新自己的配置文件 sentinel monitor master-group-name hostname port quorum quorum的解释如下： （1）至少多少个哨兵要一致同意，master进程挂掉了，或者slave进程挂掉了，或者要启动一个故障转移操作（2）quorum是用来识别故障的，真正执行故障转移的时候，还是要在哨兵集群执行选举，选举一个哨兵进程出来执行故障转移操作（3）假设有5个哨兵，quorum设置了2，那么如果5个哨兵中的2个都认为master挂掉了; 2个哨兵中的一个就会做一个选举，选举一个哨兵出来，执行故障转移; 如果5个哨兵中有3个哨兵都是运行的，那么故障转移就会被允许执行 down-after-milliseconds，超过多少毫秒跟一个redis实例断了连接，哨兵就可能认为这个redis实例挂了 parallel-syncs，新的master别切换之后，同时有多少个slave被切换到去连接新master，重新做同步，数字越低，花费的时间越多 假设你的redis是1个master，4个slave 然后master宕机了，4个slave中有1个切换成了master，剩下3个slave就要挂到新的master上面去 这个时候，如果parallel-syncs是1，那么3个slave，一个一个地挂接到新的master上面去，1个挂接完，而且从新的master sync完数据之后，再挂接下一个 如果parallel-syncs是3，那么一次性就会把所有slave挂接到新的master上去 failover-timeout，执行故障转移的timeout超时时长 2、在eshop-cache03上再部署一个redis 只要安装redis就可以了，不需要去部署redis实例的启动 wget http://downloads.sourceforge.net/tcl/tcl8.6.1-src.tar.gztar -xzvf tcl8.6.1-src.tar.gzcd /usr/local/tcl8.6.1/unix/./configuremake &amp;&amp; make install 使用redis-3.2.8.tar.gz（截止2017年4月的最新稳定版）tar -zxvf redis-3.2.8.tar.gzcd redis-3.2.8make &amp;&amp; make testmake install 2、正式的配置 哨兵默认用26379端口，默认不能跟其他机器在指定端口连通，只能在本地访问 mkdir /etc/sentinalmkdir -p /var/sentinal/5000 /etc/sentinel/5000.conf port 5000bind 192.168.31.187dir /var/sentinal/5000sentinel monitor mymaster 192.168.31.187 6379 2sentinel down-after-milliseconds mymaster 30000sentinel failover-timeout mymaster 60000sentinel parallel-syncs mymaster 1 port 5000bind 192.168.31.19dir /var/sentinal/5000sentinel monitor mymaster 192.168.31.187 6379 2sentinel down-after-milliseconds mymaster 30000sentinel failover-timeout mymaster 60000sentinel parallel-syncs mymaster 1 port 5000bind 192.168.31.227dir /var/sentinal/5000sentinel monitor mymaster 192.168.31.187 6379 2sentinel down-after-milliseconds mymaster 30000sentinel failover-timeout mymaster 60000sentinel parallel-syncs mymaster 1 3、启动哨兵进程 在eshop-cache01、eshop-cache02、eshop-cache03三台机器上，分别启动三个哨兵进程，组成一个集群，观察一下日志的输出 redis-sentinel /etc/sentinal/5000.confredis-server /etc/sentinal/5000.conf –sentinel 日志里会显示出来，每个哨兵都能去监控到对应的redis master，并能够自动发现对应的slave 哨兵之间，互相会自动进行发现，用的就是之前说的pub/sub，消息发布和订阅channel消息系统和机制 4、检查哨兵状态 redis-cli -h 192.168.31.187 -p 5000 sentinel master mymasterSENTINEL slaves mymasterSENTINEL sentinels mymaster SENTINEL get-master-addr-by-name mymaster redis cluster横向扩容master1、单机redis在海量数据面前的瓶颈 2、怎么才能够突破单机瓶颈，让redis支撑海量数据？ 3、redis的集群架构 redis cluster 支撑N个redis master node，每个master node都可以挂载多个slave node 读写分离的架构，对于每个master来说，写就写到master，然后读就从mater对应的slave去读 高可用，因为每个master都有salve节点，那么如果mater挂掉，redis cluster这套机制，就会自动将某个slave切换成master redis cluster（多master + 读写分离 + 高可用） 我们只要基于redis cluster去搭建redis集群即可，不需要手工去搭建replication复制+主从架构+读写分离+哨兵集群+高可用 4、redis cluster vs. replication + sentinal 如果你的数据量很少，主要是承载高并发高性能的场景，比如你的缓存一般就几个G，单机足够了 replication，一个mater，多个slave，要几个slave跟你的要求的读吞吐量有关系，然后自己搭建一个sentinal集群，去保证redis主从架构的高可用性，就可以了 redis cluster，主要是针对海量数据+高并发+高可用的场景，海量数据，如果你的数据量很大，那么建议就用redis cluster redis阶段性总结1、讲解redis是为了什么？ topic：高并发、亿级流量、高性能、海量数据的场景，电商网站的商品详情页系统的缓存架构 商品详情页系统，大型电商网站，会有很多部分组成，但是支撑高并发、亿级流量的，主要就是其中的大型的缓存架构 在这个大型的缓存架构中，redis是最最基础的一层 高并发，缓存架构中除了redis，还有其他的组成部分，但是redis至关重要 大量的离散请求，随机请求，各种你未知的用户过来的请求，上千万用户过来访问，每个用户访问10次; 集中式的请求，1个用户过来，一天访问1亿次 支撑商品展示的最重要的，就是redis cluster，去抗住每天上亿的请求流量，支撑高并发的访问 redis cluster在整个缓存架构中，如何跟其他几个部分搭配起来组成一个大型的缓存系统，后面再讲 2、讲解的redis可以实现什么效果？ 我之前一直在redis的各个知识点的讲解之前都强调一下，我们要讲解的每个知识点，要解决的问题是什么？？？ redis：持久化、复制（主从架构）、哨兵（高可用，主备切换）、redis cluster（海量数据+横向扩容+高可用/主备切换） 持久化：高可用的一部分，在发生redis集群灾难的情况下（比如说部分master+slave全部死掉了），如何快速进行数据恢复，快速实现服务可用，才能实现整个系统的高可用 复制：主从架构，master -&gt; slave 复制，读写分离的架构，写master，读slave，横向扩容slave支撑更高的读吞吐，读高并发，10万，20万，30万，上百万，QPS，横向扩容 哨兵：高可用，主从架构，在master故障的时候，快速将slave切换成master，实现快速的灾难恢复，实现高可用性 redis cluster：多master读写，数据分布式的存储，横向扩容，水平扩容，快速支撑高达的数据量+更高的读写QPS，自动进行master -&gt; slave的主备切换，高可用 让底层的缓存系统，redis，实现能够任意水平扩容，支撑海量数据（1T+，几十T，10G * 600 redis = 6T），支撑很高的读写QPS（redis单机在几万QPS，10台，几十万QPS），高可用性（给我们每个redis实例都做好AOF+RDB的备份策略+容灾策略，slave -&gt; master主备切换） 1T+海量数据、10万+读写QPS、99.99%高可用性 3、redis的第一套企业级的架构 如果你的数据量不大，单master就可以容纳，一般来说你的缓存的总量在10G以内就可以，那么建议按照以下架构去部署redis redis持久化+备份方案+容灾方案+replication（主从+读写分离）+sentinal（哨兵集群，3个节点，高可用性） 可以支撑的数据量在10G以内，可以支撑的写QPS在几万左右，可以支撑的读QPS可以上10万以上（随你的需求，水平扩容slave节点就可以），可用性在99.99% 4、redis的第二套企业级架构 如果你的数据量很大，比如我们课程的topic，大型电商网站的商品详情页的架构（对标那些国内排名前三的大电商网站，宝，东，*宁易购），数据量是很大的 海量数据 redis cluster 多master分布式存储数据，水平扩容 支撑更多的数据量，1T+以上没问题，只要扩容master即可 读写QPS分别都达到几十万都没问题，只要扩容master即可，redis cluster，读写分离，支持不太好，readonly才能去slave上读 支撑99.99%可用性，也没问题，slave -&gt; master的主备切换，冗余slave去进一步提升可用性的方案（每个master挂一个slave，但是整个集群再加个3个slave冗余一下） 我们课程里，两套架构都讲解了，后续的业务系统的开发，主要是基于redis cluster去做 5、我们现在课程讲解的项目进展到哪里了？ 我们要做后续的业务系统的开发，redis的架构部署好，是第一件事情，也是非常重要的，也是你作为一个架构师而言，在对系统进行设计的时候，你必须要考虑到底层的redis的并发、性能、能支撑的数据量、可用性 redis：水平扩容，海量数据，上10万的读写QPS，99.99%高可用性 从架构的角度，我们的redis是可以做到的，水平扩容，只要机器足够，到1T数据量，50万读写QPS，99.99% 正式开始做大型电商网站的商品详情页系统，大规模的缓存架构设计]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL注入]]></title>
    <url>%2F2019%2F10%2F01%2FSQL%E6%B3%A8%E5%85%A5%2F</url>
    <content type="text"><![CDATA[SQL注入简介SQL注入是网站存在最多也是最简单的漏洞，主要原因是程序员在开发用户和数据库交互的系统时，没有对用户输入的字符串进行过滤，转义，限制或处理不严谨，导致用户可以通过输入精心构造的字符串去非法获取到数据库中的数据。 SQL注入原理一般用户登录用的SQL语句为：SELECT FROM user WHERE username=’admin’ AND password=’passwd’，此处admin和passwd分别为用户输入的用户名和密码，如果程序员没有对用户输入的用户名和密码做处理，就可以构造万能密码成功绕过登录验证，如用户输入‘or 1#,SQL语句将变为：SELECT FROM user WHERE username=’’or 1#’ AND password=’’，‘’or 1为TRUE，#注释掉后面的内容，所以查询语句可以正确执行。 mybatis是如何防止SQL注入的1、首先看一下下面两个sql语句的区别：123456&lt;select id="selectByNameAndPassword" parameterType="java.util.Map" resultMap="BaseResultMap"&gt;select id, username, password, rolefrom userwhere username = #&#123;username,jdbcType=VARCHAR&#125;and password = #&#123;password,jdbcType=VARCHAR&#125;&lt;/select&gt; 123456&lt;select id="selectByNameAndPassword" parameterType="java.util.Map" resultMap="BaseResultMap"&gt;select id, username, password, rolefrom userwhere username = $&#123;username,jdbcType=VARCHAR&#125;and password = $&#123;password,jdbcType=VARCHAR&#125;&lt;/select&gt; mybatis中的#和$的区别： 1、#将传入的数据都当成一个字符串，会对自动传入的数据加一个双引号。如：where username=#{username}，如果传入的值是111,那么解析成sql时的值为where username=”111”, 如果传入的值是id，则解析成的sql为where username=”id”. 2、$将传入的数据直接显示生成在sql中。如：where username=${username}，如果传入的值是111,那么解析成sql时的值为where username=111；如果传入的值是;drop table user;，则解析成的sql为：select id, username, password, role from user where username=;drop table user;3、#方式能够很大程度防止sql注入，$方式无法防止Sql注入。4、$方式一般用于传入数据库对象，例如传入表名.5、一般能用#的就别用$，若不得不使用“${xxx}”这样的参数，要手工地做好过滤工作，来防止sql注入攻击。6、在MyBatis中，“${xxx}”这样格式的参数会直接参与SQL编译，从而不能避免注入攻击。但涉及到动态表名和列名时，只能使用“${xxx}”这样的参数格式。所以，这样的参数需要我们在代码中手工进行处理来防止注入。【结论】在编写MyBatis的映射语句时，尽量采用“#{xxx}”这样的格式。若不得不使用“${xxx}”这样的参数，要手工地做好过滤工作，来防止SQL注入攻击。 2、什么是sql注入 sql注入解释：是一种代码注入技术，用于攻击数据驱动的应用，恶意的SQL语句被插入到执行的实体字段中（例如，为了转储数据库内容给攻击者） SQL**注入，大家都不陌生，是一种常见的攻击方式。攻击者在界面的表单信息或URL上输入一些奇怪的SQL片段（例如“or ‘1’=’1’”这样的语句），有可能入侵参数检验不足的应用程序。所以，在我们的应用中需要做一些工作，来防备这样的攻击方式。在一些安全性要求很高的应用中（比如银行软件），经常使用将SQL**语句全部替换为存储过程这样的方式，来防止SQL注入。这当然是一种很安全的方式，但我们平时开发中，可能不需要这种死板的方式。 3、mybatis是如何做到防止sql注入的 MyBatis框架作为一款半自动化的持久层框架，其SQL语句都要我们自己手动编写，这个时候当然需要防止SQL注入。其实，MyBatis的SQL是一个具有“输入+输出”的功能，类似于函数的结构，参考上面的两个例子。其中，parameterType表示了输入的参数类型，resultType表示了输出的参数类型。回应上文，如果我们想防止SQL注入，理所当然地要在输入参数上下功夫。上面代码中使用 # 的即输入参数在SQL中拼接的部分，传入参数后，打印出执行的SQL语句，会看到SQL是这样的： 1select id, username, password, role from user where username=? and password=? 不管输入什么参数，打印出的SQL都是这样的。这是因为MyBatis启用了预编译功能，在SQL执行前，会先将上面的SQL发送给数据库进行编译；执行时，直接使用编译好的SQL，替换占位符“?”就可以了。因为SQL注入只能对编译过程起作用，所以这样的方式就很好地避免了SQL注入的问题。 【底层实现原理】MyBatis是如何做到SQL预编译的呢？其实在框架底层，是JDBC中的PreparedStatement类在起作用，PreparedStatement是我们很熟悉的Statement的子类，它的对象包含了编译好的SQL语句。这种“准备好”的方式不仅能提高安全性，而且在多次执行同一个SQL时，能够提高效率。原因是SQL已编译好，再次执行时无需再编译。 1234567//安全的，预编译了的Connection conn = getConn();//获得连接String sql = &quot;select id, username, password, role from user where id=?&quot;; //执行sql前会预编译号该条语句PreparedStatement pstmt = conn.prepareStatement(sql); pstmt.setString(1, id); ResultSet rs=pstmt.executeUpdate(); ...... 12345678910//不安全的，没进行预编译private String getNameByUserId(String userId) &#123; Connection conn = getConn();//获得连接 String sql = &quot;select id,username,password,role from user where id=&quot; + id; //当id参数为&quot;3;drop table user;&quot;时，执行的sql语句如下: //select id,username,password,role from user where id=3; drop table user; PreparedStatement pstmt = conn.prepareStatement(sql); ResultSet rs=pstmt.executeUpdate(); ......&#125; 【 结论：】 #{}：相当于JDBC中的PreparedStatement ${}：是输出变量的值 简单说，#{}是经过预编译的，是安全的；${}是未经过预编译的，仅仅是取变量的值，是非安全的，存在SQL注入。如果我们order by语句后用了${}，那么不做任何处理的时候是存在SQL注入危险的。你说怎么防止，那我只能悲惨的告诉你，你得手动处理过滤一下输入的内容。如判断一下输入的参数的长度是否正常（注入语句一般很长），更精确的过滤则可以查询一下输入的参数是否在预期的参数集合中。 作者：淼淼之森 ###]]></content>
      <tags>
        <tag>查漏补缺</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA锁]]></title>
    <url>%2F2019%2F10%2F01%2FJAVA%E9%94%81%2F</url>
    <content type="text"><![CDATA[JAVA锁Java SE1.6为了减少获得锁和释放锁所带来的性能消耗，引入了“偏向锁”和“轻量级锁”，所以在Java SE1.6里锁一共有四种状态，无锁状态，偏向锁状态，轻量级锁状态和重量级锁状态，它会随着竞争情况逐渐升级。锁可以升级但不能降级，意味着偏向锁升级成轻量级锁后不能降级成偏向锁。这种锁升级却不能降级的策略，目的是为了提高获得锁和释放锁的效率。 悲观锁与乐观锁悲观锁总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁，读锁，写锁等，都是在做操作之前先上锁。JAVA中synchronize和ReentrantLock等独占锁就是悲观锁思想的实现。 乐观锁总是假设最好的情况，每次拿数据都认为别人不会修改，所以不会上锁。但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。 乐观锁适用于多读的应用类型，这样可以提高吞吐量。 java中的4种锁，分别是重量级锁，自旋锁，轻量级锁和偏向锁。重量级锁是悲观锁的一种，自旋锁，轻量级锁和偏向锁属于乐观锁。 乐观锁常见的两种实现方式 乐观锁一般会使用版本号机制或CAS算法实现。 1. 版本号机制一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一。当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值为当前数据库中的version值相等时才更新，否则重试更新操作，直到更新成功。 举一个简单的例子： 假设数据库中帐户信息表中有一个 version 字段，当前值为 1 ；而当前帐户余额字段（ balance ）为 $100 。 操作员 A 此时将其读出（ version=1 ），并从其帐户余额中扣除 $50（ $100-$50 ）。 在操作员 A 操作的过程中，操作员B 也读入此用户信息（ version=1 ），并从其帐户余额中扣除 $20 （ $100-$20 ）。 操作员 A 完成了修改工作，将数据版本号加一（ version=2 ），连同帐户扣除后余额（ balance=$50 ），提交至数据库更新，此时由于提交数据版本大于数据库记录当前版本，数据被更新，数据库记录 version 更新为 2 。 操作员 B 完成了操作，也将版本号加一（ version=2 ）试图向数据库提交数据（ balance=$80 ），但此时比对数据库记录版本时发现，操作员 B 提交的数据版本号为 2 ，数据库记录当前版本也为 2 ，不满足 “ 提交版本必须大于记录当前版本才能执行更新 “ 的乐观锁策略，因此，操作员 B 的提交被驳回。 这样，就避免了操作员 B 用基于 version=1 的旧数据修改的结果覆盖操作员A 的操作结果的可能。 2.简单回顾一下CAS算法CAS算法 即compare and swap（比较与交换），是一种有名的无锁算法。无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）。CAS算法涉及到三个操作数 需要读写的内存值 V 进行比较的值 A 拟写入的新值 B 当且仅当 V 的值等于 A时，CAS通过原子方式用新值B来更新V的值，否则不会执行任何操作（比较和替换是一个原子操作）。一般情况下是一个自旋操作，即不断的重试。 乐观锁的缺点 ABA 问题是乐观锁一个常见的问题 1 ABA 问题如果一个变量V初次读取的时候是A值，并且在准备赋值的时候检查到它仍然是A值，那我们就能说明它的值没有被其他线程修改过了吗？很明显是不能的，因为在这段时间它的值可能被改为其他值，然后又改回A，那CAS操作就会误认为它从来没有被修改过。这个问题被称为CAS操作的 “ABA”问题。 JDK 1.5 以后的 AtomicStampedReference 类就提供了此种能力，其中的 compareAndSet 方法就是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 2 循环时间长开销大自旋CAS（也就是不成功就一直循环执行直到成功）如果长时间不成功，会给CPU带来非常大的执行开销。 如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，pause指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline flush），从而提高CPU的执行效率。 3 只能保证一个共享变量的原子操作CAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效。但是从 JDK 1.5开始，提供了AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作.所以我们可以使用锁或者利用AtomicReference类把多个共享变量合并成一个共享变量来操作。 什么是自旋锁？自旋锁（spinlock）：是指当一个线程在获取锁的时候，如果锁已经被其它线程获取，那么该线程将循环等待，然后不断的判断锁是否能够被成功获取，直到获取到锁才会退出循环。 获取锁的线程一直处于活跃状态，但是并没有执行任何有效的任务，使用这种锁会造成busy-waiting。 它是为实现保护共享资源而提出一种锁机制。其实，自旋锁与互斥锁比较类似，它们都是为了解决对某项资源的互斥使用。无论是互斥锁，还是自旋锁，在任何时刻，最多只能有一个保持者，也就说，在任何时刻最多只能有一个执行单元获得锁。但是两者在调度机制上略有不同。对于互斥锁，如果资源已经被占用，资源申请者只能进入睡眠状态。但是自旋锁不会引起调用者睡眠，如果自旋锁已经被别的执行单元保持，调用者就一直循环在那里看是否该自旋锁的保持者已经释放了锁，”自旋”一词就是因此而得名。 Java如何实现自旋锁？下面是个简单的例子： 1234567891011121314public class SpinLock &#123; private AtomicReference&lt;Thread&gt; cas = new AtomicReference&lt;Thread&gt;(); public void lock() &#123; Thread current = Thread.currentThread(); // 利用CAS while (!cas.compareAndSet(null, current)) &#123; // DO nothing &#125; &#125; public void unlock() &#123; Thread current = Thread.currentThread(); cas.compareAndSet(current, null); &#125;&#125;1234567891011121314 lock（)方法利用的CAS，当第一个线程A获取锁的时候，能够成功获取到，不会进入while循环，如果此时线程A没有释放锁，另一个线程B又来获取锁，此时由于不满足CAS，所以就会进入while循环，不断判断是否满足CAS，直到A线程调用unlock方法释放了该锁。 自旋锁存在的问题 如果某个线程持有锁的时间过长，就会导致其它等待获取锁的线程进入循环等待，消耗CPU。使用不当会造成CPU使用率极高。 上面Java实现的自旋锁不是公平的，即无法满足等待时间最长的线程优先获取锁。不公平的锁就会存在“线程饥饿”问题。 自旋锁的优点 自旋锁不会使线程状态发生切换，一直处于用户态，即线程一直都是active的；不会使线程进入阻塞状态，减少了不必要的上下文切换，执行速度快 非自旋锁在获取不到锁的时候会进入阻塞状态，从而进入内核态，当获取到锁的时候需要从内核态恢复，需要线程上下文切换。 （线程被阻塞后便进入内核（Linux）调度状态，这个会导致系统在用户态与内核态之间来回切换，严重影响锁的性能） TicketLock主要解决的是公平性的问题。思路：每当有线程获取锁的时候，就给该线程分配一个递增的id，我们称之为排队号，同时，锁对应一个服务号，每当有线程释放锁，服务号就会递增，此时如果服务号与某个线程排队号一致，那么该线程就获得锁，由于排队号是递增的，所以就保证了最先请求获取锁的线程可以最先获取到锁，就实现了公平性。 可以想象成银行办理业务排队，排队的每一个顾客都代表一个需要请求锁的线程，而银行服务窗口表示锁，每当有窗口服务完成就把自己的服务号加一，此时在排队的所有顾客中，只有自己的排队号与服务号一致的才可以得到服务。 偏向锁Java偏向锁(Biased Locking)是Java6引入的一项多线程优化。偏向锁，顾名思义，它会偏向于第一个访问锁的线程，如果在运行过程中，同步锁只有一个线程访问，不存在多线程争用的情况，则线程是不需要触发同步的，这种情况下，就会给线程加一个偏向锁。如果在运行过程中，遇到了其他线程抢占锁，则持有偏向锁的线程会被挂起，JVM会消除它身上的偏向锁，将锁恢复到标准的轻量级锁。 它通过消除资源无竞争情况下的同步原语，进一步提高了程序的运行性能。 偏向锁的实现偏向锁获取过程： 访问Mark Word中偏向锁的标识是否设置成1，锁标志位是否为01，确认为可偏向状态。 如果为可偏向状态，则测试线程ID是否指向当前线程，如果是，进入步骤5，否则进入步骤3。 如果线程ID并未指向当前线程，则通过CAS操作竞争锁。如果竞争成功，则将Mark Word中线程ID设置为当前线程ID，然后执行5；如果竞争失败，执行4。 如果CAS获取偏向锁失败，则表示有竞争。当到达全局安全点（safepoint）时获得偏向锁的线程被挂起，偏向锁升级为轻量级锁，然后被阻塞在安全点的线程继续往下执行同步代码。（撤销偏向锁的时候会导致stop the word） 执行同步代码。 注意：第四步中到达安全点safepoint会导致stop the word，时间很短。 偏向锁的释放：偏向锁的撤销在上述第四步骤中有提到。偏向锁只有遇到其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁，线程不会主动去释放偏向锁。偏向锁的撤销，需要等待全局安全点（在这个时间点上没有字节码正在执行），它会首先暂停拥有偏向锁的线程，判断锁对象是否处于被锁定状态，撤销偏向锁后恢复到未锁定（标志位为“01”）或轻量级锁（标志位为“00”）的状态。 偏向锁的适用场景始终只有一个线程在执行同步块，在它没有执行完释放锁之前，没有其它线程去执行同步块，在锁无竞争的情况下使用，一旦有了竞争就升级为轻量级锁，升级为轻量级锁的时候需要撤销偏向锁，撤销偏向锁的时候会导致stop the word操作；在有锁的竞争时，偏向锁会多做很多额外操作，尤其是撤销偏向所的时候会导致进入安全点，安全点会导致stw，导致性能下降，这种情况下应当禁用； 总结自旋锁线程的阻塞和唤醒需要CPU从用户态转为核心态，频繁的阻塞和唤醒对CPU来说是一件负担很重的工作。同时我们可以发现，很多对象锁的锁定状态只会持续很短的一段时间，例如整数的自加操作，在很短的时间内阻塞并唤醒线程显然不值得，为此引入了自旋锁。 所谓“自旋”，就是让线程去执行一个无意义的循环，循环结束后再去重新竞争锁，如果竞争不到继续循环，循环过程中线程会一直处于running状态，但是基于JVM的线程调度，会出让时间片，所以其他线程依旧有申请锁和释放锁的机会。 自旋锁省去了阻塞锁的时间空间（队列的维护等）开销，但是长时间自旋就变成了“忙式等待”，忙式等待显然还不如阻塞锁。所以自旋的次数一般控制在一个范围内，例如10,100等，在超出这个范围后，自旋锁会升级为阻塞锁。（即从轻量级锁转变为重量级锁） 轻量级锁加锁 线程在执行同步块之前，JVM会先在当前线程的栈桢中创建用于存储锁记录的空间，并将对象头中的Mark Word复制到锁记录中，官方称为Displaced Mark Word。然后线程尝试使用CAS将对象头中的Mark Word替换为指向锁记录的指针。如果成功，当前线程获得锁，如果失败，则自旋获取锁，当自旋获取锁仍然失败时，表示存在其他线程竞争锁(两条或两条以上的线程竞争同一个锁)，则轻量级锁会膨胀成重量级锁。 解锁 轻量级解锁时，会使用原子的CAS操作来将Displaced Mark Word替换回到对象头，如果成功，则表示同步过程已完成。如果失败，表示有其他线程尝试过获取该锁，则要在释放锁的同时唤醒被挂起的线程。 重量级锁重量锁在JVM中又叫对象监视器（Monitor），它很像C中的Mutex，除了具备Mutex(0|1)互斥的功能，它还负责实现了Semaphore(信号量)的功能，也就是说它至少包含一个竞争锁的队列，和一个信号阻塞队列（wait队列），前者负责做互斥，后一个用于做线程同步。 如果说轻量级锁是在无竞争的情况下使用CAS操作去消除同步使用的互斥量，那偏向锁就是在无竞争的情况下把整个同步都消除掉，连CAS都不做了。]]></content>
      <tags>
        <tag>查漏补缺</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决缓存数据库双写不一致]]></title>
    <url>%2F2019%2F10%2F01%2F%E8%A7%A3%E5%86%B3%E7%BC%93%E5%AD%98%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8C%E5%86%99%E4%B8%8D%E4%B8%80%E8%87%B4%2F</url>
    <content type="text"><![CDATA[在讲双写不一致的时候，先将为什么会发生双写不一致。数据发生了变更，先删除了缓存，然后要去修改数据库，此时还没修改 一个请求过来，去读缓存，发现缓存空了，去查询数据库，查到了修改前的旧数据，放到了缓存中 数据变更的程序完成了数据库的修改 完了，数据库和缓存中的数据不一样了。。。。 hash路由的算法，跟HashMap中的hash算法是一样的。 直接调用hashcode函数，求得的hash值过大，不适合拿来直接做下标，所以要通过将hash做一次扰动再跟已有的队列-1再取模。 通过线程池 +内存队列+通过同个商品的ID路由到同一个队列中，将写请求，和读请求做到一个串行化的效果。只有当写请求完成之后，工作线程才会进行读请求的进行。 更新请求：删除缓存，修改数据库 读请求：读缓存，发现空，进入队列中等待，排到了则读数据库，并将数据写入缓存，返回数据 问题1：当写请求不断积压，读请求等待的时间过长，超过最大等待时间，会直接读数据库，造成双写不一致。 所以在这时候需要添加机器，分散队列的写请求服务。 少量情况下，可能遇到读跟数据更新冲突的情况，如上所述，那么此时更新操作如果先入队列，之后可能会瞬间来了对这个数据大量的读请求，但是因为做了去重的优化，所以也就一个更新缓存的操作跟在它后面 等数据更新完了，读请求触发的缓存更新操作也完成，然后临时等待的读请求全部可以读到缓存中的数据 问题2：大量的读请求在服务上等待。 当有大量的读请求时，此时若缓存为空，那么会通过去重，只让一个读请求进入队列中等待操作。 但可能发生，队列中的读操作还没完成，大量的读请求在服务中等待，造成服务宕机。 12345678进入队列后，读请求会有一个等待的时间，等待同步更新操作完成，这样的话，一个数据变更的操作，先执行，删除缓存，然后再去更新数据库，但是还没完成更新此时如果一个读请求过来，读到了空的缓存，那么可以先将缓存更新的请求发送到队列中，此时会在队列中积压，然后同步等待缓存更新完成这里有一个优化点，一个队列中，其实多个更新缓存请求串在一起是没意义的，因此可以做过滤，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新请求操作进去了，直接等待前面的更新操作请求完成即可待那个队列对应的工作线程完成了上一个操作的数据库的修改之后，才会去执行下一个操作，也就是缓存更新的操作，此时会从数据库中读取最新的值，然后写入缓存中如果请求还在等待时间范围内，不断轮询发现可以取到值了，那么就直接返回; 如果请求等待的时间超过一定时长，那么这一次直接从数据库中读取当前的旧值 高并发下的缓存与+数据库双写不一致问题分析与设计马上开始去开发业务系统 从哪一步开始做，从比较简单的那一块开始做，实时性要求比较高的那块数据的缓存去做 实时性比较高的数据缓存，选择的就是库存的服务 库存可能会修改，每次修改都要去更新这个缓存数据; 每次库存的数据，在缓存中一旦过期，或者是被清理掉了，前端的nginx服务都会发送请求给库存服务，去获取相应的数据 库存这一块，写数据库的时候，直接更新redis缓存 实际上没有这么的简单，这里，其实就涉及到了一个问题，数据库与缓存双写，数据不一致的问题 围绕和结合实时性较高的库存服务，把数据库与缓存双写不一致问题以及其解决方案，给大家讲解一下 数据库与缓存双写不一致，很常见的问题，大型的缓存架构中，第一个解决方案 大型的缓存架构全部讲解完了以后，整套架构是非常复杂，架构可以应对各种各样奇葩和极端的情况 也有一种可能，不是说，来讲课的就是超人，万能的 讲课，就跟写书一样，很可能会写错，也可能有些方案里的一些地方，我没考虑到 也可能说，有些方案只是适合某些场景，在某些场景下，可能需要你进行方案的优化和调整才能适用于你自己的项目 大家觉得对这些方案有什么疑问或者见解，都可以找我，沟通一下 如果的确我觉得是我讲解的不对，或者有些地方考虑不周，那么我可以在视频里补录，更新到网站上面去 多多包涵 1、最初级的缓存不一致问题以及解决方案 问题：先修改数据库，再删除缓存，如果删除缓存失败了，那么会导致数据库中是新数据，缓存中是旧数据，数据出现不一致 解决思路 先删除缓存，再修改数据库，如果删除缓存成功了，如果修改数据库失败了，那么数据库中是旧数据，缓存中是空的，那么数据不会不一致 因为读的时候缓存没有，则读数据库中旧数据，然后更新到缓存中 2、比较复杂的数据不一致问题分析 数据发生了变更，先删除了缓存，然后要去修改数据库，此时还没修改 一个请求过来，去读缓存，发现缓存空了，去查询数据库，查到了修改前的旧数据，放到了缓存中 数据变更的程序完成了数据库的修改 完了，数据库和缓存中的数据不一样了。。。。 3、为什么上亿流量高并发场景下，缓存会出现这个问题？ 只有在对一个数据在并发的进行读写的时候，才可能会出现这种问题 其实如果说你的并发量很低的话，特别是读并发很低，每天访问量就1万次，那么很少的情况下，会出现刚才描述的那种不一致的场景 但是问题是，如果每天的是上亿的流量，每秒并发读是几万，每秒只要有数据更新的请求，就可能会出现上述的数据库+缓存不一致的情况 高并发了以后，问题是很多的 4、数据库与缓存更新与读取操作进行异步串行化 更新数据的时候，根据数据的唯一标识，将操作路由之后，发送到一个jvm内部的队列中 读取数据的时候，如果发现数据不在缓存中，那么将重新读取数据+更新缓存的操作，根据唯一标识路由之后，也发送同一个jvm内部的队列中 一个队列对应一个工作线程 每个工作线程串行拿到对应的操作，然后一条一条的执行 这样的话，一个数据变更的操作，先执行，删除缓存，然后再去更新数据库，但是还没完成更新 此时如果一个读请求过来，读到了空的缓存，那么可以先将缓存更新的请求发送到队列中，此时会在队列中积压，然后同步等待缓存更新完成 这里有一个优化点，一个队列中，其实多个更新缓存请求串在一起是没意义的，因此可以做过滤，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新请求操作进去了，直接等待前面的更新操作请求完成即可 待那个队列对应的工作线程完成了上一个操作的数据库的修改之后，才会去执行下一个操作，也就是缓存更新的操作，此时会从数据库中读取最新的值，然后写入缓存中 如果请求还在等待时间范围内，不断轮询发现可以取到值了，那么就直接返回; 如果请求等待的时间超过一定时长，那么这一次直接从数据库中读取当前的旧值 5、高并发的场景下，该解决方案要注意的问题 （1）读请求长时阻塞 由于读请求进行了非常轻度的异步化，所以一定要注意读超时的问题，每个读请求必须在超时时间范围内返回 该解决方案，最大的风险点在于说，可能数据更新很频繁，导致队列中积压了大量更新操作在里面，然后读请求会发生大量的超时，最后导致大量的请求直接走数据库 务必通过一些模拟真实的测试，看看更新数据的频繁是怎样的 另外一点，因为一个队列中，可能会积压针对多个数据项的更新操作，因此需要根据自己的业务情况进行测试，可能需要部署多个服务，每个服务分摊一些数据的更新操作 如果一个内存队列里居然会挤压100个商品的库存修改操作，每隔库存修改操作要耗费10ms区完成，那么最后一个商品的读请求，可能等待10 * 100 = 1000ms = 1s后，才能得到数据 这个时候就导致读请求的长时阻塞 一定要做根据实际业务系统的运行情况，去进行一些压力测试，和模拟线上环境，去看看最繁忙的时候，内存队列可能会挤压多少更新操作，可能会导致最后一个更新操作对应的读请求，会hang多少时间，如果读请求在200ms返回，如果你计算过后，哪怕是最繁忙的时候，积压10个更新操作，最多等待200ms，那还可以的 如果一个内存队列可能积压的更新操作特别多，那么你就要加机器，让每个机器上部署的服务实例处理更少的数据，那么每个内存队列中积压的更新操作就会越少 其实根据之前的项目经验，一般来说数据的写频率是很低的，因此实际上正常来说，在队列中积压的更新操作应该是很少的 针对读高并发，读缓存架构的项目，一般写请求相对读来说，是非常非常少的，每秒的QPS能到几百就不错了 一秒，500的写操作，5份，每200ms，就100个写操作 单机器，20个内存队列，每个内存队列，可能就积压5个写操作，每个写操作性能测试后，一般在20ms左右就完成 那么针对每个内存队列中的数据的读请求，也就最多hang一会儿，200ms以内肯定能返回了 写QPS扩大10倍，但是经过刚才的测算，就知道，单机支撑写QPS几百没问题，那么就扩容机器，扩容10倍的机器，10台机器，每个机器20个队列，200个队列 大部分的情况下，应该是这样的，大量的读请求过来，都是直接走缓存取到数据的 少量情况下，可能遇到读跟数据更新冲突的情况，如上所述，那么此时更新操作如果先入队列，之后可能会瞬间来了对这个数据大量的读请求，但是因为做了去重的优化，所以也就一个更新缓存的操作跟在它后面 等数据更新完了，读请求触发的缓存更新操作也完成，然后临时等待的读请求全部可以读到缓存中的数据 （2）读请求并发量过高 这里还必须做好压力测试，确保恰巧碰上上述情况的时候，还有一个风险，就是突然间大量读请求会在几十毫秒的延时hang在服务上，看服务能不能抗的住，需要多少机器才能抗住最大的极限情况的峰值 但是因为并不是所有的数据都在同一时间更新，缓存也不会同一时间失效，所以每次可能也就是少数数据的缓存失效了，然后那些数据对应的读请求过来，并发量应该也不会特别大 按1:99的比例计算读和写的请求，每秒5万的读QPS，可能只有500次更新操作 如果一秒有500的写QPS，那么要测算好，可能写操作影响的数据有500条，这500条数据在缓存中失效后，可能导致多少读请求，发送读请求到库存服务来，要求更新缓存 一般来说，1:1，1:2，1:3，每秒钟有1000个读请求，会hang在库存服务上，每个读请求最多hang多少时间，200ms就会返回 在同一时间最多hang住的可能也就是单机200个读请求，同时hang住 单机hang200个读请求，还是ok的 1:20，每秒更新500条数据，这500秒数据对应的读请求，会有20 * 500 = 1万 1万个读请求全部hang在库存服务上，就死定了 （3）多服务实例部署的请求路由 可能这个服务部署了多个实例，那么必须保证说，执行数据更新操作，以及执行缓存更新操作的请求，都通过nginx服务器路由到相同的服务实例上 （4）热点商品的路由问题，导致请求的倾斜 万一某个商品的读写请求特别高，全部打到相同的机器的相同的队列里面去了，可能造成某台机器的压力过大 就是说，因为只有在商品数据更新的时候才会清空缓存，然后才会导致读写并发，所以更新频率不是太高的话，这个问题的影响并不是特别大 但是的确可能某些机器的负载会高一些 在库存服务中实现缓存与数据库双写一致性保障方案更新数据的时候，根据数据的唯一标识，将操作路由之后，发送到一个jvm内部的队列中 读取数据的时候，如果发现数据 不在缓存中，那么将重新读取数据+更新缓存的操作，根据唯一标识路由之后，也发送同一个jvm内部的队列中 一个队列对应一个工作线程 每个工作线程串行拿到对应的操作，然后一条一条的执行 这样的话，一个数据变更的操作，先执行，删除缓存，然后再去更新数据库，但是还没完成更新 此时如果一个读请求过来，读到了空的缓存，那么可以先将缓存更新的请求发送到队列中，此时会在队列中积压，然后同步等待缓存更新完成 这里有一个优化点，一个队列中，其实多个更新缓存请求串在一起是没意义的，因此可以做过滤，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新请求操作进去了，直接等待前面的更新操作请求完成即可 待那个队列对应的工作线程完成了上一个操作的数据库的修改之后，才会去执行下一个操作，也就是缓存更新的操作，此时会从数据库中读取最新的值，然后写入缓存中 如果请求还在等待时间范围内，不断轮询发现可以取到值了，那么就直接返回; 如果请求等待的时间超过一定时长，那么这一次直接从数据库中读取当前的旧值 int h;return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); (queueNum - 1) &amp; hash 1、线程池+内存队列 化 @Beanpublic ServletListenerRegistrationBean servletListenerRegistrationBean(){ ServletListenerRegistrationBean servletListenerRegistrationBean = new ServletListenerRegistrationBean(); servletListenerRegistrationBean.setListener(new InitListener()); return servletListenerRegistrationBean;} java web应用，做系统的初始化，一般在哪里做呢？ ServletContextListener里面做，listener，会跟着整个web应用的启动，就初始化，类似于线程池初始化的构建 使用单例初始化线程池，基于静态内部类（静态代码块）初始化线程池的方式， spring boot应用，Application，搞一个listener的注册 2、两种请求对象封装 3、请求异步执行Service封装 4、两种请求Controller接口封装 5、读请求去重优化 6、空数据读请求过滤优化 队列 对一个商品的库存的数据库更新操作已经在内存队列中了 然后对这个商品的库存的读取操作，要求读取数据库的库存数据，然后更新到缓存中，多个读 这多个读，其实只要有一个读请求操作压到队列里就可以了 其他的读操作，全部都wait那个读请求的操作，刷新缓存，就可以读到缓存中的最新数据了 如果读请求发现redis缓存中没有数据，就会发送读请求给库存服务，但是此时缓存中为空，可能是因为写请求先删除了缓存，也可能是数据库里压根儿没这条数据 如果是数据库中压根儿没这条数据的场景，那么就不应该将读请求操作给压入队列中，而是直接返回空就可以了 都是为了减少内存队列中的请求积压，内存队列中积压的请求越多，就可能导致每个读请求hang住的时间越长，也可能导致多个读请求被hang住]]></content>
      <tags>
        <tag>项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[扩展]]></title>
    <url>%2F2019%2F09%2F29%2F%E6%89%A9%E5%B1%95%2F</url>
    <content type="text"><![CDATA[9.28在订单详情部分实现多缓存处理 redis 分布式缓存 + tomcat 堆缓存 二级缓存架构 redis搭建方案12345678redis持久化+备份方案+容灾方案+replication（主从+读写分离）+sentinal（哨兵集群，3个节点，高可用性）可以支撑的数据量在10G以内，可以支撑的写QPS在几万左右，可以支撑的读QPS可以上10万以上（随你的需求，水平扩容slave节点就可以），可用性在99.99%可以用公司里的一些已有的数据，导入进去，几百万，一千万，进去做各种压力测试，性能，redis-benchmark，并发，QPS，高可用的演练，每台机器最大能存储多少数据量，横向扩容支撑更多数据基于测试环境还有测试数据，做各种演练，去摸索一些最适合自己的一些细节的东西 如果页面的数据有变更，及时监听到，并且写入缓存中，提供高并发，高性能的访问 前端请求，请求服务器，先从redis中拿，redis中没有，本地搭建ehcache没有，再调用商品服务，从数据库中拿 企业级的大型缓存架构 保证数据库与缓存的双写一致性分布式缓存重建问题，使用分布式锁实现1详细在文章关于zookeeper搭建分布式锁中 服务本地堆缓存，避免数据库直接裸奔因为之前跟大家提过，三级缓存，多级缓存，服务本地堆缓存 + redis分布式缓存 + nginx本地缓存组成的 每一层缓存在高并发的场景下，都有其特殊的用途，需要综合利用多级的缓存，才能支撑住高并发场景下各种各样的特殊情况 服务本地堆缓存，作用，预防redis层的彻底崩溃，作为缓存的最后一道防线，避免数据库直接裸奔 服务本地堆缓存，我们用什么来做缓存，难道我们自己手写一个类或者程序去管理内存吗？？？java最流行的缓存的框架，ehcache 所以我们也是用ehcache来做本地的堆缓存 spring boot + ehcache整合起来，演示一下是怎么使用的 spring boot整合ehcache （1）依赖 org.springframework.boot spring-boot-starter-data-jpa org.springframework spring-context-support net.sf.ehcache ehcache 2.8.3 （2）缓存配置管理类 @Configuration@EnableCachingpublic class CacheConfiguration { @Bean public EhCacheManagerFactoryBean ehCacheManagerFactoryBean(){ EhCacheManagerFactoryBean cacheManagerFactoryBean = new EhCacheManagerFactoryBean(); cacheManagerFactoryBean.setConfigLocation(new ClassPathResource(&quot;ehcache.xml&quot;)); cacheManagerFactoryBean.setShared(true); return cacheManagerFactoryBean; } @Bean public EhCacheCacheManager ehCacheCacheManager(EhCacheManagerFactoryBean bean){ return new EhCacheCacheManager(bean.getObject()); } } （3）ehcache.xml &lt;?xml version=”1.0” encoding=”UTF-8”?&gt; （4）CacheService @Service(“cacheService”)public class CacheServiceImpl implements CacheService { 1234567891011public static final String CACHE_NAME = "local";@Cacheable(value = CACHE_NAME, key = "'key_'+#id")public ProductInfo findById(Long id)&#123; return null;&#125; @CachePut(value = CACHE_NAME, key = "'key_'+#productInfo.getId()")public ProductInfo saveProductInfo(ProductInfo productInfo) &#123; return productInfo;&#125; } （5）写一个Controller测试一下ehcache的整合 1234567891011121314151617181920212223@Controllerpublic class CacheTestController &#123; @Resource private CacheService cacheService; @RequestMapping("/testPutCache") @ResponseBody public void testPutCache(ProductInfo productInfo) &#123; System.out.println(productInfo.getId() + ":" + productInfo.getName()); cacheService.saveProductInfo(productInfo); &#125; @RequestMapping("/testGetCache") @ResponseBody public ProductInfo testGetCache(Long id) &#123; ProductInfo productInfo = cacheService.findById(id); System.out.println(productInfo.getId() + ":" + productInfo.getName()); return productInfo; &#125;&#125; ehcache已经整合进了我们的系统，spring boot 封装好了对ehcache本地缓存进行添加和获取的方法和service 小程序写入操作，防止并发中，HashMap的使用，改成使用ConcurrentHashMap 10.1商品详情页的多级缓存架构我们之前的三十讲，主要是在讲解redis如何支撑海量数据、高并发读写、高可用服务的架构，redis架构 redis架构，在我们的真正类似商品详情页读高并发的系统中，redis就是底层的缓存存储的支持 从这一讲开始，我们正式开始做业务系统的开发 亿级流量以上的电商网站的商品详情页的系统，商品详情页系统，大量的业务，十几个人做一两年，堆出来复杂的业务系统 几十个小时的课程，讲解复杂的业务 把整体的架构给大家讲解清楚，然后浓缩和精炼里面的业务，提取部分业务，做一些简化，把整个详情页系统的流程跑出来 架构，骨架，有少量的业务，血和肉，把整个项目串起来，在业务背景下，去学习架构 讲解商品详情页系统，缓存架构，90%大量的业务代码（没有什么技术含量），10%的最优技术含量的就是架构，上亿流量，每秒QPS几万，上十万的，读并发 读并发，缓存架构 1、上亿流量的商品详情页系统的多级缓存架构 很多人以为，做个缓存，其实就是用一下redis，访问一下，就可以了，简单的缓存 做复杂的缓存，支撑电商复杂的场景下的高并发的缓存，遇到的问题，非常非常之多，绝对不是说简单的访问一下redsi就可以了 采用三级缓存：nginx本地缓存+redis分布式缓存+tomcat堆缓存的多级缓存架构 时效性要求非常高的数据：库存 一般来说，显示的库存，都是时效性要求会相对高一些，因为随着商品的不断的交易，库存会不断的变化 当然，我们就希望当库存变化的时候，尽可能更快将库存显示到页面上去，而不是说等了很长时间，库存才反应到页面上去 时效性要求不高的数据：商品的基本信息（名称、颜色、版本、规格参数，等等） 时效性要求不高的数据，就还好，比如说你现在改变了商品的名称，稍微晚个几分钟反应到商品页面上，也还能接受 商品价格/库存等时效性要求高的数据，而且种类较少，采取相关的服务系统每次发生了变更的时候，直接采取数据库和redis缓存双写的方案，这样缓存的时效性最高 商品基本信息等时效性不高的数据，而且种类繁多，来自多种不同的系统，采取MQ异步通知的方式，写一个数据生产服务，监听MQ消息，然后异步拉取服务的数据，更新tomcat jvm缓存+redis缓存 nginx+lua脚本做页面动态生成的工作，每次请求过来，优先从nginx本地缓存中提取各种数据，结合页面模板，生成需要的页面 如果nginx本地缓存过期了，那么就从nginx到redis中去拉取数据，更新到nginx本地 如果redis中也被LRU算法清理掉了，那么就从nginx走http接口到后端的服务中拉取数据，数据生产服务中，现在本地tomcat里的jvm堆缓存中找，ehcache，如果也被LRU清理掉了，那么就重新发送请求到源头的服务中去拉取数据，然后再次更新tomcat堆内存缓存+redis缓存，并返回数据给nginx，nginx缓存到本地 2、多级缓存架构中每一层的意义 nginx本地缓存，抗的是热数据的高并发访问，一般来说，商品的购买总是有热点的，比如每天购买iphone、nike、海尔等知名品牌的东西的人，总是比较多的 这些热数据，利用nginx本地缓存，由于经常被访问，所以可以被锁定在nginx的本地缓存内 大量的热数据的访问，就是经常会访问的那些数据，就会被保留在nginx本地缓存内，那么对这些热数据的大量访问，就直接走nginx就可以了 那么大量的访问，直接就可以走到nginx就行了，不需要走后续的各种网络开销了 redis分布式大规模缓存，抗的是很高的离散访问，支撑海量的数据，高并发的访问，高可用的服务 redis缓存最大量的数据，最完整的数据和缓存，1T+数据; 支撑高并发的访问，QPS最高到几十万; 可用性，非常好，提供非常稳定的服务 nginx本地内存有限，也就能cache住部分热数据，除了各种iphone、nike等热数据，其他相对不那么热的数据，可能流量会经常走到redis那里 利用redis cluster的多master写入，横向扩容，1T+以上海量数据支持，几十万的读写QPS，99.99%高可用性，那么就可以抗住大量的离散访问请求 tomcat jvm堆内存缓存，主要是抗redis大规模灾难的，如果redis出现了大规模的宕机，导致nginx大量流量直接涌入数据生产服务，那么最后的tomcat堆内存缓存至少可以再抗一下，不至于让数据库直接裸奔 同时tomcat jvm堆内存缓存，也可以抗住redis没有cache住的最后那少量的部分缓存 Cache Aside Pattern缓存 + 数据库读写模式的分析最经典的缓存+数据库读写的模式，cache aside pattern 1、Cache Aside Pattern （1）读的时候，先读缓存，缓存没有的话，那么就读数据库，然后取出数据后放入缓存，同时返回响应 （2）更新的时候，先删除缓存，然后再更新数据库 2、为什么是删除缓存，而不是更新缓存呢？ 原因很简单，很多时候，复杂点的缓存的场景，因为缓存有的时候，不简单是数据库中直接取出来的值 商品详情页的系统，修改库存，只是修改了某个表的某些字段，但是要真正把这个影响的最终的库存计算出来，可能还需要从其他表查询一些数据，然后进行一些复杂的运算，才能最终计算出 现在最新的库存是多少，然后才能将库存更新到缓存中去 比如可能更新了某个表的一个字段，然后其对应的缓存，是需要查询另外两个表的数据，并进行运算，才能计算出缓存最新的值的 更新缓存的代价是很高的 是不是说，每次修改数据库的时候，都一定要将其对应的缓存去跟新一份？也许有的场景是这样的，但是对于比较复杂的缓存数据计算的场景，就不是这样了 如果你频繁修改一个缓存涉及的多个表，那么这个缓存会被频繁的更新，频繁的更新缓存 但是问题在于，这个缓存到底会不会被频繁访问到？？？ 举个例子，一个缓存涉及的表的字段，在1分钟内就修改了20次，或者是100次，那么缓存跟新20次，100次; 但是这个缓存在1分钟内就被读取了1次，有大量的冷数据 28法则，黄金法则，20%的数据，占用了80%的访问量 实际上，如果你只是删除缓存的话，那么1分钟内，这个缓存不过就重新计算一次而已，开销大幅度降低 每次数据过来，就只是删除缓存，然后修改数据库，如果这个缓存，在1分钟内只是被访问了1次，那么只有那1次，缓存是要被重新计算的，用缓存才去算缓存 其实删除缓存，而不是更新缓存，就是一个lazy计算的思想，不要每次都重新做复杂的计算，不管它会不会用到，而是让它到需要被使用的时候再重新计算 mybatis，hibernate，懒加载，思想 查询一个部门，部门带了一个员工的list，没有必要说每次查询部门，都里面的1000个员工的数据也同时查出来啊 80%的情况，查这个部门，就只是要访问这个部门的信息就可以了 先查部门，同时要访问里面的员工，那么这个时候只有在你要访问里面的员工的时候，才会去数据库里面查询1000个员工]]></content>
      <tags>
        <tag>项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络零散知识]]></title>
    <url>%2F2019%2F09%2F28%2F%E7%BD%91%E7%BB%9C%E9%9B%B6%E6%95%A3%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[TCP和UDP的区别（1）TCP是面向连接的，udp是无连接的即发送数据前不需要先建立链接。 （2）TCP提供可靠的服务。也就是说，通过TCP连接传送的数据，无差错，不丢失，不重复，且按序到达;UDP尽最大努力交付，即不保证可靠交付。 并且因为tcp可靠，面向连接，不会丢失数据因此适合大数据量的交换。 （3）TCP是面向字节流，UDP面向报文，并且网络出现拥塞不会使得发送速率降低（因此会出现丢包，对实时的应用比如IP电话和视频会议等）。 （4）TCP只能是1对1的，UDP支持1对1,1对多。 （5）TCP的首部较大为20字节，而UDP只有8字节。 （6）TCP是面向连接的可靠性传输，而UDP是不可靠的。 基于TCP的应用层协议有：POP3、SMTP（简单邮件传输协议）、TELNET（远程登陆协议）、HTTP（超文本传输协议）、HTTPS（超文本传输安全协议）、FTP（文件传输协议） 基于UDP的应用层协议：TFTP（简单文件传输协议）、RIP（路由信息协议）、DHCP（动态主机设置协议）、BOOTP（引导程序协议，DHCP的前身）、IGMP（Internet组管理协议） 基于TCP和UDP协议：DNS（域名系统）、ECHO（回绕协议） 以下内容来自：公众号 前端指南，作者 前端指南 简述 http 1.1 与 http 1.0 的区别 http 1.0 对于每个连接都得建立一次连接, 一次只能传送一个请求和响应, 请求就会关闭, http1.0 没有 Host 字段 而 http1.1 在同一个连接中可以传送多个请求和响应, 多个请求可以重叠和同时进行, http1.1 必须有 host 字段 http1.1 中引入了 ETag 头, 它的值 entity tag 可以用来唯一的描述一个资源. 请求消息中可以使用 If-None-Match 头域来匹配资源的 entitytag 是否有变化 http1.1 新增了 Cache-Control 头域(消息请求和响应请求都可以使用), 它支持一个可扩展的指令子集 http1.0 中只定义了 16 个状态响应码, 对错误或警告的提示不够具体. http1.1 引入了一个 Warning 头域, 增加对错误或警告信息的描述. 且新增了 24 个状态响应码 从输入 URL 到页面加载发生了什么[必考]总体来说分为以下几个过程: DNS 解析 TCP 连接 发送 HTTP 请求 服务器处理请求并返回 HTTP 报文 浏览器解析渲染页面 连接结束 Socket 连接与 HTTP 连接的联系与区别由于通常情况下 Socket 连接就是 TCP 连接，因此 Socket 连接一旦建立，通信双方即可开始相互发送数据内容，直到双方连接断开。但在实际网络应用中，客户端到服务器之间的通信往往需要穿越多个中间节点，例如路由器、网关、防火墙等，大部分防火墙默认会关闭长时间处于非活跃状态的连接而导致 Socket 连接断连，因此需要通过轮询告诉网络，该连接处于活跃状态。 而 HTTP 连接使用的是“请求—响应”的方式，不仅在请求时需要先建立连接，而且需要客户端向服务器发出请求后，服务器端才能回复数据。 很多情况下，需要服务器端主动向客户端推送数据，保持客户端与服务器数据的实时与同步。此时若双方建立的是 Socket 连接，服务器就可以直接将数据传送给客户端;若双方建立的是 HTTP 连接，则服务器需要等到客户端发送一次请求后才能将数据传回给客户端，因此，客户端定时向服务器端发送连接请求，不仅可以保持在线，同时也是在“询问”服务器是否有新的数据，如果有就将数据传给客户端。 Http2.0 与 http1.x 相比有什么优点(常考) 二进制格式:http1.x 是文本协议，而 http2.0 是二进制以帧为基本单位，是一个二进制协议，一帧中除了包含数据外同时还包含该帧的标识：Stream Identifier，即标识了该帧属于哪个 request,使得网络传输变得十分灵活。 多路复用: 一个很大的改进，原先 http1.x 一个连接一个请求的情况有比较大的局限性，也引发了很多问题，如建立多个连接的消耗以及效率问题。 http1.x 为了解决效率问题，可能会尽量多的发起并发的请求去加载资源，然而浏览器对于同一域名下的并发请求有限制，而优化的手段一般是将请求的资源放到不同的域名下来突破这种限制。 而 http2.0 支持的多路复用可以很好的解决这个问题，多个请求共用一个 TCP 连接，多个请求可以同时在这个 TCP 连接上并发，一个是解决了建立多个 TCP 连接的消耗问题，一个也解决了效率的问题。那么是什么原理支撑多个请求可以在一个 TCP 连接上并发呢？基本原理就是上面的二进制分帧，因为每一帧都有一个身份标识，所以多个请求的不同帧可以并发的无序发送出去，在服务端会根据每一帧的身份标识，将其整理到对应的 request 中。 header 头部压缩:主要是通过压缩 header 来减少请求的大小，减少流量消耗，提高效率。因为之前存在一个问题是，每次请求都要带上 header，而这个 header 中的数据通常是一层不变的。 支持服务端推送]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[查漏补缺]]></title>
    <url>%2F2019%2F09%2F26%2F%E6%9F%A5%E6%BC%8F%E8%A1%A5%E7%BC%BA%2F</url>
    <content type="text"><![CDATA[自19.09.26开始，对复习的知识遗漏点进行总结 19.09.26JAVA锁Java SE1.6为了减少获得锁和释放锁所带来的性能消耗，引入了“偏向锁”和“轻量级锁”，所以在Java SE1.6里锁一共有四种状态，无锁状态，偏向锁状态，轻量级锁状态和重量级锁状态，它会随着竞争情况逐渐升级。锁可以升级但不能降级，意味着偏向锁升级成轻量级锁后不能降级成偏向锁。这种锁升级却不能降级的策略，目的是为了提高获得锁和释放锁的效率。 悲观锁与乐观锁悲观锁总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁，读锁，写锁等，都是在做操作之前先上锁。JAVA中synchronize和ReentrantLock等独占锁就是悲观锁思想的实现。 乐观锁总是假设最好的情况，每次拿数据都认为别人不会修改，所以不会上锁。但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。 乐观锁适用于多读的应用类型，这样可以提高吞吐量。 java中的4种锁，分别是重量级锁，自旋锁，轻量级锁和偏向锁。重量级锁是悲观锁的一种，自旋锁，轻量级锁和偏向锁属于乐观锁。 乐观锁常见的两种实现方式 乐观锁一般会使用版本号机制或CAS算法实现。 1. 版本号机制一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一。当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值为当前数据库中的version值相等时才更新，否则重试更新操作，直到更新成功。 举一个简单的例子： 假设数据库中帐户信息表中有一个 version 字段，当前值为 1 ；而当前帐户余额字段（ balance ）为 $100 。 操作员 A 此时将其读出（ version=1 ），并从其帐户余额中扣除 $50（ $100-$50 ）。 在操作员 A 操作的过程中，操作员B 也读入此用户信息（ version=1 ），并从其帐户余额中扣除 $20 （ $100-$20 ）。 操作员 A 完成了修改工作，将数据版本号加一（ version=2 ），连同帐户扣除后余额（ balance=$50 ），提交至数据库更新，此时由于提交数据版本大于数据库记录当前版本，数据被更新，数据库记录 version 更新为 2 。 操作员 B 完成了操作，也将版本号加一（ version=2 ）试图向数据库提交数据（ balance=$80 ），但此时比对数据库记录版本时发现，操作员 B 提交的数据版本号为 2 ，数据库记录当前版本也为 2 ，不满足 “ 提交版本必须大于记录当前版本才能执行更新 “ 的乐观锁策略，因此，操作员 B 的提交被驳回。 这样，就避免了操作员 B 用基于 version=1 的旧数据修改的结果覆盖操作员A 的操作结果的可能。 2.简单回顾一下CAS算法CAS算法 即compare and swap（比较与交换），是一种有名的无锁算法。无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）。CAS算法涉及到三个操作数 需要读写的内存值 V 进行比较的值 A 拟写入的新值 B 当且仅当 V 的值等于 A时，CAS通过原子方式用新值B来更新V的值，否则不会执行任何操作（比较和替换是一个原子操作）。一般情况下是一个自旋操作，即不断的重试。 乐观锁的缺点 ABA 问题是乐观锁一个常见的问题 1 ABA 问题如果一个变量V初次读取的时候是A值，并且在准备赋值的时候检查到它仍然是A值，那我们就能说明它的值没有被其他线程修改过了吗？很明显是不能的，因为在这段时间它的值可能被改为其他值，然后又改回A，那CAS操作就会误认为它从来没有被修改过。这个问题被称为CAS操作的 “ABA”问题。 JDK 1.5 以后的 AtomicStampedReference 类就提供了此种能力，其中的 compareAndSet 方法就是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 2 循环时间长开销大自旋CAS（也就是不成功就一直循环执行直到成功）如果长时间不成功，会给CPU带来非常大的执行开销。 如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，pause指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline flush），从而提高CPU的执行效率。 3 只能保证一个共享变量的原子操作CAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效。但是从 JDK 1.5开始，提供了AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作.所以我们可以使用锁或者利用AtomicReference类把多个共享变量合并成一个共享变量来操作。 什么是自旋锁？自旋锁（spinlock）：是指当一个线程在获取锁的时候，如果锁已经被其它线程获取，那么该线程将循环等待，然后不断的判断锁是否能够被成功获取，直到获取到锁才会退出循环。 获取锁的线程一直处于活跃状态，但是并没有执行任何有效的任务，使用这种锁会造成busy-waiting。 它是为实现保护共享资源而提出一种锁机制。其实，自旋锁与互斥锁比较类似，它们都是为了解决对某项资源的互斥使用。无论是互斥锁，还是自旋锁，在任何时刻，最多只能有一个保持者，也就说，在任何时刻最多只能有一个执行单元获得锁。但是两者在调度机制上略有不同。对于互斥锁，如果资源已经被占用，资源申请者只能进入睡眠状态。但是自旋锁不会引起调用者睡眠，如果自旋锁已经被别的执行单元保持，调用者就一直循环在那里看是否该自旋锁的保持者已经释放了锁，”自旋”一词就是因此而得名。 Java如何实现自旋锁？下面是个简单的例子： 1234567891011121314public class SpinLock &#123; private AtomicReference&lt;Thread&gt; cas = new AtomicReference&lt;Thread&gt;(); public void lock() &#123; Thread current = Thread.currentThread(); // 利用CAS while (!cas.compareAndSet(null, current)) &#123; // DO nothing &#125; &#125; public void unlock() &#123; Thread current = Thread.currentThread(); cas.compareAndSet(current, null); &#125;&#125;1234567891011121314 lock（)方法利用的CAS，当第一个线程A获取锁的时候，能够成功获取到，不会进入while循环，如果此时线程A没有释放锁，另一个线程B又来获取锁，此时由于不满足CAS，所以就会进入while循环，不断判断是否满足CAS，直到A线程调用unlock方法释放了该锁。 自旋锁存在的问题 如果某个线程持有锁的时间过长，就会导致其它等待获取锁的线程进入循环等待，消耗CPU。使用不当会造成CPU使用率极高。 上面Java实现的自旋锁不是公平的，即无法满足等待时间最长的线程优先获取锁。不公平的锁就会存在“线程饥饿”问题。 自旋锁的优点 自旋锁不会使线程状态发生切换，一直处于用户态，即线程一直都是active的；不会使线程进入阻塞状态，减少了不必要的上下文切换，执行速度快 非自旋锁在获取不到锁的时候会进入阻塞状态，从而进入内核态，当获取到锁的时候需要从内核态恢复，需要线程上下文切换。 （线程被阻塞后便进入内核（Linux）调度状态，这个会导致系统在用户态与内核态之间来回切换，严重影响锁的性能） TicketLock主要解决的是公平性的问题。思路：每当有线程获取锁的时候，就给该线程分配一个递增的id，我们称之为排队号，同时，锁对应一个服务号，每当有线程释放锁，服务号就会递增，此时如果服务号与某个线程排队号一致，那么该线程就获得锁，由于排队号是递增的，所以就保证了最先请求获取锁的线程可以最先获取到锁，就实现了公平性。 可以想象成银行办理业务排队，排队的每一个顾客都代表一个需要请求锁的线程，而银行服务窗口表示锁，每当有窗口服务完成就把自己的服务号加一，此时在排队的所有顾客中，只有自己的排队号与服务号一致的才可以得到服务。 偏向锁Java偏向锁(Biased Locking)是Java6引入的一项多线程优化。偏向锁，顾名思义，它会偏向于第一个访问锁的线程，如果在运行过程中，同步锁只有一个线程访问，不存在多线程争用的情况，则线程是不需要触发同步的，这种情况下，就会给线程加一个偏向锁。如果在运行过程中，遇到了其他线程抢占锁，则持有偏向锁的线程会被挂起，JVM会消除它身上的偏向锁，将锁恢复到标准的轻量级锁。 它通过消除资源无竞争情况下的同步原语，进一步提高了程序的运行性能。 偏向锁的实现偏向锁获取过程： 访问Mark Word中偏向锁的标识是否设置成1，锁标志位是否为01，确认为可偏向状态。 如果为可偏向状态，则测试线程ID是否指向当前线程，如果是，进入步骤5，否则进入步骤3。 如果线程ID并未指向当前线程，则通过CAS操作竞争锁。如果竞争成功，则将Mark Word中线程ID设置为当前线程ID，然后执行5；如果竞争失败，执行4。 如果CAS获取偏向锁失败，则表示有竞争。当到达全局安全点（safepoint）时获得偏向锁的线程被挂起，偏向锁升级为轻量级锁，然后被阻塞在安全点的线程继续往下执行同步代码。（撤销偏向锁的时候会导致stop the word） 执行同步代码。 注意：第四步中到达安全点safepoint会导致stop the word，时间很短。 偏向锁的释放：偏向锁的撤销在上述第四步骤中有提到。偏向锁只有遇到其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁，线程不会主动去释放偏向锁。偏向锁的撤销，需要等待全局安全点（在这个时间点上没有字节码正在执行），它会首先暂停拥有偏向锁的线程，判断锁对象是否处于被锁定状态，撤销偏向锁后恢复到未锁定（标志位为“01”）或轻量级锁（标志位为“00”）的状态。 偏向锁的适用场景始终只有一个线程在执行同步块，在它没有执行完释放锁之前，没有其它线程去执行同步块，在锁无竞争的情况下使用，一旦有了竞争就升级为轻量级锁，升级为轻量级锁的时候需要撤销偏向锁，撤销偏向锁的时候会导致stop the word操作；在有锁的竞争时，偏向锁会多做很多额外操作，尤其是撤销偏向所的时候会导致进入安全点，安全点会导致stw，导致性能下降，这种情况下应当禁用； 总结自旋锁线程的阻塞和唤醒需要CPU从用户态转为核心态，频繁的阻塞和唤醒对CPU来说是一件负担很重的工作。同时我们可以发现，很多对象锁的锁定状态只会持续很短的一段时间，例如整数的自加操作，在很短的时间内阻塞并唤醒线程显然不值得，为此引入了自旋锁。 所谓“自旋”，就是让线程去执行一个无意义的循环，循环结束后再去重新竞争锁，如果竞争不到继续循环，循环过程中线程会一直处于running状态，但是基于JVM的线程调度，会出让时间片，所以其他线程依旧有申请锁和释放锁的机会。 自旋锁省去了阻塞锁的时间空间（队列的维护等）开销，但是长时间自旋就变成了“忙式等待”，忙式等待显然还不如阻塞锁。所以自旋的次数一般控制在一个范围内，例如10,100等，在超出这个范围后，自旋锁会升级为阻塞锁。（即从轻量级锁转变为重量级锁） 轻量级锁加锁 线程在执行同步块之前，JVM会先在当前线程的栈桢中创建用于存储锁记录的空间，并将对象头中的Mark Word复制到锁记录中，官方称为Displaced Mark Word。然后线程尝试使用CAS将对象头中的Mark Word替换为指向锁记录的指针。如果成功，当前线程获得锁，如果失败，则自旋获取锁，当自旋获取锁仍然失败时，表示存在其他线程竞争锁(两条或两条以上的线程竞争同一个锁)，则轻量级锁会膨胀成重量级锁。 解锁 轻量级解锁时，会使用原子的CAS操作来将Displaced Mark Word替换回到对象头，如果成功，则表示同步过程已完成。如果失败，表示有其他线程尝试过获取该锁，则要在释放锁的同时唤醒被挂起的线程。 重量级锁重量锁在JVM中又叫对象监视器（Monitor），它很像C中的Mutex，除了具备Mutex(0|1)互斥的功能，它还负责实现了Semaphore(信号量)的功能，也就是说它至少包含一个竞争锁的队列，和一个信号阻塞队列（wait队列），前者负责做互斥，后一个用于做线程同步。 如果说轻量级锁是在无竞争的情况下使用CAS操作去消除同步使用的互斥量，那偏向锁就是在无竞争的情况下把整个同步都消除掉，连CAS都不做了。 ConcurrentHashMapJDk7版本 ConcurrentHashMap的锁分段技术可有效提升并发访问率 HashTable容器在竞争激烈的并发环境下表现出效率低下的原因是所有访问HashTable的线程都必须竞争同一把锁，假如容器中有多把锁，每一个把锁用于锁容器其中一部分数据，那么当多线程访问容器里不同数据段的数据时，线程间就不会存在锁竞争，从而可以有效提高并发访问效率，这就是ConcurrentHashMap所使用的锁分段技术。首先将数据分为一段一段地存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能够别其他数据访问。 ConcurrentHashMap的结构ConcurrentHashMap是由Segment数组结构和HashEntry数组结构组成。Segment是一种可重入锁（ReentrantLock），在ConcurrentHashMap里扮演锁的角色；HashEntry则用于存储键值对数据。Segment的结构和HashMap类似，是一种数组和链表结构。一个Segment里包含一个HashEntry数组，每个HashEntry里是一个链表结构的元素，每个Segment守护着一个HashEntry数组里的元素，当对HashEntry数组的数据进行修改时，必须首先获得与它对应的Segment锁。 getJDK1.7的ConcurrentHashMap的get操作是不加锁的，因为在每个Segment中定义的HashEntry数组和在每个HashEntry中定义的value和next HashEntry节点都是volatile类型的，volatile类型的变量可以保证其在多线程之间的可见性，因此可以被多个线程同时读，从而不用加锁。而其get操作步骤也比较简单，定位Segment –&gt; 定位HashEntry –&gt; 通过getObjectVolatile()方法获取指定偏移量上的HashEntry –&gt; 通过循环遍历链表获取对应值。 定位Segment：(((h &gt;&gt;&gt; segmentShift) &amp; segmentMask) &lt;&lt; SSHIFT) + SBASE 定位HashEntry：(((tab.length - 1) &amp; h)) &lt;&lt; TSHIFT) + TBASE put在Segment的put方法中，首先需要调用tryLock()方法获取锁，然后通过hash算法定位到对应的HashEntry，然后遍历整个链表，如果查到key值，则直接插入元素即可；而如果没有查询到对应的key，则需要调用rehash()方法对Segment中保存的table进行扩容，扩容为原来的2倍，并在扩容之后插入对应的元素。插入一个key/value对后，需要将统计Segment中元素个数的count属性加1。最后，插入成功之后，需要使用unLock()释放锁。 JDK8版本在JDK1.7之前，ConcurrentHashMap是通过分段锁机制来实现的，所以其最大并发度受Segment的个数限制。因此，在JDK1.8中，ConcurrentHashMap的实现原理摒弃了这种设计，而是选择了与HashMap类似的数组+链表+红黑树的方式实现，而加锁则采用CAS和synchronized实现。 数据结构JDK1.8的ConcurrentHashMap数据结构比JDK1.7之前的要简单的多，其使用的是HashMap一样的数据结构：数组+链表+红黑树。ConcurrentHashMap中包含一个table数组，其类型是一个Node数组；而Node是一个继承自Map.Entry&lt;K, V&gt;的链表，而当这个链表结构中的数据大于8，则将数据结构升级为TreeBin类型的红黑树结构。另外， JDK1.8中的ConcurrentHashMap中还包含一个重要属性sizeCtl，其是一个控制标识符，不同的值代表不同的意思：其为0时，表示hash表还未初始化，而为正数时这个数值表示初始化或下一次扩容的大小，相当于一个阈值；即如果hash表的实际大小&gt;=sizeCtl，则进行扩容，默认情况下其是当前ConcurrentHashMap容量的0.75倍；而如果sizeCtl为-1，表示正在进行初始化操作；而为-N时，则表示有N-1个线程正在进行扩容。 源码12345678910111213141516//内部类node里的val，next都使用了volatile关键字static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; volatile V val; volatile Node&lt;K,V&gt; next; Node(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.val = val; this.next = next; &#125; &#125;// 使用node数组， 也是用volatile数组transient volatile Node&lt;K,V&gt;[] table; get方法同1.7的一致，由于node都是使用volatile修饰，在get的源码实现中，并不用加锁。 SQL注入简介SQL注入是网站存在最多也是最简单的漏洞，主要原因是程序员在开发用户和数据库交互的系统时，没有对用户输入的字符串进行过滤，转义，限制或处理不严谨，导致用户可以通过输入精心构造的字符串去非法获取到数据库中的数据。 SQL注入原理一般用户登录用的SQL语句为：SELECT FROM user WHERE username=’admin’ AND password=’passwd’，此处admin和passwd分别为用户输入的用户名和密码，如果程序员没有对用户输入的用户名和密码做处理，就可以构造万能密码成功绕过登录验证，如用户输入‘or 1#,SQL语句将变为：SELECT FROM user WHERE username=’’or 1#’ AND password=’’，‘’or 1为TRUE，#注释掉后面的内容，所以查询语句可以正确执行。 mybatis是如何防止SQL注入的1、首先看一下下面两个sql语句的区别：123456&lt;select id="selectByNameAndPassword" parameterType="java.util.Map" resultMap="BaseResultMap"&gt;select id, username, password, rolefrom userwhere username = #&#123;username,jdbcType=VARCHAR&#125;and password = #&#123;password,jdbcType=VARCHAR&#125;&lt;/select&gt; 123456&lt;select id="selectByNameAndPassword" parameterType="java.util.Map" resultMap="BaseResultMap"&gt;select id, username, password, rolefrom userwhere username = $&#123;username,jdbcType=VARCHAR&#125;and password = $&#123;password,jdbcType=VARCHAR&#125;&lt;/select&gt; mybatis中的#和$的区别： 1、#将传入的数据都当成一个字符串，会对自动传入的数据加一个双引号。如：where username=#{username}，如果传入的值是111,那么解析成sql时的值为where username=”111”, 如果传入的值是id，则解析成的sql为where username=”id”. 2、$将传入的数据直接显示生成在sql中。如：where username=${username}，如果传入的值是111,那么解析成sql时的值为where username=111；如果传入的值是;drop table user;，则解析成的sql为：select id, username, password, role from user where username=;drop table user;3、#方式能够很大程度防止sql注入，$方式无法防止Sql注入。4、$方式一般用于传入数据库对象，例如传入表名.5、一般能用#的就别用$，若不得不使用“${xxx}”这样的参数，要手工地做好过滤工作，来防止sql注入攻击。6、在MyBatis中，“${xxx}”这样格式的参数会直接参与SQL编译，从而不能避免注入攻击。但涉及到动态表名和列名时，只能使用“${xxx}”这样的参数格式。所以，这样的参数需要我们在代码中手工进行处理来防止注入。【结论】在编写MyBatis的映射语句时，尽量采用“#{xxx}”这样的格式。若不得不使用“${xxx}”这样的参数，要手工地做好过滤工作，来防止SQL注入攻击。 2、什么是sql注入 sql注入解释：是一种代码注入技术，用于攻击数据驱动的应用，恶意的SQL语句被插入到执行的实体字段中（例如，为了转储数据库内容给攻击者） SQL**注入，大家都不陌生，是一种常见的攻击方式。攻击者在界面的表单信息或URL上输入一些奇怪的SQL片段（例如“or ‘1’=’1’”这样的语句），有可能入侵参数检验不足的应用程序。所以，在我们的应用中需要做一些工作，来防备这样的攻击方式。在一些安全性要求很高的应用中（比如银行软件），经常使用将SQL**语句全部替换为存储过程这样的方式，来防止SQL注入。这当然是一种很安全的方式，但我们平时开发中，可能不需要这种死板的方式。 3、mybatis是如何做到防止sql注入的 MyBatis框架作为一款半自动化的持久层框架，其SQL语句都要我们自己手动编写，这个时候当然需要防止SQL注入。其实，MyBatis的SQL是一个具有“输入+输出”的功能，类似于函数的结构，参考上面的两个例子。其中，parameterType表示了输入的参数类型，resultType表示了输出的参数类型。回应上文，如果我们想防止SQL注入，理所当然地要在输入参数上下功夫。上面代码中使用 # 的即输入参数在SQL中拼接的部分，传入参数后，打印出执行的SQL语句，会看到SQL是这样的： 1select id, username, password, role from user where username=? and password=? 不管输入什么参数，打印出的SQL都是这样的。这是因为MyBatis启用了预编译功能，在SQL执行前，会先将上面的SQL发送给数据库进行编译；执行时，直接使用编译好的SQL，替换占位符“?”就可以了。因为SQL注入只能对编译过程起作用，所以这样的方式就很好地避免了SQL注入的问题。 【底层实现原理】MyBatis是如何做到SQL预编译的呢？其实在框架底层，是JDBC中的PreparedStatement类在起作用，PreparedStatement是我们很熟悉的Statement的子类，它的对象包含了编译好的SQL语句。这种“准备好”的方式不仅能提高安全性，而且在多次执行同一个SQL时，能够提高效率。原因是SQL已编译好，再次执行时无需再编译。 1234567//安全的，预编译了的Connection conn = getConn();//获得连接String sql = &quot;select id, username, password, role from user where id=?&quot;; //执行sql前会预编译号该条语句PreparedStatement pstmt = conn.prepareStatement(sql); pstmt.setString(1, id); ResultSet rs=pstmt.executeUpdate(); ...... 12345678910//不安全的，没进行预编译private String getNameByUserId(String userId) &#123; Connection conn = getConn();//获得连接 String sql = &quot;select id,username,password,role from user where id=&quot; + id; //当id参数为&quot;3;drop table user;&quot;时，执行的sql语句如下: //select id,username,password,role from user where id=3; drop table user; PreparedStatement pstmt = conn.prepareStatement(sql); ResultSet rs=pstmt.executeUpdate(); ......&#125; 【 结论：】 #{}：相当于JDBC中的PreparedStatement ${}：是输出变量的值 简单说，#{}是经过预编译的，是安全的；${}是未经过预编译的，仅仅是取变量的值，是非安全的，存在SQL注入。如果我们order by语句后用了${}，那么不做任何处理的时候是存在SQL注入危险的。你说怎么防止，那我只能悲惨的告诉你，你得手动处理过滤一下输入的内容。如判断一下输入的参数的长度是否正常（注入语句一般很长），更精确的过滤则可以查询一下输入的参数是否在预期的参数集合中。 作者：淼淼之森 ### 19.09.27mybatis面试题总结 Spring常见面试 dockerDocker 可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。 容器虚拟化的是操作系统而不是硬件，容器之间是共享同一套操作系统资源的。虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统。因此容器的隔离级别会稍低一些。 容器是完全使用沙箱机制，相互之间不会有任何接口（类似 iPhone 的 app）,更重要的是容器性能开销极低。 Spring Security区分认证 (Authentication) 和授权 (Authorization)这是一个绝大多数人都会混淆的问题。首先先从读音上来认识这两个名词，很多人都会把它俩的读音搞混，所以我建议你先先去查一查这两个单词到底该怎么读，他们的具体含义是什么。 Authentication（认证） 是验证您的身份的凭据（例如用户名/用户ID和密码），通过这个凭据，系统得以知道你就是你，也就是说系统存在你这个用户。所以，Authentication 被称为身份/用户验证。 Authorization（授权） 发生在 Authentication（认证）之后。授权嘛，光看意思大家应该就明白，它主要掌管我们访问系统的权限。比如有些特定资源只能具有特定权限的人才能访问比如admin，有些对系统资源操作比如删除、添加、更新只能特定人才具有。 这两个一般在我们的系统中被结合在一起使用，目的就是为了保护我们系统的安全性。 Redis为什么要用 redis/为什么要用缓存主要从“高性能”和“高并发”这两点来看待这个问题。 高性能： 假如用户第一次访问数据库中的某些数据。这个过程会比较慢，因为是从硬盘上读取的。将该用户访问的数据存在缓存中，这样下一次再访问这些数据的时候就可以直接从缓存中获取了。操作缓存就是直接操作内存，所以速度相当快。如果数据库中的对应数据改变的之后，同步改变缓存中相应的数据即可！ 高并发： 直接操作缓存能够承受的请求是远远大于直接访问数据库的，所以我们可以考虑把数据库中的部分数据转移到缓存中去，这样用户的一部分请求会直接到缓存这里而不用经过数据库。 redis 设置过期时间Redis中有个设置时间过期的功能，即对存储在 redis 数据库中的值可以设置一个过期时间。作为一个缓存数据库，这是非常实用的。如我们一般项目中的 token 或者一些登录信息，尤其是短信验证码都是有时间限制的，按照传统的数据库处理方式，一般都是自己判断过期，这样无疑会严重影响项目性能。 我们 set key 的时候，都可以给一个 expire time，就是过期时间，通过过期时间我们可以指定这个 key 可以存活的时间。 如果假设你设置了一批 key 只能存活1个小时，那么接下来1小时后，redis是怎么对这批key进行删除的？ 定期删除+惰性删除。 通过名字大概就能猜出这两个删除方式的意思了。 定期删除：redis默认是每隔 100ms 就随机抽取一些设置了过期时间的key，检查其是否过期，如果过期就删除。注意这里是随机抽取的。为什么要随机呢？你想一想假如 redis 存了几十万个 key ，每隔100ms就遍历所有的设置过期时间的 key 的话，就会给 CPU 带来很大的负载！ 惰性删除 ：定期删除可能会导致很多过期 key 到了时间并没有被删除掉。所以就有了惰性删除。假如你的过期 key，靠定期删除没有被删除掉，还停留在内存里，除非你的系统去查一下那个 key，才会被redis给删除掉。这就是所谓的惰性删除，也是够懒的哈！ 但是仅仅通过设置过期时间还是有问题的。我们想一下：如果定期删除漏掉了很多过期 key，然后你也没及时去查，也就没走惰性删除，此时会怎么样？如果大量过期key堆积在内存里，导致redis内存块耗尽了。怎么解决这个问题呢？ redis 内存淘汰机制。 redis 持久化机制(怎么保证 redis 挂掉之后再重启数据可以进行恢复)很多时候我们需要持久化数据也就是将内存中的数据写入到硬盘里面，大部分原因是为了之后重用数据（比如重启机器、机器故障之后恢复数据），或者是为了防止系统故障而将数据备份到一个远程位置。 Redis不同于Memcached的很重一点就是，Redis支持持久化，而且支持两种不同的持久化操作。Redis的一种持久化方式叫快照（snapshotting，RDB），另一种方式是只追加文件（append-only file,AOF）。这两种方法各有千秋，下面我会详细这两种持久化方法是什么，怎么用，如何选择适合自己的持久化方法。 快照（snapshotting）持久化（RDB） Redis可以通过创建快照来获得存储在内存里面的数据在某个时间点上的副本。Redis创建快照之后，可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本（Redis主从结构，主要用来提高Redis性能），还可以将快照留在原地以便重启服务器的时候使用。 快照持久化是Redis默认采用的持久化方式，在redis.conf配置文件中默认有此下配置： 12345save 900 1 #在900秒(15分钟)之后，如果至少有1个key发生变化，Redis就会自动触发BGSAVE命令创建快照。save 300 10 #在300秒(5分钟)之后，如果至少有10个key发生变化，Redis就会自动触发BGSAVE命令创建快照。save 60 10000 #在60秒(1分钟)之后，如果至少有10000个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 AOF（append-only file）持久化 与快照持久化相比，AOF持久化 的实时性更好，因此已成为主流的持久化方案。默认情况下Redis没有开启AOF（append only file）方式的持久化，可以通过appendonly参数开启： 1appendonly yes 开启AOF持久化后每执行一条会更改Redis中的数据的命令，Redis就会将该命令写入硬盘中的AOF文件。AOF文件的保存位置和RDB文件的位置相同，都是通过dir参数设置的，默认的文件名是appendonly.aof。 在Redis的配置文件中存在三种不同的 AOF 持久化方式，它们分别是： 123appendfsync always #每次有数据修改发生时都会写入AOF文件,这样会严重降低Redis的速度appendfsync everysec #每秒钟同步一次，显示地将多个写命令同步到硬盘appendfsync no #让操作系统决定何时进行同步 为了兼顾数据和写入性能，用户可以考虑 appendfsync everysec选项 ，让Redis每秒同步一次AOF文件，Redis性能几乎没受到任何影响。而且这样即使出现系统崩溃，用户最多只会丢失一秒之内产生的数据。当硬盘忙于执行写入操作的时候，Redis还会优雅的放慢自己的速度以便适应硬盘的最大写入速度。 Redis 4.0 对于持久化机制的优化 Redis 4.0 开始支持 RDB 和 AOF 的混合持久化（默认关闭，可以通过配置项 aof-use-rdb-preamble 开启）。 如果把混合持久化打开，AOF 重写的时候就直接把 RDB 的内容写到 AOF 文件开头。这样做的好处是可以结合 RDB 和 AOF 的优点, 快速加载同时避免丢失过多的数据。当然缺点也是有的， AOF 里面的 RDB 部分是压缩格式不再是 AOF 格式，可读性较差。 缓存雪崩和缓存穿透问题解决方案缓存雪崩 简介：缓存同一时间大面积的失效，所以，后面的请求都会落到数据库上，造成数据库短时间内承受大量请求而崩掉。 解决办法（中华石杉老师在他的视频中提到过，视频地址在最后一个问题中有提到）： 事前：尽量保证整个 redis 集群的高可用性，发现机器宕机尽快补上。选择合适的内存淘汰策略。 事中：本地ehcache缓存 + hystrix限流&amp;降级，避免MySQL崩掉 事后：利用 redis 持久化机制保存的数据尽快恢复缓存 缓存穿透 简介：一般是黑客故意去请求缓存中不存在的数据，导致所有的请求都落到数据库上，造成数据库短时间内承受大量请求而崩掉。 解决办法： 有很多种方法可以有效地解决缓存穿透问题，最常见的则是采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被 这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。另外也有一个更为简单粗暴的方法（我们采用的就是这种），如果一个查询返回的数据为空（不管是数 据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。 ThreadLocal]]></content>
      <tags>
        <tag>复习知识总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络]]></title>
    <url>%2F2019%2F09%2F10%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[OSI参考模型OSI参考模型结构包括以下7层：物理层，数据链路层，网络层，传输层，会话层，表示层和应用层。 物理层实现比特流的透明传输，为数据链路层提供数据传输服务物理层的数据传输单位是比特（bit） 数据链路层数据链路层在物理层提供比特流传输的基础上，通过建立数据链路连接，采用差错控制与流量控制方法，使有差错的物理线路变成无差错的数据链路。数据链路层的数据传输单位是帧。 网络层网络层通过路由选择算法为分组通过通信子网选择适当的传输路径，实现流量控制，拥塞控制与网络互联的功能。网络层的数据传输功能是分组。 传输层传输层为分布在不同地理位置计算机的进程通信提供可靠的端-端连接与数据传输服务。传输层的数据传输单元是报文。TCP/IP参考模型TCP/IP起源目前TCP/IP已经成为公认的Internet工业标准与事实上的Internet协议标准。目前使用的TCP/IP是版本4，即IPv4。TCP/IP参考模型的层次主机-网络层与OSI参考模型的数据链路层和物理层对应。TCP/IP协议对主机-网络层并没有规定具体的协议。互联网络层（网络层）(IP)使用的是IP协议，IP是一种不可控，无连接的数据报传输服务协议，它提供的是一种“尽力而为”的服务。传输层（UDP，TCP）传输层定义两种不同的协议，传输控制层（Transport Control Protocol,TCP）与用户数据报协议（User Datagram Protocol，UDP）。TCP是一种可靠的，面向连接，面向字节流的传输层协议。TCP提供比较完善的流量控制与拥塞控制功能。UDP是一种不可靠的，无连接的传输层协议。TCP（Transmisson Control Protocol）TCP 是面向连接的（需要先建立连接）；每一条 TCP 连接只能有两个端点，每一条 TCP 连接只能是一对一；TCP提供可靠交付的服务。通过TCP连接传送的数据，无差错、不丢失、不重复、并且按序到达；TCP 提供全双工通信。TCP 允许通信双方的应用进程在任何时候都能发送数据。TCP 连接的两端都设有发送缓存和接收缓存，用来临时存放双方通信的数据；面向字节流。TCP 中的“流”（Stream）指的是流入进程或从进程流出的字节序列。UDP（User Datagram Protocol）UDP 是无连接的；UDP 是尽最大努力交付，即不保证可靠交付，因此主机不需要维持复杂的链接状态；UDP 是面向报文的；UDP 没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（对实时应用很有用，如直播，实时视频会议等）；UDP 支持一对一、一对多、多对一和多对多的交互通信；UDP 的首部开销小，只有 8 个字节，比 TCP 的 20 个字节的首部要短。 1231.单工数据传输只支持数据在一个方向上传输2.半双工数据传输允许数据在两个方向上传输，但是，在某一时刻，只允许数据在一个方向上传输，它实际上是一种切换方向的单工通信；3.全双工数据通信允许数据同时在两个方向上传输，因此，全双工通信是两个单工通信方式的结合，它要求发送设备和接收设备都有独立的接收和发送能力。 应用层()应用层包括各种标准的网络应用协议，并且总有不断有新的协议加入。TCP/IP应用层基本的协议主要有： 远程登录协议（TELNET） 文件传输协议（File Transfer Protocol,FTP） 超文本传输协议（HTTP） 域名服务协议（DNS）IPv4协议的基本内容IP协议的主要特点 IP协议是一种无连接，不可靠的分组传送服务的协议。 无连接意味着IP协议并不维护IP分组发送后的任何状态信息。每个分组的传输过程是相互独立的。 不可靠意味着IP协议不能保证每个IP分组都能够正确地，不丢失和顺序地到达目的主机。 IP协议是点-点的网络层通信协议。网络层需要在Internet中为通信的两个主机之间寻找一条路径，而这条路径通常是由多个路由器，点-点链路组成。因此，IP协议是针对源主机-路由器，路由器-路由器，路由器-目的主机之间的数据传输的点-点线路的网络层通信协议。IP地址的点分十进制的表示方法“网络号-主机号”的两级IP地址结构。IPv4的地址长度为32位，用点分十进制表示。通常采用x.x.x.x的格式来表示，每个x为8位，每个X的值为0~255. 标准IP地址的分类 A类地址A类地址网络号的第一位为0，其余的7位可以分配，0 网络号（7位），主机号（24位）。A类地址共分为大小相同的128（2^7 = 128）,每一块的netID不同。 B类地址B类地址的前两位为10，其余14位可以分配，可分配的网络号为2^14。B类地址的主机号长度为16位。10 网络号（14） 主机号（16位） C类地址C类地址的前三位为110，其余的21位可以分配。主机号为8位。 D类地址前4位为1110 E类地址前5位为11110。特殊地址形式特殊的IP地址包括以下四种类型 直接广播地址在A类，B类与C类IP地址中，如果主机号是全1，那么这个地址为直接广播地址，路由器将这个分组以广播方式发送给特定网络的所有主机。 受限广播地址32位网络号与主机号为全1的IP地址（255.255.255.255）为受限广播地址。它是用来将一个分组以广播方式发送给本网络中的所有主机。 “这个网络上的特定主机”地址在A类，B类，C类IP地址中，如果网络号是全0（如0.0.0.25），该地址是这个网络上的特定主机地址。划分子网的三级地址结构子网划分的基本思想是：借用主机号的一部分作为子网的子网号，划分出更多的子网IP地址。IPv6地址IPv6的128地址按每16位划分为一个位段，每个位段被转换为一个4位的十六进制数，并用冒号隔开，这种表示法称为“冒号十六进制表示法”。x:x:x:x:x:x:x:x零压缩法双冒号在一个地址中只能出现一次，如何确定双冒号之间被压缩0的位数？可以数一下地址中还有多少个位段，然后用8减去这个数，再将结果乘以16。例如，在地址FF02:3::5中有三个位段，可以根据公式计算：(8-3)*16=80,则 ::表示有80位的二进制数字0被压缩。2]]></content>
      <categories>
        <category>复习</category>
      </categories>
      <tags>
        <tag>复习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[事务]]></title>
    <url>%2F2019%2F09%2F10%2F%E4%BA%8B%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[事务的概念事务就是一组原子性的sql,或者说一个独立的工作单元。事务就是说，要么MySql引擎会全部执行这一组SQL语句，要么全部都不执行。 事务的ACIDAtomicity，原子性：一个事务必须保证其中的操作要么全部执行，要么全部回滚，不可能存在只执行了一部分这种情况出现。Consistency，一致性：事务必须保证从一种一致性状态转换为另一种一致性状态。Isolation,隔离性：事务互相隔离，在一个事务未执行完毕时，通常会保证其他Session无法看到这个事务的执行结果。Durability，持久性：事务一旦commit，则数据就会保存下来，即使提交完之后系统崩溃，数据也不会丢失。 隔离级别以下，我们来详细来说一说隔离性我们都知道，事务控制的太严格，程序在并发訪问的情况下，会减少程序的性能。所以。人们总是想让事务为性能做出让步。那么就分出了四种隔离级别：为未提交读、提交读、可重复度读、序列化。可是。因为隔离界别限制的程度不同，那么就会产生脏读、不可反复读、幻读的情况。 脏读：脏读就是指当一个事务正在訪问数据，而且对数据进行了改动，而这样的改动还没有提交到数据库中，这时。另外一个事务也訪问这个数据，然后使用了这个数据。 不可反复读：是指在一个事务内，多次读同一数据。在这个事务还没有结束时。另外一个事务也訪问该同一数据。那么，在第一个事务中的两次读数据之间，因为第二个事务的改动，那么第一个事务两次读到的的数据可能是不一样的。这样就发生了在一个事务内两次读到的数据是不一样的，因此称为是不可反复读。比如。一个编辑人员两次读取同一文档，但在两次读取之间，作者重写了该文档。当编辑人员第二次读取文档时，文档已更改。原始读取不可反复。假设仅仅有在作者所有完毕编写后编辑人员才干够读取文档，则能够避免该问题。 幻读:是指当事务不是独立运行时发生的一种现象，比如第一个事务对一个表中的数据进行了改动，这样的改动涉及到表中的所有数据行。同一时候。第二个事务也改动这个表中的数据。这样的改动是向表中插入一行新数据。那么，以后就会发生操作第一个事务的用户发现表中还有没有改动的数据行，就好象发生了幻觉一样。比如。一个编辑人员更改作者提交的文档。但当生产部门将其更改内容合并到该文档的主复本时，发现作者已将未编辑的新材料加入到该文档中。事务并发下隔离级别的场景 未提交读公司发工资了，领导把5000元打到singo的账号上，但是该事务并未提交，而singo正好去查看账户，发现工资已经到账，是5000元整。很高兴。但是不幸的是。领导发现发给singo的工资金额不正确。是2000元。于是迅速回滚了事务，改动金额后，将事务提交，最后singo实际的工资仅仅有2000元，singo空欢喜一场。出现上述情况，即我们所说的脏读。两个并发的事务，“事务A：领导给singo发工资”、“事务B：singo查询工资账户”，事务B读取了事务A尚未提交的数据。当隔离级别设置为Readuncommitted时，就可能出现脏读，怎样避免脏读。请看下一个隔离级别。 读提交singo拿着工资卡去消费。系统读取到卡里确实有2000元。而此时她的老婆也正好在网上转账，把singo工资卡的2000元转到还有一账户，并在singo之前提交了事务，当singo扣款时，系统检查到singo的工资卡已经没有钱，扣款失败，singo十分纳闷，明明卡里有钱，为何……出现上述情况，即我们所说的不可反复读。两个并发的事务，“事务A：singo消费”、“事务B：singo的老婆网上转账”。事务A事先读取了数据。事务B紧接了更新了数据，并提交了事务，而事务A再次读取该数据时，数据已经发生了改变。当隔离级别设置为Readcommitted时，避免了脏读。可是可能会造成不可反复读。大多数数据库的默认级别就是Readcommitted。比方Sql Server , Oracle。怎样解决不可反复读这一问题。请看下一个隔离级别。 反复读当隔离级别设置为Repeatableread时。能够避免不可反复读。当singo拿着工资卡去消费时。一旦系统開始读取工资卡信息（即事务開始）。singo的老婆就不可能对该记录进行改动，也就是singo的老婆不能在此时转账。尽管Repeatableread避免了不可反复读，但还有可能出现幻读。singo的老婆工作在银行部门。她时常通过银行内部系统查看singo的信用卡消费记录。有一天，她正在查询到singo当月信用卡的总消费金额（select sum(amount) from transaction where month =本月）为80元。而singo此时正好在外面胡吃海塞后在收银台买单。消费1000元，即新增了一条1000元的消费记录（insert transaction… ）。并提交了事务。随后singo的老婆将singo当月信用卡消费的明细打印到A4纸上。却发现消费总额为1080元，singo的老婆非常诧异，以为出现了幻觉。幻读就这样产生了。注：Mysql的默认隔离级别就是Repeatableread。 序列化Serializable是最高的事务隔离级别。同一时候代价也花费最高，性能非常低，一般非常少使用，在该级别下，事务顺序运行，不仅能够避免脏读、不可反复读，还避免了幻像读。mysql开启事务默认情况下，mysql每执行一条sql语句都是一条事务，自动提交。如果一条事务有多条sql语句需要执行，要开启一个新的事务12345678910开启事务start transaction....结束事务commit/rollback或 通过AutoCommit设置事务开启或关闭show variables like "autocommit"set autocommit = 0; //0表示AutoCommit关闭set autocommit = 1； //1表示AutoCommit开启 在执行SQL语句之前，先执行start transaction，这就开启了一个事务（事务的起点），然后可以去执行多条SQL语句，最后要结束事务，commit表示提交，即事务中的多条SQL语句所作出的影响会持久到数据库中，或者rollback，表示回滚到事务的起点，之前做的所有操作都被撤销了。 jdbc 开启事务默认自动提交事务，若要手动提交，则关闭事务。在JDBC中处理事务，都是通过Connection完成的。同一事务中所有的操作，都在使用同一个Connection对象。1234con.setAutoCommit(false) 表示开启事务。...(进行多条sql)commit（）：提交结束事务。rollback（）：回滚结束事务。 附上jdbc代码练习12345678910111213141516171819202122232425262728public static void main(String[] args) throws ClassNotFoundException &#123; Connection conn= null; String driver = "com.mysql.jdbc.Driver"; //时区问题，serverTimezone=UTC String url = "jdbc:mysql://localhost:3306/czt?characterEncoding=utf-8&amp;useSSL=false&amp;serverTimezone=UTC"; String name = "root"; String password = "666666"; //先获取链接驱动 Class.forName(driver); String sql =null; try &#123; //通过driverManager获取链接 conn = DriverManager.getConnection(url, name, password); if (conn != null )&#123; System.out.println("成功连接"); &#125; //通过链接，发送statement preperStatement的区别是，可以预编译sql语句，再注入参数， Statement statement = conn.createStatement(); sql = "create table student(NO char(20),name varchar(20),primary key(NO))"; //返回影响行数 int i = statement.executeUpdate(sql); if (i!=-1)&#123; System.out.println("修改成功"); &#125; conn.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; spring 开启事务spring 事务管理详解在xml配置文件中第一步：配置事务管理器第二步：开启事务注解 12345678910&lt;!-- 第一步：配置事务管理器 (和配置文件方式一样)--&gt; &lt;bean id="dataSourceTransactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager"&gt; &lt;!-- 注入dataSource --&gt; &lt;property name="dataSource" ref="dataSource"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 第二步： 开启事务注解 --&gt; &lt;tx:annotation-driven transaction-manager="dataSourceTransactionManager" /&gt; &lt;!-- 第三步 在方法所在类上加注解 --&gt;]]></content>
      <categories>
        <category>事务</category>
      </categories>
      <tags>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[位运算符]]></title>
    <url>%2F2019%2F09%2F07%2F%E4%BD%8D%E8%BF%90%E7%AE%97%E7%AC%A6%2F</url>
    <content type="text"><![CDATA[二进制的正负二进制的正负是从高位看，最高位如果1则是负数，如果是0则是正数。如果负数单纯是把最高位变为1的话，在运算中会出现不是我们想要的值，所以引入了：原码，反码，补码。正数的原码，反码，补码都一样，负数的反码是对除了符号位（最高位）对原码取反，补码是对反码+1（最高位不变）-5的原码是 1000 0000 0000 0101求出反码的是 1111 1111 1111 1010求出补码是 1111 1111 1111 1011 JAVA按位运算符1、“与”、“位与”（&amp;）按位“与”操作符，如果两个数的二进制，相同位数都是1，则该位结果是1，否则是0.例1 5&amp;45的二进制是 0000 0000 0000 01014的二进制是 0000 0000 0000 0100则结果是 0000 0000 0000 0100 转为十进制是4。2、“或”、“位或”（|）按位“或”操作符，如果两个数的二进制，相同位数有一个是1，则该位结果是1，否则是0例2 5 | 45的二进制是 0000 0000 0000 01014的二进制是 0000 0000 0000 0100则结果是 0000 0000 0000 0101 转为十进制是5。3、“异或、“位异或”（^）按位“异或”操作符，如果两个数的二进制，相同位数只有一个是1，则该位结果是1，否则是0（相同为0，不同为1） 例3 5 ^ 45的二进制是 0000 0000 0000 01014的二进制是 0000 0000 0000 0100则结果是 0000 0000 0000 0001 转为十进制是14、“非”、“位非”（~）也称为取反操作符按位“非”操作符，属于一元操作符，只对一个操作数进行操作，（其他按位操作符是二元操作符）。按位“非”生成与输入位相反的值，——若输入0，则输出1，若输入1，则输出0。例4 ~55的二进制是 0000 0000 0000 0101则~5是 1111 1111 1111 1010 转为十进制是 -6。 JAVA位运算符移位操作符操作的运算对象也是二进制的“位” 移位操作符只可用来处理整数类型，左移位操作符（&lt;&lt;）能按照操作符右侧指定的位数将操作符左边的操作数向左移动（在低位补0） 有符号”右移位操作符（&gt;&gt;）则按照操作符右侧指定的位数将操作符左边的操作数向右移。“有符号”右移位操作符使用“符号扩展”；若符号位正，则在高位插入0；若符号位负。则在高位插入1。 java中增加了一种“无符号”右移位操作符（&gt;&gt;&gt;）,他使用“零扩展”；无论正负，都在高位插入0。这一操作符是C或C++中所没有的。例6 5&lt;&gt;2 等于 15的二进制是 0000 0000 0000 0101右移两位 0000 0000 0000 0001例8 -5&gt;&gt;2 等于 -2 计算机中对于负数的计算都是通过补码。-5的二进制是 1111 1111 1111 1011右移两位 1111 1111 1111 1110 转十进制，例5反着来，先-1，然后取反 ————————————————版权声明：本文为CSDN博主「mxiaoyem」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/mxiaoyem/article/details/78569782 JAVA 基础数据类型的转换java语言提供了八种基本类型。六种数字类型（四个整数型，两个浮点型），一种字符类型，还有一种布尔型。自动转换按从低到高的顺序转换。int —-&gt; long —-&gt;float —-&gt; double int：int 数据类型是32位、有符号的以二进制补码表示的整数；最小值是 -2,147,483,648（-2^31）；最大值是 2,147,483,647（2^31 - 1）；一般地整型变量默认为 int 类型；默认值是 0 ；例子：int a = 100000, int b = -200000。 long：long 数据类型是 64 位、有符号的以二进制补码表示的整数；最小值是 -9,223,372,036,854,775,808（-2^63）；最大值是 9,223,372,036,854,775,807（2^63 -1）；这种类型主要使用在需要比较大整数的系统上；默认值是 0L；例子： long a = 100000L，Long b = -200000L。“L”理论上不分大小写，但是若写成”l”容易与数字”1”混淆，不容易分辩。所以最好大写。 float：float 数据类型是单精度、32位、符合IEEE 754标准的浮点数；float 在储存大型浮点数组的时候可节省内存空间；默认值是 0.0f；浮点数不能用来表示精确的值，如货币；例子：float f1 = 234.5f。 double：double 数据类型是双精度、64 位、符合IEEE 754标准的浮点数；浮点数的默认类型为double类型；double类型同样不能表示精确的值，如货币；默认值是 0.0d；例子：double d1 = 123.4。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发艺术编程]]></title>
    <url>%2F2019%2F09%2F03%2F%E5%B9%B6%E5%8F%91%E8%89%BA%E6%9C%AF%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[https://juejin.im/post/5aeed586f265da0b8262b019 并发专题 12AQS队列同步器 依赖内部的同步队列（一个FIFO双向队列）来完成同步状态的管理。当前线程获取同步状态失败时，同步器会将当前线程以及等待状态信息构造成一个节点（Node）并将其加入同步队列，同时会阻塞当前线程，当同步状态释放时，会把首节点中的线程唤醒，使其再次尝试获取同步状态。AQS独占锁：当一个线程获取同步状态时，如果获取失败，会生成一个节点，并加入同步队列尾部，只有当前驱节点是头节点才能够尝试获取同步状态，所以会陷入一个自旋操作，进入等待状态，当线程被中断或前驱节点被释放，则判断前驱节点是否为头节点，如果是，则尝试获取同步状态，获取成功，当前节点是指为头节点，获取失败，继续进入等待状态。 公平锁每次都是从同步队列中的第一个节点获取到锁，而非公平锁出现了一个线程连续获取锁的情况。 共享锁的调用框架和独占锁很相似，它们最大的不同在于获取锁的逻辑——共享锁可以被多个线程同时持有，而独占锁同一时刻只能被一个线程持有。 由于共享锁同一时刻可以被多个线程持有，因此当头节点获取到共享锁时，可以立即唤醒后继节点来争锁，而不必等到释放锁的时候。因此，共享锁触发唤醒后继节点的行为可能有两处，一处在当前节点成功获得共享锁后，一处在当前节点释放共享锁后。 在非公平锁的实现中，当一个线程请求锁时，只要获取了同步状态即成功获取锁。在这个前提下，刚释放锁的线程再次获取同步状态的几率会非常大，使得其他线程只能在同步队列中等待。 共享锁https://segmentfault.com/a/1190000016447307 启动一个java程序，操作系统就会创建一个进程，一个进程可以创建多个线程，这些线程都用友各自的计数器，堆栈和局部变量等属性，访问共享的内存变量。JAVA程序天生就是多线程程序，执行main（）方法的是一个名为main的线程** 为什么要多线程 更多的处理器核心 更多的响应时间 更好的编程模型线程优先级通过一个整型变量priority控制优先级，thread.setPriority(priority)。 默认优先级 ： 5 范围从 1-10，优先级逐级增高 对于优先级的选择 对于频繁阻塞的任务（休眠 或 I/O操作） ，优先级应该高。对于偏重计算的任务（需要较多的CPU） , 优先级应该低，确保处理器不会被独占注意 ：线程优先级不能作为程序正确性的依赖，因为操作系统可以完成不用理会JAVA线程对于优先级的设定，对线程优先级的设置会被忽略。线程中断中断好比其他线程对该线程打了个招呼，其他线程通过调用该线程的interrupt()方法对其进行了中断操作。等待/通知机制等待/通知机制，是指一个线程A调用了对象O的wait（）方法进入等待状态，而另一个线程B调用了对象O的notify（）或者notifyAll（）方法，线程A收到通知后从对象O的wait（）方法返回，进而执行后续操作。notify（）：通知一个在对象上等待的线程，使其从wait()返回，前提是获得对象的锁notifyAll（）：通知所有等待在该对象上的线程wait（）：调用该方法的线程进入waiting状态，只有等待另外线程的通知或中断才会返回，需要注意，调用wait（）方法后，会返回对象的锁。 使用wait() , notify() 和 notify() 时需要先对调用对象加锁。 notify（）或notifyAll（）方法调用后，等待线程依旧不会从wait返回，需要调用notify（）或notifyAll（）的线程释放锁后，才有机会。 notify()方法将等待队列中的一个等待线程从等待队列中移到同步队列中，而notifyAll（）方法则是将等待队列中所有线程全部移到同步队列中，被移动的线程状态由waiting变为blocked 从wait（）方法返回的前提是获得了调用对象的锁。volatile关键字 —最轻量级的同步机制当一个变量被定义为volatile，两种特性 保证此变量对所有线程的可见性“可见性” ：当一个线程修改此变量，其他线程是立即得知新值。但由于JAVA里面的运算并非原子操作，导致volatile变量在并发下也并不安全。（例如 a++）。具有可见性和原子性，但类似于volatile++这种复合操作不具有原子性。通过字节码可以知道，其指令多了一个Lock前缀，使得本CPU的cache写入内存，该写入使得别的CPU无效其Cache，会重新从内存中读取，保证可见性。 禁止指令重排序优化（会干扰并发）保证变量赋值操作的顺序与程序代码中的执行顺序一致。通过内存屏障，使重排序时后面的指令不能比屏障之前的先执行。意味着所有之前的操作都已经执行完成。重排序：编译器和处理器为了优化程序性能而对指令序列进行重新排序的一种手段。Synchronized关键字，最基本的互斥同步手段Synchronize经过编译后，会在同步块前后分别形成两个monitorenter，monitorexit两个字节码指令。工作原理 –&gt;monitorenter –&gt;尝试获取对象的锁 —&gt;失败，线程阻塞—- &gt;成功，对象没被锁定，当前线程拥有对象的锁，锁的计数器加一— &gt; monitorexit —&gt;锁计数器减一 ，当计数器为0，锁就被释放 —&gt;唤醒被阻塞的线程锁释放 - 获取的内存语义与volatile写 - 读的内存语义是相同的 线程A释放一个锁（写一个volatile变量），实质上线程A向接下来将要获取这个锁的某个线程发出了（线程A对共享变量所做修改的）消息。 线程B获取了一个锁（读一个volatile变量），实质上是线程B接收了之前某个线程发出的（在释放之前对共享变量所做修改的）消息。 线程A释放锁，随后线程B获取锁，这个过程实质上是线程A通过主内存向线程B发送消息。实现多线程的几种方法 继承Thread类通过JDK提供的Thread类，重写Thread类run方法即可 123class Thread1 extends Thread&#123;...&#125;//启动 new Thread1().start; 实现Runnable接口 Runnable接口中仅定义一个run（）方法 1234class Thread2 implements Runnable&#123; @Override public void run()&#123;...&#125;&#125; 使用内部类的方式上面两种方式都需要再定义一个类，显得麻烦，通过匿名内部类实现，依然有两种 继承Thread a 实现 Runnable b 1234new Thread(new Runnable()&#123; @Override public void run()&#123;&#125;&#125;).start; 带返回值的callable实现callable接口 基于线程池的方式Thread.join()若一个线程A执行了thread.join()，含义是当前线程A等待Thread线程终止后才从join方法返回。锁锁的内部实现依赖于队列同步器同步器底层通过一个双向队列，存放工作队列，当一个线程成功获取同步状态，其他线程将无法获取到同步状态，转而被构造成为节点并加入同步队列中。可重入锁 自己可以再次获取自己的内部锁，任意线程在获取到锁之后能够再次获取该锁而不会被该锁所阻塞。Synchronize 和 基于Lock 实现的ReentrantLock都是可重入锁。 可重入锁 有两个特性 线程再次获取锁锁需要识别获取锁的线程是否是当前占据锁的线程，如果是，可再成功获取 锁的最终释放重复n次获取锁，在第n次释放锁后，其他线程才能获取到该锁 读写锁维护一对锁，一个读锁，一个写锁。通过分离读锁，写锁提高效率。JAVA并发包提供读写锁的实现是 ReetrantReadWriteLock。 123ReetrantReadWriteLock rwl = new ReetrantReadWriteLock();Lock r = rwl.readLock(); // 读锁Lock w = rwl.writeLock(); // 写锁 公平性选择 默认是非公平性 重进入锁 锁降级 先拿读锁（保证可见性） 放读锁，拿写锁，再拿读锁。读写锁的实现是通过int变量维护多种状态，读写锁将变量切分，高16位表示读，低16位表示写。ReentrantLock ***重点 通过调用Lock（）方法获取锁，unlock（）释放锁 12ReentrantLock lock = new ReentrantLock();lock.lock() ; lock.unlock(); 实现依赖于AQS框架，使用一个整型的volatile变量（state）来维持同步 **重点 以ReentrantLock为例，state初始化为0，表示未锁定状态。A线程lock()时，会调用tryAcquire()独占该锁并将state+1。此后，其他线程再tryAcquire()时就会失败，直到A线程unlock()到state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A线程自己是可以重复获取此锁的（state会累加），这就是可重入的概念。但要注意，获取多少次就要释放多么次，这样才能保证state是能回到零态的。 123456789101112131415161718final boolean nonfairTryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; if (compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error("Maximum lock count exceeded"); setState(nextc); return true; &#125; return false; &#125; 公平锁的实现 *重点happens-before程序顺序规则是JMM核心的概念JMM通过happens-before来指定两个操作之间的执行顺序定义： ①如果一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个可见，且第一个操作的执行顺序排在第二个之前 ———————-对程序员可见②如果一个操作happens-before另一个操作，并不意味着必须按照happens-before关系指定的顺序执行，如果重排序之后的执行结果，与按happens-before顺序结果一致，那么这种重排序并不违法。 ————————对编译器和处理器的约束原则CAS 比较与替换是设计并发算法时用到的技术CAS是使用一个期望值和一个变量的当前值进行比较，如果当前变量的值与我们期望的值相等，就使用一个新值替换当前变量的值1234567public class MyLock&#123; private AtomicBoolean locked = nnew AtomicBoolean(false); public boolean lock()&#123; return locked.compareAndSet(false,true); &#125; //比较locked 和 false ，如果相等，则把它修改成true&#125; ConcurrentHashMap 是线程安全且高效的HashMapConcurrentHashMap的锁分段技术可有效提升并发访问率 与HashTable容器相比，HashTable容器并发环境效率低下是所有访问HashTable的线程都必须访问同一把锁。假如容器中有很多锁，每把锁用于锁容器其中一部分数据，那么当多线程访问容器里不同数据段的数据时，线程间就不存在竞争。这就是ConcurrentHashMap的锁分段技术。 ConcurrentHashMap的结构是有Segment数组结构 和 HashEntry 数组结构组成 Segment是一种可重入锁，结构和HahMap类似，是一种数组和链表结构。 一个ConcurrentHashMap里包含一个Segment数组，扮演锁的角色。 每个HashEntry是一个链表结构的元素，一个Segment里包含一个HashEntry数组。 每个Segment守护着一个HashEntry数组里的元素，当对HashEntry数组的数据进行修改时，必须首先获得与他对应的Segment锁。get操作 在定位Segment时，都会对元素的hashcode进行一次再散列。在HashTable中，get方法是需要加锁的，但在ConcurrentHashMap中的get操作是不用加锁的。原因： 在它的get方法中将要使用的共享变量都定义成volatile类型。能够在线程之间保持可见性，能够被多线程读，但只能被单线程写（有种情况可被多线程写，就是写入的值不依赖原值）。就像有多线程写， 也能get（）到最新的值。根据JMM的happens-before规则，对volatile的写入操作时优先于读操作的。这是用volatile替换锁的经典场景。put操作put方法首先定位到Segment,然后在Segment里进入插入操作。ConcurrentHashMap的扩容首先会创建一个容量是原来容量两倍的数组，然后将原来的元素再散列插入到新数组中。为了高校，ConcurrentHashMap不会对整个容器进行扩容，而只对某个Segment进行扩容。比HashMap更高校，HashMap是在插入后进行判断是否需要扩容， 但扩容后可能就不插入新元素。浪费了扩容的其他空间。阻塞队列（BlockingQueue）是一个支持两个附加操作的队列，这两个附加的操作支持阻塞的插入和移除 支持阻塞的插入方法，意思是当队列满时，队列会阻塞插入元素的线程，直到队列不满。 支持阻塞的移除方法，当队列为空时，会阻塞移除元素的线程，等待队列不为空。并发工具类等待多线程完成的CountDownLatchCountDownLatch允许一个或多个线程等待其他线程完成操作。假如需求，解析一个Excel里多个sheet的数据，每个线程解析一个sheet，等所有的sheet都解析完，程序需要提示解析完成。实现主线程等待所有线程完成sheet的解析，最简单是使用join（）方法1234Thread1.start();Thread2.start();Thread1.join(); //主线程需等待join线程执行结束。THread2.join(); CountDownLatch c = new CountDownLatch(2);CountDownLatch接收一个int类型的参数作为计数器，如果想等待N个点完成，这里就传入N。调用c.countDown()方法，N就会-1,c.await()方法会阻塞当前线程，直到N变成0； 同步屏障 CyclicBarrier让一组线程到达一个屏障（同步点）时被阻塞，直至最后一个屏障到达才会开门，所有被屏障拦截的线程才会继续运行。每个线程调用await（）告诉其已到屏障。new CyclicBarrier（2）; 如果修改成3，但只有两个线程调用await（），主线程和子线程和永远等待，因为没有第3个线程执行await方法，即没有第3个线程到达屏障，所以之前到达屏障的线程都不会执行。 CountDownLatch 和 CyclicBarrier的区别CountDownLatch的计数器只能使用一次， CyclicBarrier的计数器可以使用reset（）重置。 3. CountDownLatch与CyclicBarrier的比较CountDownLatch与CyclicBarrier都是用于控制并发的工具类，都可以理解成维护的就是一个计数器，但是这两者还是各有不同侧重点的： CountDownLatch一般用于某个线程A等待若干个其他线程执行完任务之后，它才执行；而CyclicBarrier一般用于一组线程互相等待至某个状态，然后这一组线程再同时执行；CountDownLatch强调一个线程等多个线程完成某件事情。CyclicBarrier是多个线程互等，等大家都完成，再携手共进。 调用CountDownLatch的countDown方法后，当前线程并不会阻塞，会继续往下执行；而调用CyclicBarrier的await方法，会阻塞当前线程，直到CyclicBarrier指定的线程全部都到达了指定点的时候，才能继续往下执行； CountDownLatch方法比较少，操作比较简单，而CyclicBarrier提供的方法更多，比如能够通过getNumberWaiting()，isBroken()这些方法获取当前多个线程的状态，并且CyclicBarrier的构造方法可以传入barrierAction，指定当所有线程都到达时执行的业务功能； CountDownLatch是不能复用的，而CyclicLatch是可以复用的。 ## 线程池判断核心线程池里的线程是否都在执行任务（运行的线程少于corePoolSize，核心线程池还能添加工作线程），则创建新的工作线程来执行任务。（线程池创建线程时，会将线程封装成工作线程worker，Worker在执行任务后，还会循环获取工作队列里的任务来执行）若工作线程都在执行任务，且核心线程池已满，则进入下个流程。注意：创建新线程这一步需要获取全局锁，消耗资源。 线程池判断能否将任务加入工作队列，可以则加入工作队列FIFO，不可以则创建新线程。 如果创建新线程将使当前运行的线程超过MaxnumPoolSize（线程池大小），则交由饱和策略。线程池的创建 通过ThreadPoolExecutor来创建new ThreadPoolExecutor（corePoolSize–线程池基本大小，MaxnumPoolSize—线程池最大数量，keepAliveTime，millsecond，runnablequeue—保存任务的阻塞队列，hanlder—饱和策略）向线程池提交任务 threadPool.execute (new Runnable(){…})execute()方法用于提交不需要返回值的任务。（无法判断任务是否成功） Future(Object) future = threadPool.submit(new Runnable{})submit()方法用于提交需要返回值的任务，会返回一个future对象，并且可以通过future的get（）方法来获取返回值，get（）会阻塞当前线程直到任务完成。关闭线程池threadPool.shutdown();原理：遍历工作线程，逐个调用线程的interrupt方法中断线程。]]></content>
      <categories>
        <category>并发</category>
      </categories>
      <tags>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序]]></title>
    <url>%2F2019%2F09%2F02%2F%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[排序的稳定性假设在排序前的序列中ri 领先于rj(即i&lt;j)。如果排序后ri仍领先于rj，则称所用的排序方法是稳定的；反之，若可能使得排序后的序列中rj领先ri,则称所用的排序方法是不稳定的。 冒泡排序冒泡排序一种交换排序，它的基本思想是：两两比较相邻记录的关键字，如果反序则交换，直到没有反序的记录为止。1234567891011//对顺序表做冒泡排序void BubbleSort(SqList list)&#123; int i,j; for (i = 1;i&lt;list.length;i++)&#123; for (j = list.length-1;j&gt;=i;j--)&#123; //注意j是从后往前递减 if(list[j-1]&gt;list[j])&#123; //若前者大于后者 swap(list,j-1,j); //交换前者和后者的位置 &#125; &#125; &#125;&#125; 较小的数字如同气泡般慢慢浮到上面，因此将此算法命名为冒泡排序。 冒泡排序优化如果待排序的序列是{2,1,3,4,5,6,7,8}，也就是说，除了第一和第二的关键字需要交换外，别的已经是正常的顺序。当i=1时，交换了2和1，此时序列已将有序，但是算法仍然将i=2到8都执行了一遍，尽管没有交换数据，但是之后的大量比较大大的多余。当i=2时，我们已经对9与8,8与7……3与2作了比较，没有任何数据交换，这就说明此序列已将有序，不需要再继续后面的循环判断工作。为了实现这个想法，需要改进代码，增加一个标记变量flag来实现算法的改进。12345678910111213void BubbleSort2(SqList list)&#123; int i,j; Boolean flag = false; //flag用来做标记 for (i = 1;i&lt;list.length &amp;&amp; flag;i++)&#123; //若flag为false则退出循环 flag = false; //初始flag为false for (j = list.length-1;j&gt;=i;j--)&#123; if(list[j-1]&gt;list[j])&#123; swap(list,j-1,j); flag = true; //若发生交换，flag=true &#125; &#125; &#125;&#125; 复杂度分析最好的情况，排序的表本身就是有序的，根据最后改进的代码，可以推断出就是n-1次的比较，没有数据交换，时间复杂度是O(n)。最坏的情况，即排序表时逆序的情况，此时需要比较1+2+3+4+5…+（n-1）= n(n-1)/2次，因此，总的时间复杂度为O(n2)。 简单选择排序选择排序法的初步思想，冒泡排序的思想是不断的交换，通过交换完成最终的排序，我们可以在排序时找到合适的关键字再做交换，并且只移动一次就完成相应关键字的排序定位工作。简单选择排序法就是通过n-i次关键字间的比较，从n-i+1个记录中选出关键字最小的记录，并和第i个记录交换之。123456789101112131415/*对顺序表L作简单选择排序*/void SelectSort(SqList list)&#123; int i,j,min; for (i=1;i&lt;list.length;i++)&#123; min = i; //将当前下标定义为最小值下标 for(j=i+1;j&lt;list.length;j++)&#123; if (list[min] &gt; list[j])&#123; //从n-i+1中选出关键字最小的记录 min = j; //将此关键字的下标赋值给min &#125; &#125; if (min != i)&#123; //若min不等于i,找到最小值，交换 swap(list,i,min); //交换list[i]和list[min]的值 &#125; &#125;&#125; 简单选择排序复杂度分析从过程来看，它最大的特点就是交换移动数据次数相当少，这样也就节约了相应的时间。分析复杂度发现，无论最好最坏情况，其比较次数都是一样的多，第i趟排序需要进行n-i次关键字的比较，此时需要比较n-1+n-2+…=1 = n(n-1)/2次。因此，总的时间复杂度依然是O(n2).应该说，尽管与冒泡排序同为O(n2),但简单选择排序的性能上还是要略优于冒泡排序。 直接插入排序直接插入排序的基本操作是将一个记录插入到已经排好序的有序表中，从而得到一个新的，记录数增1的有序表。12345678910111213void InsertSort(SqList list)&#123; int i,j; //0的位置当成哨兵，假设list[1]已经排好位置，后面的牌其实就是插入到它的左侧还是右侧的问题 for (i = 2,j&lt;=list.length;i++)&#123; if (list[i] &lt; list[i-1])&#123; //升序，需将list[i]插入有序子表 list[0] = list[i]; //设置哨兵 for (j=i-1;list[j]&gt;list[0];j--) &#123; list[j+1] = list[j]; //记录后移 &#125; list[j+1] = list[0]; //插入到正确位置 &#125; &#125;&#125; 直接插入排序时间复杂度分析最好的情况，也就是要排序的表本身就是有序的，那么比较次数，其实就是代码第6行每个list[i]与list[i-1]的比较。时间复杂度为O(n)。最坏的情况，2+3+4+…+n=(n+2)(n-1)/2,直接插入排序的时间复杂度为O(n2),同样的O(n2)时间复杂度，直接插入排序法比冒泡和简单选择排序的性能要好一些。 希尔排序在这之前排序算法的时间复杂度基本都是O(n2)，希尔排序算法是突破这个时间复杂度的第一批算法之一。在直接插入排序的基础上进行改进。将原本有大量记录数的记录进行分组，分割成若干序列，采取跳跃分割的策略：将相距某个“增量”的记录组成一个子序列，这样才能保证在子序列内分别进行直接插入排序后得到的结果是基本有序而不是局部有序。12345678910111213141516171819//对顺序表L做希尔排序//将关键字较小的记录，不是一步一步地往前挪动，而是跳跃式地往前移，使得每完成一轮循环后，整个序列就朝着有序坚持地迈进了一步void SheelSort(SqList list)&#123; int i,j; int increment = list.length(); do &#123; increment = increment/3 +1; //增量序列 for(i=increment+1;i&lt;=list.length();i++)&#123; if (list[i] &lt; list[i-increment])&#123; //跳跃判断 list[0] = list[i]; //暂存在list[0] for (j = i-increment;j&gt;0&amp;&amp;list[0]&lt;list[i];j-=increment)&#123; list[j+increment] = list[j]; //记录后移，查找插入位置 &#125; list[j+increment] = list[0]; //插入 &#125; &#125; &#125;while(increment&gt;1); //当增量为1时，就停止循环&#125; 希尔排序复杂度分析希尔排序的关键并不是随便分组后各自排序，而是将相隔某个“增量”的记录组成一个子序列，实现跳跃式的移动，使得排序的效率提高。其时间复杂度为O(n3/2)，要好于直接排序的O(n2)，需要注意的是，增量序列的最后一个增量值必须等于1才行，另外记录是跳跃式的移动，希尔排序并不是一种稳定的排序算法。 堆排序前面讲到简单选择排序，他在待排序的n个记录中选择一个最小的记录需要比较n-1次，本来可以理解，查找第一个数据需要比较这么多次是正常的，可惜的是，这样的操作并没有把每一趟的比较结果保存下来，在后一趟的比较中，有许多比较在前一趟已经做过了，但由于前一趟排序时未保存这些比较结果，所以后一趟排序时又重复执行了这些比较操作，因而记录的比较次数较多，如果可以做到每次在选择到最小记录的同时，并根据比较结果对其他记录做出相应的调整，那排序的总体效率就会非常高。而堆排序就是对简单排序进行的一种改进。同时，他们发明了“堆”这样的数据结构。 堆堆是具有下列性质的完成二叉树：每个结点的值都大于或等于其左右孩子结点的值，称为大顶堆。或者每个结点的值都小于或等于其左右孩子结点的值，称为小顶堆。根结点一定是堆中所有结点最大（小）者。如果按照层序遍历的方式给结点从1开始编号，则结点之间满足如下关系：完全二叉树的特性，下标i与2i和2i+1是双亲子女关系。核心 大顶堆：ki &gt;= k2i , ki &gt;= k2i+1 小顶堆： ki &lt;=k2i , ki&lt;=k2i+1 ( 1&lt;= i &lt;= n/2 )堆排序利用此规则，完成排序。 堆排序算法将要排序的数组，看成是按照层序遍历的完全二叉树。堆排序就是利用堆（假设利用大顶堆）进行排序的方法。它的基本思想是，将待排序的序列构造成一个大顶堆，此时整个序列的最大值就是堆顶的根结点。将它移走（其实就是将其与堆数组的末尾元素交换，此时末尾元素就是最大值），然后将剩余的n-1个序列重新构造成一个堆，这样就会得到n个元素的次大顶堆，如此反复执行，便能得到一个有序序列。12345678910111213141516171819//本函数调整list[s]的关键字，使list[s..m]成为一个大顶堆//s 根节点 ，m 数组长度void HeadAdjust(SqlList list,int s,int m)&#123; int temp,j; temp = list[s]; for (j=2*s;j&lt;=m;j*=2)&#123; //沿关键字较大的孩子结点向下筛选 //找出s结点下，最大的孩子结点。j为关键字中较大的记录的下标。 if (j&lt;m &amp;&amp; list[j]&lt;list[j+1])&#123; ++j; &#125; if (temp &gt;= list[j])&#123; break; //根节点是最大的值，不需要改变位置 &#125; list[s] =list[j]; //将根节点与孩子结点数值交换， //s替换为j,进入下一次循环，看是否需要与下一个根节点互换 s=j; &#125; list[s] = temp; //将孩子结点换成根节点的数值&#125; 堆排序的核心算法已经有了，接下来就是对数组进行排序。12345678910void HeapSort(SqList list)&#123; int i; for (i = list.length()/2;i&gt;0;i--)&#123; ** HeadAdjust(list,i,list.length()); //把list构建成一个大顶堆 &#125; for (i = list.length() ;i&gt;1;i--)&#123; swap(list,1,i); //将堆顶记录和当前子序列的最后一个记录交换。 HeadAdjust(list,1,i-1); //将list[1..i-1]重新调整为大顶堆 &#125;&#125; 假设list长度是9，从4往下递减，是因为都是有孩子的结点。我们所谓的将待排序的序列构建成为一个大顶堆，其实就是从下往上，从右往左，将每个非终端结点（非叶结点）当成根节点，将其和其子树调整成大顶堆。 堆排序的时间复杂度堆排序的运行时间主要是消耗在初始构建堆和重建堆时的反复筛选上。在构建堆的过程中，因为我们是完成二叉树从最下层最右边的非终端结点开始构建将它与其孩子进行和若有必要的互换，对于每个非终端结点来说，其实最多进行两次比较和互换操作，因此整个构建堆的时间复杂度为O(n)。在正式排序时，第i次取堆顶记录重建堆需要用O(logi)的时间（完全二叉树的某个结点到根节点的距离为logi+1）,并且需要去n-1次堆顶记录，因此，重建堆的时间复杂度为O(nlogn)。所以，总体来说，堆排序的时间复杂度为O(nlogn). 归并排序前面我们讲 了堆排序，因为它用到了完全二叉树，充分利用了完全二叉树的深度是log2n + 1的特性，所以效率比较高。不过堆结构的设计本身是比较复杂的，有没有更直接简单的方法利用完成二叉树来排序。当然有。归并排序法涉及到完全二叉树结构的排序算法。归并排序就像是一颗倒置的完全二叉树 归并排序算法归并排序就是利用归并的思想实现的排序方法。他的原理是假设初始序列含有n个记录，则可以看成是n个有序的子序列，每个子序列的长度为1，然后两两归并，得到[n/2]个长度为2或1的有序子序列，再两两归并，…..,如此重复，直至得到一个长度为n的有序序列位置，这种排序方法称为2路归并排序。12345678910111213141516//对顺序表L作归并排序void MergeSort(Sqlist list)&#123; Msort(list,list,1,list.length());&#125;void Msort(int sr[],int tr1[],int s,int t)&#123; int m; int tr2[MAXSIZE+1]; if (s == t)&#123; //当细分到一个记录填入tr2后，此时s和t相等，递归返回 tr1[s] = sr[s]; &#125;else&#123; m = (s+t) /2; //将sr[s..t]平分为sr[s..m]和sr[m+1..t] 10 Msort(sr,tr2,s,m); //递归将sr[s..m]归并为有序的tr2[s..m] Msort(sr,tr2,m+1,t); //递归将sr[m+1..t]归并成有序的tr2[m+1..m] 12 Merge(tr2,tr1,s,m,t); //将tr2[s..m]和tr2[m+1..t],归并到tr1[s..t] &#125;&#125; 看第10行继续递归进去后，直到细分为一个记录填入tr2,此时s与t相等，递归返回，每次递归返回后都会执行当前递归函数的第12行，将tr2归并到tr1中，最终使得当前序列有序。现在我们来看看Merge函数的代码是如果实现的。123456789101112131415161718192021//sr[] 待归并的数组，tr[]归并排序后的数组，i=1，m正中间值,n数组长度void Merge(int sr[],int tr[],int i,int m,int n)&#123; int j,k,l; //k记录tr数组坐标。 j记录sr数组右半段数组下标 for (j = m+1,k=i;i&lt;=m &amp;&amp; j&lt;=n;k++)&#123; //将sr中记录由小到大归并到tr if (sr[i] &lt;sr[j])&#123; tr[k] = sr[i++]; //将前后两段数组进行对比，牛逼 &#125;else&#123; tr[k] = sr[j++]; &#125; &#125; if (i &lt;=m)&#123; //将没有归并到tr的sr[1..m]复制到tr for(l=0;l&lt;m-i;l++)&#123; tr[k+1]=sr[i+1]; &#125; &#125; if (j &lt;=n)&#123; //将没有归并到tr的sr[m+1..n]复制到tr for(l=0;l&lt;n-j;l++)&#123; tr[k+1]=sr[i+1]; &#125; &#125;&#125; 归并排序复杂度分析总的时间复杂度为O(nlogn),这是归并排序算法中最好，最坏，平均的时间性能。归并排序Merge函数中有if（sr[i]&lt;sr[j]）,这就说明它需要两两比较，不存在跳跃，因此归并排序是一种稳定的排序算法。 快速排序希尔排序相当于直接插入排序的升级，堆排序相当于简单选择排序的升级，它们同属于选择排序类，而快速排序其实就是最慢的冒泡排序的升级，都属于交换排序类，只不过它的实现，增大了记录的比较和移动的距离，将关键字较大的记录从前面直接移动到后面，关键字较小的记录从后面直接移动到前面，从而减小了总的比较次数和移动交换次数。 快速排序算法快速排序的基本思想是:通过一趟排序将待排序记录分割成独立的两部分，其中一部分记录的关键字均比另一部分的关键字小，则可分别对这两部分记录继续进行排序，以达到整个序列有序的目的。1234567891011121314151617181920212223242526//对顺序表L的子序列作快速排序void QSort(SqList list , int low ,int high)&#123; int pivot; if (low &lt; high)&#123; pivot = Partition(L,low,high); //将list一分为二 算出枢轴值pivot Qsort(list,low,pivot-1); //对低子表递归排序 Qsort(list,pivot+1,length); //对高子表递归排序 &#125;&#125;int Partition(SqList list,int low ,int high)&#123; int pivotkey; pivotkey = list[low]; //用子表的第一个记录做枢轴记录 while(low &lt; high)&#123; //将第一个记录作为枢轴，则一定要从后往前扫 while (low &lt;high &amp;&amp; list[high] &gt;=pivotkey)&#123; high --; &#125; swap(list,low,high); //将比枢轴小的记录交换到低端 while(low &gt;high &amp;&amp; list[low] &lt;=pivotkey)&#123; low ++; &#125; swap(list,low,high); //将比枢轴大的记录交换到高端 &#125; return low; //返回枢轴所在位置&#125; 快速排序时间复杂度分析在最优的情况下，快速排序算法的时间复杂度为O(nlogn)。最坏情况下，其时间复杂度是O(n*n)。快速排序是一种不稳定的排序方法。java对快速的实现 java.util.Arrays —-&gt; static void sort(type[] a)]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[递归]]></title>
    <url>%2F2019%2F08%2F21%2F%E9%80%92%E5%BD%92%2F</url>
    <content type="text"><![CDATA[http://lylblog.cn/blog/4]]></content>
  </entry>
  <entry>
    <title><![CDATA[react框架]]></title>
    <url>%2F2019%2F08%2F09%2Freact%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[通过react脚手架创建项目create-react-app [项目名] 启动脚手架npm start打包生产环境npm run build JSXJSX本身也是一种表达式 组件 组件是可以复用的UI元素]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>react</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前端学习网站]]></title>
    <url>%2F2019%2F08%2F08%2F%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%AB%99%2F</url>
    <content type="text"><![CDATA[React官方文档 https://reactjs.org/ECMAScript 6入门 http://es6.ruanyifeng.com/（特别是解构赋值、箭头函数和 Class）MDN - JavaScript https://developer.mozilla.org/zh-CN/docs/Web/JavaScript（JS基础语法）MDN - Array.prototype.map() https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/map（数组map方法） ES6入门javaScript JavaScript里一切皆对象，一切皆可储存在变量里。 let 声明变量。 浏览器出现Cannot set property ‘onclick’ of null的问题浏览器先加载玩按钮节点才执行的js ,所以当浏览器自顶向下解析时，找不到onclick绑定的按钮节点。因此，需要把js文件放在底部加载，就会避免该问题 ES6 中let 命令，用来声明变量，所声明的变量，只在let命令所在的代码块中有效。var命令声明的，在全局范围内都有效 1234567 for (let i = 0; i &lt; 3; i++) &#123; let i = &apos;abc&apos;; console.log(i);&#125;// abc// abc// abc 上面代码正确运行，输出了 3 次abc。let 命令在for循环中，函数内部变量的i,与循环变量i不在同一个作用域。 变量的结构赋值 1234567891011121314151617181920let [foo, [[bar], baz]] = [1, [[2], 3]];foo // 1bar // 2baz // 3let [ , , third] = [&quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;];third // &quot;baz&quot;let [x, , y] = [1, 2, 3];x // 1y // 3let [head, ...tail] = [1, 2, 3, 4];head // 1tail // [2, 3, 4]let [x, y, ...z] = [&apos;a&apos;];x // &quot;a&quot;y // undefinedz // [] 解构失败例子12let [foo] = [];let [bar, foo] = [1]; 等号左边的模式，只匹配一部分的等号右边的数组。这种情况下，解构依然可以成功。 12345678 let [x, y] = [1, 2, 3];x // 1y // 2let [a, [b], d] = [1, [2, 3], 4];a // 1b // 2d // 4]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>前端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据锁]]></title>
    <url>%2F2019%2F08%2F06%2F%E6%95%B0%E6%8D%AE%E9%94%81%2F</url>
    <content type="text"><![CDATA[解决并发问题，数据库常用的两把锁]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux]]></title>
    <url>%2F2019%2F08%2F05%2FLinux%2F</url>
    <content type="text"><![CDATA[1. vim编辑器一般模式：编辑模式：[i,0,a,r] , [ESC] 退出指令列命令模式 ：[: / ?]:wq 保存后退出:q! 强制退出不保存 Linux的分类：1.内核版本：Linux不是一个操作系统，严格来讲，只是一个操作系统的内核。内核建立计算机软件和硬件之间通讯的平台。2.发行版本：一些组织或公司在内核上进行二次开发的版本。Linux一个重要概念，一切都是文件。 Linux常用命令 mkdir：增加目录 rmdir：删除目录 ls 或 ll :( ll 是ls -l缩写 ll可看到该目录下所有目录和文件的详细信息 ) find 目录 参数 :寻找目录eg : find . -name ‘x.txt’ -o -name ‘x.pdf’当前目录及子目录下 所有以 .txt , .pdf结尾的文件 mv 目录名称 新目录名称 （改名）mv 目录名称 目录新位置 （剪切） cp -r 目录 目录新名字 （拷贝，-r递归拷贝） rm [-rf] 目录 ：删除目录tar -zcvf 压缩后的名字 要打包的文件-s 还原文件的顺序和备份文件内的存放顺序相同。-t 列出备份文件的内容。-v 显示指令执行过程。-f 指定压缩文件-x 从备份文件中还原文件。tar -xvf 压缩文件 -x 解压 Linux 查看负载 uptime 显示当前机器负载 w 列出所有user分别的情况 top wa超10%表示IO压力很大。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务架构]]></title>
    <url>%2F2019%2F08%2F02%2F%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[idea编码Enoding 记得设置UTF-8spring-boot如官网所说，帮我们构建一个spring项目，直接可以运行。微服务架构，将每一个模块均分成每一个项目。每个项目之间通过接口等，相互联系。将项目拆分几个独立的功能单元（服务） 的架构 优点 技术异构性：开发者可以自由的选择合理的技术和框架，只要服务遵守API协定即可。 弹性：将不同服务部署在不同机器上，降低整体功能不可用的概率 可扩展性：可以只对需要扩展的服务进行扩展 简化部署：各个服务模块福利部署，可以快速对特定代码进行更新。缺点 实现复杂度搞 测试微服务复杂。对于微服务的一个类似的测试则需要运行该服务以及依赖的服务，也可能会增加额外的沟通成本。 分割的数据库架构：对于微服务间的事务性操作，因为不同的微服务采用了不同的数据库，将无法利用数据库本身的事务机制保证一致性。 接口匹配问题。服务依赖于彼此间的接口进行通信。改变一个服务的接口会对其他服务造成影响。 部署复杂 运维复杂SpringCloudSpringCloud是基于SpringBoot提供了一套为微服务解决方案，包括服务注册与发现。 Eureka:服务发现/注册中心，用于定位服务。 Hystrix:熔断器，容错管理工具。 Zuul：是在云平台上提供动态路由，监控，弹性，安全等边缘服务的框架 Ribbon/Feign:客户端负载均衡1.1 微服务架构 - 常见微服务框架 Dubbo/Dubbox 一个分布性，高性能，透明化的RPC服务框架 阿里巴巴开发 ， 当当改良 基于RPC Spring Cloud Spring团队开发 基于RESTful1.2 微服务架构 - 通信方式 RPC Remote Procedure Call 支持RPC的微服务框架：Dubbo/Dubbox 基于TCP,平台有关 RESTful Representational State Transfer 支持RESTful 的微服务框架：Spring Cloud/Dubbox 基于HTTP，平台无关其他概念 — 分布式和集群 分布式 关注项目拆分（水平拆分，垂直拆分） 集群关注项目部署 1.3 主要组件Eureka：服务发现/注册中心，一个基于 REST 的服务，用于定位服务，以实现云端中间层服务发现和故障转移Spring Cloud Eureka是对Netflix的Eureka的进一步封装。Hystrix：熔断器，容错管理工具，旨在通过熔断机制控制服务和第三方库的节点,从而对延迟和故障提供更强大的容错能力Zuul：Zuul 是在云平台上提供动态路由,监控,弹性,安全等边缘服务的框架Ribbon/Feign：客户端负载均衡，Feign是一种声明式、模板化的HTTP客户端Turbine：集群监控Springcloud-config：配置管理工具包，让你可以把配置放到远程服务器，集中化管理集群配置，目前支持本地存储、Git以及SubversionSpringcloud-bus：轻量级消息代理Springcloud-sleuth/zipkin：链路追踪Springcloud-security：基于spring security的安全工具包，为你的应用程序添加安全控制 问题小结Feign客户端customer在启动类中添加客户端EnableFeignClients12345//Eureka客户端@EnableEurekaClient@SpringBootApplication//Feign客户端@EnableFeignClients 当无法注入bean时，可以通过@Configuration手动注入@bean123456789101112@Configurationpublic class FeignConfig &#123; @Bean public UserFeignClient userFeignClient()&#123; return new UserFeignClient() &#123; @Override public String login() &#123; return null; &#125; &#125;; &#125;&#125; Feign,http客户端，替服务发送http请求，解决接口调用问题。使用lombok插件时，Intellij idea开发的话需要安装Lombok plugin，同时设置 Setting -&gt; Compiler -&gt; Annotation Processors -&gt; Enable annotation processing勾选。 容错处理Hystrix（容错机制应该深入理解下），解决调用过程中的异常处理基于feign整合hystrix采用熔断器机制，就像try….catch….一样。通过在调用者实现feign接口，实现具体的失败业务逻辑。 12345678// 注入spring容器中@Component public class UserFeignclientFallback implements UserFeignClient &#123; @Override public Boolean login(User user) &#123; return false; &#125;&#125; 在接口定义中，添加fallback = UserFeignclientFallback.class(发生宕机后处理错误的类)字段，当服务宕机，则执行这个方法。123456@FeignClient(name = "provider-demo",fallback = UserFeignclientFallback.class)public interface UserFeignClient &#123; @RequestMapping(value = "/login",method = RequestMethod.POST) public Boolean login(@RequestBody User user);&#125; Ribbon解决负载均衡的一个组件Nginx解决服务端（被调用方）负载均衡————–Ribbon是解决客户端（调用方）调用默认分配策略是平均分配。 案列测试 123&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt;&lt;/dependency&gt; 先集群provider服务（通过springboot复制一个新服务，设置端口号-Dserver.port=8082） 通过发起多次请求可以发现，ribbon默认算法是均衡的（你一次我一次。） 更换ribbon的负载均衡策略。在调用者方更改。即customer12345# 更改ribbon的负载算法 provider-demo: ribbon: #随机策略算法 NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule Zuul微服务网关 例如filter，对进入服务的请求进行过滤 springcloud实现的过滤器。 ########## 具体实现 通过继承ZuulFilter ，实现默认方法 注入spring容器 12345yml文件中zuul:routes: # 对路径是带有user的，则认为是要进入customer，执行网关拦截。/user --》网关入口 customer-demo: /user/** 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061 @Componentpublic class PreFilter extends ZuulFilter &#123; /*** * 过滤器类型,过滤器运行时间 * FilterConstants.PRE_TYPE == pre .相当于pre （前置，中，后置等） * @return */ @Override public String filterType() &#123; return FilterConstants.PRE_TYPE; &#125; /** * 同级别过滤器优先级，数越大越低 * @return */ @Override public int filterOrder() &#123; return 0; &#125; /*** * 过滤器是否发挥作用,对于过滤器的控制更加灵活 * @return */ @Override public boolean shouldFilter() &#123; return true; &#125; /*** * 过滤器做的事情 * @return * @throws ZuulException */ @Override public Object run() throws ZuulException &#123; //通过requestContext实现接口之间的信息传递 RequestContext context = RequestContext.getCurrentContext(); //通过context拿request请求 HttpServletRequest httpServletRequest = context.getRequest(); /* //在请求头中拿用户验证的token 测试验证 String token = httpServletRequest.getHeader("token"); if (token == null || token.equals(""))&#123; context.setSendZuulResponse(false); context.setResponseStatusCode(401); context.setResponseBody("&#123;\"msg\":\"401,access without permission,login first.\"&#125;"); return "access denied"; &#125; return "pass";*/ String key = httpServletRequest.getParameter("key"); System.out.println(key+"filter 1"); if ("1".equals(key))&#123; context.setSendZuulResponse(false); &#125; return null; &#125;&#125; 过滤器之间的协调作用通过request，context传递信息12345678//通过requestContext实现接口之间的信息传递RequestContext context = RequestContext.getCurrentContext();//通过context拿request请求HttpServletRequest httpServletRequest = context.getRequest(); context.sendZuulResponse(); 获取到上一个过滤器的状态 config 分布式配置 访问路径 /项目名/版本/分支 （get请求）http://localhost:7900/gateway-zull/dev/springcloud在远程仓库上，关于配置文件的命名规范 前缀：/项目名-版本号.properties dev 开发环境 pro 生产环境 test 测试环境远程配置管理，将公共配置集中起来，统一管理通过git配置，将一些配置文件的信息，配置到项目中来 1.添加config依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt; &lt;/dependency&gt; 2. 修改配置信息123456789cloud: config: server: git: uri: https://github.com/czetao/config-server.git username: czetao password: ilzzr888 # 如果不是在根目录下，需要标注下一个包 search-paths: config-file 3. 在启动类上，添加configserver注解。12@EnableDiscoveryClient@EnableConfigServer]]></content>
      <categories>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vue项目小记]]></title>
    <url>%2F2019%2F07%2F29%2Fvue%E9%A1%B9%E7%9B%AE%E5%B0%8F%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[win + Q 打开搜索框npm 与 cnpm 的区别DOM （Document Object Model）是指文档对象模型，通过它，可以访问html文档的所有元素。 下载cnpm(淘宝镜像，更快)npm install cnpm -g –registry=https://registry.npm.taobao.org 全局安装vue -clicnpm install -g vue-cli npm i install下载安装模块vue项目管理系统环境搭建使用vue脚本架搭建工程vue init webpack vuetest(项目名)package.json 总项目的js控制文件启动项目 npm run dev 集成elementUI组件npm i element-ui -S12345//引入element-ui 样式 main.jsimport ElementUI from &apos;element-ui&apos;;import &apos;element-ui/lib/theme-chalk/index.css&apos;;//注册elementuiVue.use(ElementUI); 使用axiosnpm install axios –save 123456789101112//在main.js中引入axiosimport axios from &apos;axios&apos;//挂载在Vue的原型上。Vue.prototype.axios = axios//通过代理方式，将请求地址代理到888端口下，即解决跨域 this.axios.post(&apos;/api/checklogin&apos;,&#123; username : that.loginForm.username, password : that.loginForm.password &#125;) .then(response =&gt; &#123; console.log(&quot;接收后端响应请求的数据：&quot; ,response.data) &#125;) 通过axios与后端交互。 使用express+node.js快速搭建后台。通过后台的/routes/index.js文件，接收请求123router.post(&quot;/checklogin&quot;,(req,res) =&gt; res.send(&quot;1&quot;)) //全局安装expressnpm install express-generator -gexpress -e server 使用nodemon工具启动项目 记得给安装新项目中的模块。cnpm installnpm install -g nodemon12//启动后台服务nodemon app 出现跨域错误error : changeOrigin通过自己添加一个代理 解决1234567891011proxyTable: &#123; &apos;/api&apos;: &#123; target: &apos;http://localhost:888/&apos;, //目标接口域名 changeOrigin: true, //是否跨域 pathRewrite: &#123; &apos;^/api&apos;: &apos;/api&apos; //重写接口 &#125; &#125; &#125;, 这段代码的效果就是将本地8080端口的一个请求代理到了http://www.abc.com这一域名下： &apos;http://localhost:8080/api&apos; ===&gt; &apos;http://localhost:888/api&apos; sql 语法当使用${}拼接时，sql应该用套起1const sql = `select * from users where username=&apos;$&#123;username&#125;&apos; and password=&apos;$&#123;password&#125;&apos;` 使用vuex做到数据之间的共享cnpm i vuex –save通过vuex中的state做到一个全局变量的储存，形同与小程序的全局变量localstore。通过定义mutations操作state123456const mutations = &#123; SAVE_USERINFO(state,userinfo)&#123; console.log(&quot;函数被触发&quot;); state.userinfo = userinfo; &#125;&#125; 通过commit 触发函数1$store.commit(&apos;SAVE_USERINFO&apos;,data); 在样式中加个!imporant路由出口 在属性列表 增添router属性 使用vuex中mapState 获取state数据vuex中state 是保存全局状态的常用方法。改变state通过改变提交方式123456789101112//状态 const state = &#123; userinfo : JSON.parse(localStorage.getItem(&apos;userinfo&apos;))&#125;//mutations 主要用来操作stateconst mutations = &#123; SAVE_USERINFO(state,userinfo)&#123; //存入本地 需要将对象转成字符串 localStorage.setItem(&apos;userinfo&apos;,JSON.stringify(userinfo)) state.userinfo = userinfo; &#125;&#125; 通过vuex actions 异步获取所有数据mapState 辅助组件给state状态中的属性映射。通过提交computed方法123456789101112import &#123;mapState&#125; from &apos;vuex&apos;;computed :&#123; // username ()&#123; // return this.$store.state.userinfo.username // &#125;, //通过mapState 辅助函数获取state数据 ...mapState(&#123; userinfo :state =&gt; state.userinfo, username :state =&gt; state.userinfo.username &#125;) &#125;]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>vue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[牛客java小结]]></title>
    <url>%2F2019%2F07%2F26%2F%E7%89%9B%E5%AE%A2java%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[零散知识存在继承的情况下，初始化顺序为： 父类（静态变量，静态代码块） 子类（静态变量，静态代码块） 父类（实例变量，普通代码块） 父类（构造函数） 子类（实例变量，普通代码块） 子类（构造函数） 继承：在继承关系之中，如果要实例化子类对象，会默认先调用父类构造，为父类之中的属性初始化，之后再调用子类构造，为子类之中的属性初始化，即：默认情况下，子类会找到父类之中的无参构造方法。 牛客java 捕捉到异常时 ， 程序就会停止运行 ，抛出异常 重载 ：仅返回值类型不同时， 不足以构成重载 。重载的基本条件 参数类型不同 参数次序不通过 参数个数不同 重写 ： 函数名， 函数参数，返回值 应该相同 true false null sizeof 不是java的关键字 ， 但是也不能当成java标识符用const goto 是java的保留字（关键字） boolean 的默认值是false this() 和 super() 为构造方法，作用是在jvm 堆中构建出一个对象 。因此避免多次创建对象，同一个方法中只能调用一次this（ ） 和super() . 且必须在第一行实现，避免操作对象时，对象还未构建成功。 实例一个内部类 异常通常分为编译时异常， 运行异常 。编译时异常需要手动进行捕捉处理（文件不存在），运行时异常只有在编译器编译运行才会出现，不需要自己手动捕捉（空指针异常，溢出） 对于外部类来说 ， 只有两种修饰，public 和默认（default ） A instanceOf B ,是判断对象A 是否属于B 或B的子类，子类接口实现类，实现类的实例。 final 类型的变量一定要初始化 ， 因为final 的变量不可更改。 java 类是单继承 ，java 接口可以多继承。 static 方法只能使用 static 变量 ， 想使用非静态变量， 只能通过实例化对象 ，再通过对象引用 堆区 ： 只存放类对象 ， 线程共享 类中的成员变量， 存放在堆区 栈区 ： 存放局部变量，线程不共享 方法区 ： 静态存储区， 存放class文件 和静态数据。线程共享 静态语句块中变量为局部变量，不影响静态变量的值。 权限登记 ：public &gt; protected&gt; default&gt; private 在JDK1.7中，如果通过无参构造的话，初始数组容量为0（应该也是10），当真正对数组进行添加时，才真正分配容量。每次按照1.5倍（位运算）的比率通过copeOf的方式扩容。在JKD1.6中，如果通过无参构造的话，初始数组容量为10.每次通过copeOf的方式扩容后容量为原来的1.5倍加1.以上就是动态扩容的原理。1234567891011121314/**jdk8*/private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); &#125; jdk1.8后 关于arrayList 初始化和扩容详解 public:具有最大访问权限。 可以被同一项目下的任何类所调用，一般用于对外的情况。protected:与public不同的是不同包下的类是不能使用的，但是其子孙类除外。所以我认为这是特意为子类设计的。default:它是针对本包设计的，它所修饰的在本包下的其他类都访问。private:只为类本身提供。是一种封装的体现 A. 非抽象类继承抽象类，必须将抽象类中的方法重写，否则需将方法再次申明为抽象。所以这个方法还可再次声明为抽象，而不用重写。而用重载也错了，重载是在同一个类中，重写、覆盖才是在父子类中。B.抽象类可以没有抽象方法，接口是完全的抽象，只能出现抽象方法。C.抽象类无法实例化，无法创建对象。现实生活中也有抽象类的类子，比如说人类是一个抽象类，无法创建一个叫人类的对象，人继承人类来创建对象。况且抽象类中的抽象方法只有声明，没有主体，如果实例化了，又如何去实现调用呢？D因为类是单继承的，类继承了一个抽象类以后，就不能再继承其他类了。 静态方法是属于类的，当实例化该类时，静态会被优先加载并且只加载一次，不受实例化new 的影响，只要是使用了类，都会加载静态类。静态块只会执行一次。 鲁棒性(Robust,即健壮性)Java在编译和运行程序时，都要对可能出现的问题进行检查，以消除错误的产生。它提供自动垃圾收集来进行内存管理，防止程序员在管理内存时容易产生 的错误。通过集成的面向对象的例外处理机制，在编译时，Java揭示出可能出现但未被处理的例外，帮助程序员正确地进行选择以防止系统的崩溃。另外， Java在编译时还可捕获类型声明中的许多常见错误，防止动态运行时不匹配问题的出现。 Java中定义String数组，有两种定义方式：String a[]和String[] a byte能表示的范围[-128,127] 类的继承，自动向上转型 在定义方法参数时，通常总是应该优先使用父类或接口 对应成list ， 不需要定义多个对象 类里面只有属性和方法。 %取余操作，只适用于整型 如某个JAVA进程的JVM参数配置如下：-Xms1G -Xmx2G -Xmn500M -XX:MaxPermSize=64M -XX:+UseConcMarkSweepGC -XX:SurvivorRatio=3,请问eden区最终分配的大小是多少？java -Xmx2G -Xms1G -Xmn500M -Xss128k-Xmx2G：设置JVM最大可用内存为2G。-Xms1G：设置JVM促使内存为1G。此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存。-Xmn500M：设置年轻代大小为2G。整个JVM内存大小=年轻代大小 + 年老代大小 + 持久代大小。-XX:SurvivorRatio=3:新生代中又会划分为 Eden 区，from Survivor、to Survivor 区。其中 Eden 和 Survivor 区的比例默认是 8:1:1，当然也支持参数调整 -XX:SurvivorRatio=3的话就是3:1:1。故该题为500*（1/3）=300M. interface中合法方法定义?()public void main(String [] args);没有关键字static，可以当成普通方法。 Iterator和 ListIterator主要区别 ListItertor有add()方法，可以向list中添加对象，而 iterator不能。 ListIterator 和 Iterator都有 hashNext()和 next()方法，可以实现顺序向后遍历。但是ListIterator 有 hasPrevious()和previous()方法，可以实现逆向遍历。Iterator就不可以 ListIterator 可以定位当前的索引位置，nextIndex() 和 previousIndex()可以实现。Iterator没有此功能。 都可实现删除对象，但是ListIterator可以实现对象的修改，set()方法可以实现。Iterator仅能遍历，不能修改。因为ListIterator的这些功能，可以实现对LinkedList等list数据结构的操作。 为了更好地组织类，Java 提供了包机制，用于区别类名的命名空间。包的作用 把功能相似或相关的类或接口组织在同一个包中，方便类的查找和使用。 如同文件夹一样，包也采用了树形目录的存储方式。同一个包中的类名字是不同的，不同的包中的类的名字是可以相同的，当同时调用两个不同包中相同类名的类时，应该加上包名加以区别。因此，包可以避免名字冲突。 包也限定了访问权限，拥有包访问权限的类才能访问某个包中的类。Java 使用包（package）这种机制是为了防止命名冲突，访问控制，提供搜索和定位类（class）、接口、枚举（enumerations）和注释（annotation）等。 Garbage Collection：当对象的所有引用都消失后，对象使用的内存将自动回收 cho $$ 当前登录shell 的PIDecho $? 最后运行的命令的结束代码（返回值）即执行上一个指令的返回值 (显示最后命令的退出状态。0表示没有错误，其他任何值表明有错误) 接口中的属性在不提供修饰符修饰的情况下，会自动加上public static final注意（在1.8的编译器下可试）：（1）属性不能用private，protected,default 修饰，因为默认是public（2）如果属性是基本数据类型，需要赋初始值，若是引用类型，也需要初始化，因为默认有final修饰，必须赋初始值；（3）接口中常规的来说不能够定义方法体，所以无法通过get和set方法获取属性值，所以属性不属于对象，属于类（接口），因为默认使用static修饰。 start方法会开启一个新的线程执行run方法，所以start方法执行完，不代表run方法执行完，线程也不一定销毁！ 可以有方法名与类名相同的普通方法 “is”+100+5 在字符串后面的会自动转成字符串在字符串前面的不会转成字符串真题总结 内部类的访问规则 可以直接访问外部类的成员，包括私有 外部类要想访问内部类成员，必须创建对象 父子类中方法执行顺序 静态优先，普通代码块，构造随后 无论静态还是构造，先父再子 序列化：将数据转为n个byte序列的过程，也就是将数据结构转换称为二进制数据流或者文本流的过程。把对象转换为字节序列的过程，序列化后的数据方便在网络上传输和在硬盘上存储。反序列化：与序列化相反，是将二进制数据流或者文本流转换称为易于处理和阅读的数据结构的过程。这是java进程之间通信的方式。 按照流是否直接与特定的地方（如磁盘、内存、设备等）相连，分为节点流和处理流两类。 节点流：可以从或向一个特定的地方（节点）读写数据。如FileReader. 处理流：是对一个已存在的流的连接和封装，通过所封装的流的功能调用实现数据读写。如BufferedReader.处理流的构造方法总是要带一个其他的流对象做参数。一个流对象经过其他流的多次包装，称为流的链接。 JAVA常用的节点流： 文 件 FileInputStream FileOutputStrean FileReader FileWriter 文件进行处理的节点流。 字符串 StringReader StringWriter 对字符串进行处理的节点流。 数 组 ByteArrayInputStream ByteArrayOutputStreamCharArrayReader CharArrayWriter 对数组进行处理的节点流（对应的不再是文件，而是内存中的一个数组）。 管 道 PipedInputStream PipedOutputStream PipedReaderPipedWriter对管道进行处理的节点流。常用处理流（关闭处理流使用关闭里面的节点流）缓冲流：BufferedInputStrean BufferedOutputStream BufferedReader BufferedWriter 增加缓冲功能，避免频繁读写硬盘。转换流：InputStreamReader OutputStreamReader 实现字节流和字符流之间的转换。数据流 DataInputStream DataOutputStream 等-提供将基础数据类型写入到文件中，或者读取出来流的关闭顺序一般情况下是：先打开的后关闭，后打开的先关闭另一种情况：看依赖关系，如果流a依赖流b，应该先关闭流a，再关闭流b。例如，处理流a依赖节点流b，应该先关闭处理流a，再关闭节点流b可以只关闭处理流，不用关闭节点流。处理流关闭的时候，会调用其处理的节点流的关闭方法。 接口是更抽象的东西，属性默认是：public static final的，方法默认是public abstract的！ XML中DTD,XSD的区别DTD和XSD相比：DTD 是使用非 XML 语法编写的。DTD 不可扩展,不支持命名空间,只提供非常有限的数据类型 。DTD即文档类型定义，是一种XML约束模式语言，是XML文件的验证机制,属于XML文件组成的一部分。XML Schema语言也就是XSD。XML Schema描述了XML文档的结构。 JAVA中使用DOM方法解析XML文件XML现在已经成为一种通用的数据交换格式，平台的无关性使得很多场合都需要用到XML，DOM解析是将XML文件全部载入到内存，组装成一颗DOM树，然后通过节点以及节点之间的关系来解析XML文件。 多线程相关 调用run()，与调用start()的区别直接调用run()方法，并没有创建线程，跟调用普通方法是一样的。创建一个线程，需要覆盖Thread类的run（）方法，然后调用Thread类的start（）方法启动。Java集合体系Connection接口:— List 有序,可重复ArrayList优点: 底层数据结构是数组，查询快，增删慢。缺点: 线程不安全，效率高Vector优点: 底层数据结构是数组，查询快，增删慢。缺点: 线程安全，效率低LinkedList优点: 底层数据结构是链表，查询慢，增删快。缺点: 线程不安全，效率高 —Set 无序,唯一HashSet底层数据结构是哈希表。(无序,唯一)如何来保证元素唯一性?1.依赖两个方法：hashCode()和equals()LinkedHashSet底层数据结构是链表和哈希表。(FIFO插入有序,唯一)1.由链表保证元素有序2.由哈希表保证元素唯一TreeSet底层数据结构是红黑树。(唯一，有序) 如何保证元素排序的呢?自然排序比较器排序2.如何保证元素唯一性的呢?Map接口数据类型 基本数据类型包装类：包装类是对象，拥有方法和字段。 包装类型都是有重写equals方法初始值不同，int是0，boolean是false,包装类初始值是null,在定义javabean时，通常使用包装类，为防止数据库中有null数据。更例如定义List中，数据具体类型也是用包装类。 自动拆箱 包装类 ==》基本数据类型 int a = new Integer(100); [-128,127]在这个范围内，会直接从缓存(堆中的常量池)中取。 自动装箱 Double和BigDecimal123double d1 = 2.0;double d2 = 1.1;d1-d2 = 0.89999999; 实际运用中，在对金额的运算，通常使用的是BigDecimal。bigdecimal的equals方法，还会对数据中的小数点进行比较。通常使用compareTO方法比较大小。 {-1：小于， 0 ：等于， 1：大于}为了防止精度丢失，构造方法BigDecimal(“String”)来定义bigdecimal对象，禁止直接使用double。 类设计（jdk8新特性，接口中可以有方法） 接口默认是public,所有方法在接口中不能有实现（java8开始接口可以有默认实现 允许在接口中定义static方法和default方法（带方法体）） 接口本身可以通过extends关键字扩展多个接口 StringBuffer 和StringBuilder的区别 可变性 源码没有用final修饰，是可变的，String是final char[] ,不可变 线程安全性 StringBuilder非线程安全 性能 hashset 去重。HashSet ，通过重写teacher实体类中的eqauls和hashcode方法，定义去重原理。对list中存放对象，需重写equals方法和 list == null || list.size()==0 Date日期转换成指定格式的字符串日期。SimpleDataFormat是线程不安全的类，建议使用DateFormateUtils工具包// 日期String转DateDateUtils.parseDate(“2019-08-04”,”YYYY-MM-DD”) 代码规范 可以抽离代码 鼠标右击 》Refactor 》Extract]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实习第四周]]></title>
    <url>%2F2019%2F07%2F25%2F%E5%AE%9E%E4%B9%A0%E7%AC%AC%E5%9B%9B%E5%91%A8%2F</url>
    <content type="text"><![CDATA[遗漏点 jpa技术 h2 database spring mvc @PathVariable method = RequestMethod.GET @ResponseStatus(HttpStatus.OK)返回状态码 @JsonIgnore作用：在json序列化时将java bean中的一些属性忽略掉，序列化和反序列化都受影响。使用方法：一般标记在属性或者方法上，返回的json数据即不包含该属性。 date parse()返回的是一个Date类型数据，format返回的是一个StringBuffer类型的数据 普通web项目右击项目，Project Structure -&gt; Artifacts -&gt; Output Layout，新建lib文件夹，将基于maven导入的jar包全部加进去，重新运行，顺利解决。 github记得.gitnore eclipse加上get/set shift+alt+s]]></content>
      <categories>
        <category>实习</category>
      </categories>
      <tags>
        <tag>实习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实习第三周]]></title>
    <url>%2F2019%2F07%2F25%2F%E5%AE%9E%E4%B9%A0%E7%AC%AC%E4%B8%89%E5%91%A8%2F</url>
    <content type="text"><![CDATA[新增功能节点之后，给用户分配职责。 使用用户账号登录后，查看功能节点。 用户分配角色，分配职责。 参照设置 nc65sqlinsert into bd_refinfo (CODE, DR, ISNEEDPARA, ISSPECIALREF, LAYER, METADATANAMESPACE,METADATATYPENAME, MODULENAME, NAME, PARA1, PARA2, PARA3, PK_COUNTRY, PK_INDUSTRY, PK_REFINFO, REFCLASS, REFPATH, REFSYSTEM, REFTYPE, RESERV1, RESERV2, RESERV3, RESID, RESIDPATH, TS, WHEREPART)values (‘TR1010020’, 0, null, null, null, ‘uap’, ‘CustVo’, ‘uap’, ‘客户分类’, null, null, null, null, null,‘0001Z0100000002TRAIN’, ‘’, ‘nc.ui.train.pub.ref.CustClassRefModel’, null, 2, null, null, null, ‘客户分类’, ‘ref’,‘2019-05-29 13:44:08’, null);参照注意点：METADATATYPENAME：设置为空，对应的是实体的属性。 ORA-12541 数据库服务未监听服务管理员启动监听服务 –》lsnrctl start]]></content>
      <categories>
        <category>实习</category>
      </categories>
      <tags>
        <tag>实习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实习第二周]]></title>
    <url>%2F2019%2F07%2F25%2F%E5%AE%9E%E4%B9%A0%E7%AC%AC%E4%BA%8C%E5%91%A8%2F</url>
    <content type="text"><![CDATA[oracle数据库配置：在本地oracle环境中，tnsnames.ora文件中配置数据库信息。12345678910APPORCL(连接数据库的别名) = (DESCRIPTION = (ADDRESS_LIST = (ADDRESS = (PROTOCOL = TCP)(HOST = 115.28.**.**)(PORT = 1521)) ) (CONNECT_DATA = (SERVICE_NAME （数据库名）= corcl) )) nc63train/a 创建视图12345678910CREATE [OR REPLACE] [&#123;FORCE|NOFORCE&#125;] VIEW view_nameASSELECT查询[WITH READ ONLY CONSTRAINT]例子：create or REPLACE view p_rateASselect r.userid,r.rateset,p.usercode,p.username,p.alipayid from rate r,pro_user p where r.userid=p.userid; OR REPLACE：如果视图已经存在，则替换旧视图。 FORCE：即使基表不存在，也可以创建该视图，但是该视图不能正常使用，当基表创建成功后，视图才能正常使用。 NOFORCE：如果基表不存在，无法创建视图，该项是默认选项。 WITH READ ONLY：默认可以通过视图对基表执行增删改操作，但是有很多在基表上的限制(比如：基表中某列不能为空，但是该列没有出现在视图中，则不能通过视图执行insert操作)，WITH READ ONLY说明视图是只读视图，不能通过该视图进行增删改操作。现实开发中，基本上不通过视图对表中的数据进行增删改操作。删除视图可以使用“DROP VIEW 视图名称”，删除视图不会影响基表的数据。 全局数据库名称orcl管理口令 ilzzr888]]></content>
      <categories>
        <category>实习</category>
      </categories>
      <tags>
        <tag>实习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实习第一周]]></title>
    <url>%2F2019%2F07%2F25%2F%E5%AE%9E%E4%B9%A0%E7%AC%AC%E4%B8%80%E5%91%A8%2F</url>
    <content type="text"><![CDATA[接触产品 erp nc65 中间件 UAP uap用友UAP平台是一体化平台，其中包括了开发平台、集成平台、动态建模平台、商业分析平台（用友BQ）、数据处理平台（用友AE）、云管理平台和运行平台等7个领域产品，这些平台产品涵盖了软件应用的全生命周期和IT服务管理过程，用于全面支撑平台化企业，可以为大中型企业与公共组织构建信息化平台提供核心工具与服务。账号密码 管理系统(用超级管理员打开)超级管理员root ilzzr888!! 系统管理员2 ilzzr888qwe（普通开启方式）集团管理员(失效日期设置，不对，还有奇怪)统一新增密码 ilzzr888用01 ilzzr888qwe职责里面没有分配功能一块 邮箱密码ilzzr888.. 用友云社区ilzzr888开发环境基础设置 窗口–&gt;首选项–&gt; java–&gt;jre 选用UAP自带的Runtime jre UAP-STUDIO–&gt;开发配置 UAP HOME设置，位置是uaphome 数据源等在配置基础环境sysconfig.bat脚本中就配置好，会自动导入 开发设置–&gt;客户端链接 端口号要与脚本设置一样。 oracle数据库nc65erp本地搭建UAP中间件搭建元数据建立与使用 新建实体组件 无业务组件[train.bill] 实体属性：创建实体时，访问器类型：当组件代码风格选择传统样式时，针对主子表或者多子表中主表对应的实体，访问器要设置为AggVO，即聚合VO访问器，其他的一律选择NCVO。 代码风格：自定义样式 向导生成单据属性 类型样式：SINGLE 单一样式，最终的类型就是原始数据类型。（与数据库对应，或者封装的类型。） REF:引用样式，用于实体，值对象，随后的类型即为参照。 主键一定设置为UFID类型。 字段名称即为生成数据库表列的名称。 对于设置为AggVO样式的实体，访问策略有设置为BodyOfAggVOAccessor. 参照设置：一个实体可以设置多个参照，但必须设置一个缺省参照。获取属性的参照时，如果没有设置则取属性对应实体的缺省参照。 树管理开发界面 与树卡档案不同的是，右边的信息只有一个字段与之关联，新增删除是对节点内容的修改。数据挂在节点下面。 树卡档案，树与卡片相联系，一个树节点就是一个卡片。增改卡片就是对树节点的修改。 整体开发流程： 元数据建模，节点注册配置，单击模板制作，输出模板制作，节点代码编写。 实现接口，记得配置接口属性映射（如何设置接口的属性。） 元数据发布：发布元数据，生成java源代码，生成建库sql脚本并执行。NC65学习记录 uap-studio新增模块后，需将模块信息注册到数据源。 生成元数据时，映射以及命名要规范。 orcl数据库安装管理口令： 创建模块之后，记得给登录系统管理员，给模块分配角色。]]></content>
      <categories>
        <category>实习</category>
      </categories>
      <tags>
        <tag>实习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sql语句]]></title>
    <url>%2F2019%2F07%2F23%2Fsql%E8%AF%AD%E5%8F%A5%2F</url>
    <content type="text"><![CDATA[数据库分页limit 2，5从第三条开始，每次5条。 UNION去重且排序UNION ALL不去重不排序NION用的比较多union all是直接连接，取到得是所有值，记录可能有重复 union 是取唯一值，记录没有重复 UNION 的语法如下：[SQL 语句 1]UNION[SQL 语句 2] UNION ALL 的语法如下：[SQL 语句 1]UNION ALL[SQL 语句 2]UNION和UNION ALL关键字都是将两个结果集合并为一个]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git]]></title>
    <url>%2F2019%2F07%2F16%2Fgit%2F</url>
    <content type="text"><![CDATA[git remote add &lt;主机名&gt; &lt;远程地址&gt; git remote -v 查看别称 git add . 添加所有 git reset 撤回所有add ,也可以针对特定文件。 git commit -m “提交说明” [git commit -a -m “ “] 组合命令，[-a] = git add . git push -u &lt;主机名&gt; master推送出错时，error: failed to push some refs to &#39;git@github.com:…..” 是因为没有将远程仓库同步，将远程仓库与本地代码合并。git pull –rebase origin mastergit pull origin master –allow-unrelated-histories git clone &lt;远程地址&gt; . 注意最后加 . 克隆当前目录 git diff #查看difference git reset]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git小记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ajax跨域]]></title>
    <url>%2F2019%2F07%2F05%2Fajax%E8%B7%A8%E5%9F%9F%2F</url>
    <content type="text"><![CDATA[为什么会发生跨域 浏览器限制（浏览器会对请求做校验，校验失败则是跨域。 ） 跨域 只要端口号，域名，协议..不同，浏览器都会将其标记为跨域 XHR(XMLHttpRequest)请求。如果不是XHR请求，浏览器也不会对其限制。解决思路 JSONP对json的补充，不是一个官方协议，但也是一个约定（前后台约定callback）。动态创建一个script标签，返回js代码（需要对后台代码做修改。）原理即是对发送的请求加一个callback参数，后台发现有callback参数，即知道是一个jsonp请求，就会将返回的数据将json改成JavaScript。JavaScript里的内容就是一个函数调用。dataType：”jsonp”,jsonp:”callback” 弊端： 服务器需要改动代码支持。 只支持get方法。 发送的不是XHR请求。（异步，各种事件的特性都无法使用） 支持跨域被调用方的角度。跨域请求是直接从浏览器发送过去的，在响应头中添加字段，告诉浏览器允许跨域。 服务器端实现 通过拦截器在响应头中添加字段 response.addHeader(“Access-Control-Allow-Origin”,”http://localhost:8081/*(允许所有方法)&quot;); 将这个字段设置为*，即为允许跨域，并不完善。 response.addHeader(“Access-Control-Allow-Methods”,”GET”); NGINX配置 APACHE配置 带cookie的跨域12345678910$.ajax(&#123; url : &apos;http://remote.domain.com/corsrequest&apos;, data : data, dataType: &apos;json&apos;, type : &apos;POST&apos;, xhrFields: &#123; withCredentials: true &#125;, crossDomain: true, contentType: &quot;application/json&quot;, 通过设置 withCredentials: true ，发送Ajax时，Request header中便会带上 Cookie 信息。通过设置document里添加cookie 隐藏跨域在调用方的角度, 通过一个代理http服务器，将从A域名发出的请求，将指定的url加入B域名里面，就不会判定为跨域。静态请求：请求与用户数据无关。动态请求：请求与用户数据有关。]]></content>
      <categories>
        <category>跨域</category>
      </categories>
      <tags>
        <tag>小识跨域</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实习记录]]></title>
    <url>%2F2019%2F07%2F02%2F%E5%AE%9E%E4%B9%A0%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[在此记录实习过程中遇到的难点以及解决方案。]]></content>
      <tags>
        <tag>实习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java容器]]></title>
    <url>%2F2019%2F07%2F01%2Fjava%E5%AE%B9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[数组与链表的适用场景数组 优点具有快速查找特性。O(1)。 缺点在内存中地址是连续的，当需要扩容时，将会重新新建数组空间，拷贝旧数组的内容到新数组。每当删除，新增时，需要移动数组内容的位置。时间复杂度为O(N)。空间利用率低。适合多查，少增改链表 优点数据在内存中并不是连续的，当需要增改时，只需要改变指针位置。每一个结点都有一个指针域和数据域。 缺点遍历查找时麻烦，需要从头遍历查找。 容器主要包括 Collection 和 Map 两种，Collection 存储着对象的集合，而 Map 存储着键值对（两个对象）的映射表。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql数据库]]></title>
    <url>%2F2019%2F06%2F18%2Fmysql%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[存储引擎mysql数据库引擎属于插件式存储引擎，基于业务可在不同的表上使用合适的存储引擎。在mysql5.5后，默认是InnoDB存储引擎。InnoDB：InnoDB是造成mysql灵活性的技术的直接产品。 支持事务处理，支持外键，行锁，当需要频繁的更新，删除操作时选择此引擎。MyISAM:不支持事务，支持表锁。读多写少可使用，查询效率更好。 更换存储引擎： Alter table XXX engine = InnoDB由于存储引擎不同，索引的结构也会相对不同。Myisam 的索引结构（非聚集索引）InnoDB表 （聚集索引） :即存数据又存索引 索引—–最常见的慢查询优化方式是一种优化查询的数据结构，mysql是基于B+树实现的，可以优化查询速度。 建索引原则：适合 ： 频繁作为where条件的字段。关联字段可以建索引，例如外键。 Order bt col, Group by col.不适合 ： where条件中用不到的字段，频繁更新的字段 ，数据值分布比较均匀的不适合建索引，例如男女 ， 真假值 ，表的数据量少]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[复习小结]]></title>
    <url>%2F2019%2F06%2F18%2F%E5%A4%8D%E4%B9%A0%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[学习网站https://www.runoob.com/ bootstrap布局http://www.ibootstrap.cn 牛客备战网站https://www.nowcoder.com/studypath/1 整理复习 常忘小结（三门基本课程：数据结构，计算机网络，操作系统） 计算机网络 http协议 TCP三次握手，四次挥手 TCP滑动窗口机制Linux 常用命令 文件权限控制mysql数据库 两个常用存储引擎 数据库优化（sql优化，索引，水平垂直分表）JVM 对象的创建 垃圾回收java小结 java集合类大纲（全盘掌握） 基本数据类型对象的创建（spring,int）常忘 知识点小结框架小结 对于IOC，AOP的理解 spring MVC工作流程java java 容器]]></content>
      <categories>
        <category>复习</category>
      </categories>
      <tags>
        <tag>整理小结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP协议]]></title>
    <url>%2F2019%2F06%2F18%2FHTTP%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[HTTP协议细讲 HTTP有哪些方法？ HTTP1.0定义了三种请求方法： GET, POST 和 HEAD方法 HTTP1.1新增了五种请求方法：OPTIONS, PUT, DELETE, TRACE 和 CONNECT 这些方法的具体作用是什么？ GET: 通常用于请求服务器发送某些资源(幂等、缓存) HEAD: 请求资源的头部信息, 并且这些头部与 HTTP GET 方法请求时返回的一致. 该请求方法的一个使用场景是在下载一个大文件前先获取其大小再决定是否要下载, 以此可以节约带宽资源 OPTIONS: 用于获取目的资源所支持的通信选项 POST: 发送数据给服务器 PUT: 用于新增资源或者使用请求中的有效负载替换目标资源的表现形式(幂等) DELETE: 用于删除指定的资源(幂等) PATCH: 用于对资源进行部分修改 CONNECT: HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器 TRACE: 回显服务器收到的请求，主要用于测试或诊断 GET和POST有什么区别？ 数据传输方式不同：GET请求通过URL传输数据，而POST的数据通过请求体传输。 安全性不同：POST的数据因为在请求主体内，所以有一定的安全性保证，而GET的数据在URL中，通过历史记录，缓存很容易查到数据信息。 数据类型不同：GET只允许 ASCII 字符，而POST无限制 GET无害： 刷新、后退等浏览器操作GET请求是无害的，POST可能重复提交表单 特性不同：GET是安全（这里的安全是指只读特性，就是使用这个方法不会引起服务器状态变化）且幂等（幂等的概念是指同一个请求方法执行多次和仅执行一次的效果完全相同），而POST是非安全非幂等 PUT和POST都是给服务器发送新增资源，有什么区别？PUT 和POST方法的区别是,PUT方法是幂等的：连续调用一次或者多次的效果相同（无副作用），而POST方法是非幂等的。 除此之外还有一个区别，通常情况下，PUT的URI指向是具体单一资源，而POST可以指向资源集合。 举个例子，我们在开发一个博客系统，当我们要创建一篇文章的时候往往用POST https://www.jianshu.com/articles，这个请求的语义是，在articles的资源集合下创建一篇新的文章，如果我们多次提交这个请求会创建多个文章，这是非幂等的。 而PUT https://www.jianshu.com/articles/820357430的语义是更新对应文章下的资源（比如修改作者名称等），这个URI指向的就是单一资源，而且是幂等的，比如你把『刘德华』修改成『蔡徐坤』，提交多少次都是修改成『蔡徐坤』 ps: 『POST表示创建资源，PUT表示更新资源』这种说法是错误的，两个都能创建资源，根本区别就在于幂等性 HTTP协议掌握考点 HTTP协议主要特点 HTTP报文的组成部分（请求报文，响应报文） get和post HTTP状态码主要特点 无连接 ：连接一次就会断开，不会继续保持连接 无状态客户端和服务器端是两种身份。第一次请求结束后，就断开了，第二次请求时，服务器端并没有记住之前的状态，也就是说，服务器端无法区分客户端是否为同一个人、同一个身份。有的时候，我们访问网站时，网站能记住我们的账号，这个是通过其他的手段（比如 session）做到的，并不是http协议能做到的。HTTP报文的组成部分 请求报文get/post请求方法 请求url http协议及版本号请求头 一堆键值对 请求体 name = tom &amp;password = 123(数据部分) 响应报文http协议/版本号 状态码及状态描述响应头响应体 返回信息记住组成部分，会考聊一聊HTTP的状态码有哪些？ 2XX 成功 200 OK，表示从客户端发来的请求在服务器端被正确处理 ✨ 201 Created 请求已经被实现，而且有一个新的资源已经依据请求的需要而建立 202 Accepted 请求已接受，但是还没执行，不保证完成请求 204 No content，表示请求成功，但响应报文不含实体的主体部分 206 Partial Content，进行范围请求 ✨ 3XX 重定向 301 moved permanently，永久性重定向，表示资源已被分配了新的 URL 302 found，临时性重定向，表示资源临时被分配了新的 URL ✨ 303 see other，表示资源存在着另一个 URL，应使用 GET 方法丁香获取资源 304 not modified，表示服务器允许访问资源，但因发生请求未满足条件的情况 307 temporary redirect，临时重定向，和302含义相同 4XX 客户端错误 400 bad request，请求报文存在语法错误 ✨ 401 unauthorized，表示发送的请求需要有通过 HTTP 认证的认证信息 ✨ 403 forbidden，表示对请求资源的访问被服务器拒绝 ✨ 404 not found，表示在服务器上没有找到请求的资源 ✨ 408 Request timeout, 客户端请求超时 409 Confict, 请求的资源可能引起冲突 5XX 服务器错误 500 internal sever error，表示服务器端在执行请求时发生了错误 ✨ 501 Not Implemented 请求超出服务器能力范围，例如服务器不支持当前请求所需要的某个功能，或者请求是服务器不支持的某个方法 503 service unavailable，表明服务器暂时处于超负载或正在停机维护，无法处理请求 505 http version not supported 服务器不支持，或者拒绝支持在请求中使用的 HTTP 版本 HTTP的keep-alive是干什么的？在早期的HTTP/1.0中，每次http请求都要创建一个连接，而创建连接的过程需要消耗资源和时间，为了减少资源消耗，缩短响应时间，就需要重用连接。在后来的HTTP/1.0中以及HTTP/1.1中，引入了重用连接的机制，就是在http请求头中加入Connection: keep-alive来告诉对方这个请求响应完成后不要关闭，下一次咱们还用这个请求继续交流。协议规定HTTP/1.0如果想要保持长连接，需要在请求头中加上Connection: keep-alive。 keep-alive的优点： 较少的CPU和内存的使用（由于同时打开的连接的减少了） 允许请求和应答的HTTP管线化 降低拥塞控制 （TCP连接减少了） 减少了后续请求的延迟（无需再进行握手） 报告错误无需关闭TCP连接 什么是长连接、短连接？在HTTP/1.0中默认使用短连接。也就是说，客户端和服务器每进行一次HTTP操作，就建立一次连接，任务结束就中断连接。当客户端浏览器访问的某个HTML或其他类型的Web页中包含有其他的Web资源（如JavaScript文件、图像文件、CSS文件等），每遇到这样一个Web资源，浏览器就会重新建立一个HTTP会话。 而从HTTP/1.1起，默认使用长连接，用以保持连接特性。使用长连接的HTTP协议，会在响应头加入这行代码： Connection:keep-alive在使用长连接的情况下，当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接不会关闭，客户端再次访问这个服务器时，会继续使用这一条已经建立的连接。Keep-Alive不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如Apache）中设定这个时间。实现长连接需要客户端和服务端都支持长连接。 HTTP协议的长连接和短连接，实质上是TCP协议的长连接和短连接。 HTTPS的工作原理 我们都知道HTTPS能够加密信息，以免敏感信息被第三方获取，所以很多银行网站或电子邮箱等等安全级别较高的服务都会采用HTTPS协议。 客户端在使用HTTPS方式与Web服务器通信时有以下几个步骤，如图所示。 （1）客户使用https的URL访问Web服务器，要求与Web服务器建立SSL连接。 （2）Web服务器收到客户端请求后，会将网站的证书信息（证书中包含公钥）传送一份给客户端。 （3）客户端的浏览器与Web服务器开始协商SSL连接的安全等级，也就是信息加密的等级。 （4）客户端的浏览器根据双方同意的安全等级，建立会话密钥，然后利用网站的公钥将会话密钥加密，并传送给网站。 （5）Web服务器利用自己的私钥解密出会话密钥。 （6）Web服务器利用会话密钥加密与客户端之间的通信。 HTTPS是如何保证安全的？过程比较复杂，我们得先理解两个概念 对称加密：即通信的双方都使用同一个秘钥进行加解密，比如特务接头的暗号，就属于对称加密 对称加密虽然很简单性能也好，但是无法解决首次把秘钥发给对方的问题，很容易被黑客拦截秘钥。 非对称加密： 私钥 + 公钥= 密钥对 即用私钥加密的数据,只有对应的公钥才能解密,用公钥加密的数据,只有对应的私钥才能解密 因为通信双方的手里都有一套自己的密钥对,通信之前双方会先把自己的公钥都先发给对方 然后对方再拿着这个公钥来加密数据响应给对方,等到到了对方那里,对方再用自己的私钥进行解密 非对称加密虽然安全性更高，但是带来的问题就是速度很慢，影响性能。 解决方案： 那么结合两种加密方式，将对称加密的密钥使用非对称加密的公钥进行加密，然后发送出去，接收方使用私钥进行解密得到对称加密的密钥，然后双方可以使用对称加密来进行沟通。 此时又带来一个问题，中间人问题： 如果此时在客户端和服务器之间存在一个中间人,这个中间人只需要把原本双方通信互发的公钥,换成自己的公钥,这样中间人就可以轻松解密通信双方所发送的所有数据。 所以这个时候需要一个安全的第三方颁发证书（CA），证明身份的身份，防止被中间人攻击。 证书中包括：签发者、证书用途、使用者公钥、使用者私钥、使用者的HASH算法、证书到期时间等 但是问题来了，如果中间人篡改了证书，那么身份证明是不是就无效了？这个证明就白买了，这个时候需要一个新的技术，数字签名。 数字签名就是用CA自带的HASH算法对证书的内容进行HASH得到一个摘要，再用CA的私钥加密，最终组成数字签名。 当别人把他的证书发过来的时候,我再用同样的Hash算法,再次生成消息摘要，然后用CA的公钥对数字签名解密,得到CA创建的消息摘要,两者一比,就知道中间有没有被人篡改了。 这个时候就能最大程度保证通信的安全了。 HTTP2相对于HTTP1.x有什么优势和特点？#二进制分帧帧：HTTP/2 数据通信的最小单位消息：指 HTTP/2 中逻辑上的 HTTP 消息。例如请求和响应等，消息由一个或多个帧组成。 流：存在于连接中的一个虚拟通道。流可以承载双向消息，每个流都有一个唯一的整数ID HTTP/2 采用二进制格式传输数据，而非 HTTP 1.x 的文本格式，二进制协议解析起来更高效。 #头部压缩HTTP/1.x会在请求和响应中中重复地携带不常改变的、冗长的头部数据，给网络带来额外的负担。 HTTP/2在客户端和服务器端使用“首部表”来跟踪和存储之前发送的键－值对，对于相同的数据，不再通过每次请求和响应发送 首部表在HTTP/2的连接存续期内始终存在，由客户端和服务器共同渐进地更新; 每个新的首部键－值对要么被追加到当前表的末尾，要么替换表中之前的值。 你可以理解为只发送差异数据，而不是全部发送，从而减少头部的信息量 #服务器推送服务端可以在发送页面HTML时主动推送其它资源，而不用等到浏览器解析到相应位置，发起请求再响应。例如服务端可以主动把JS和CSS文件推送给客户端，而不需要客户端解析HTML时再发送这些请求。 服务端可以主动推送，客户端也有权利选择是否接收。如果服务端推送的资源已经被浏览器缓存过，浏览器可以通过发送RST_STREAM帧来拒收。主动推送也遵守同源策略，服务器不会随便推送第三方资源给客户端。 #多路复用HTTP 1.x 中，如果想并发多个请求，必须使用多个 TCP 链接，且浏览器为了控制资源，还会对单个域名有 6-8个的TCP链接请求限制。 HTTP2中： 同域名下所有通信都在单个连接上完成。 单个连接可以承载任意数量的双向数据流。 数据流以消息的形式发送，而消息又由一个或多个帧组成，多个帧之间可以乱序发送，因为根据帧首部的流标识可以重新组装 拓展阅读：HTTP/2特性及其在实际应用中的表现 #HTTP的缓存的过程是怎样的？通常情况下的步骤是: 客户端向服务器发出请求，请求资源 服务器返回资源，并通过响应头决定缓存策略 客户端根据响应头的策略决定是否缓存资源（这里假设是），并将响应头与资源缓存下来 在客户端再次请求且命中资源的时候，此时客户端去检查上次缓存的缓存策略，根据策略的不同、是否过期等判断是直接读取本地缓存还是与服务器协商缓存]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo]]></title>
    <url>%2F2019%2F06%2F06%2Fhexo%2F</url>
    <content type="text"><![CDATA[hexo g 生成静态文件 generate hexo d 部署网站，需要预先生成静态文件 hexo clean 清楚缓存文件 hexo s 启动博客 hexo new “文章名称” 创建文章名称 hexo new page “tags” 新页面 组合命令 hexo d -g 部署上传 这是next主题优化网站 这是Typora官方网站]]></content>
      <categories>
        <category>hexo记录</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面经]]></title>
    <url>%2F2019%2F06%2F06%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[JavaGuide：JavaGuide，对于复习很有帮助，面试时候都会尽量刷一遍知识点。 云趣科技（凉）第一家面试的公司，准备不充分被刷。常规做笔试题（java基础应该掌握的东西）：数据库sql，多线程创建方式，适配器模式概述，代码重构的方法。面试：文件流操作，jsp隐藏属性，spring事务机制，jdbc如何实现，jdbc开启事务，jsp自定义标签，项目基本思路。总结：一定要尽早迈出第一步，尽早的查漏补缺，认识自己复习的到哪一步，有助于提升动力复习。 粤信科技（拿offer） 6.6面试距离第一次面试的公司，期间1个星期。重新整理复习的重点，掌握jvm，多线程，集合类，计网常用知识点等能够在面试中的加分项。笔试：java基础知识。面试：hr面和技术面自我介绍：也要准备好个稿，参考javaGuide。hr面：聊一下你在学校的生活，参加过什么活动。还有挑一个比较熟悉的项目讲给她听。这个感觉只要态度端正，要给面试官感觉到你尊重她，你在认真的听她讲话。这一部分暂不知道什么需要注意的。但排前一个的同学，就在这一面挂了，hr面完直接让他走了，没有技术面。技术面：全程都是根据你写的项目在问。所以一定要整理好自己项目的知识点。主要问题：挑一个你最熟悉的项目，说一说你是怎么设计数据库表的？你认为你这个项目中有哪个技术点让你印象深刻，你是怎么解决的？收到offer通知后，出错了。没能做保底，在hr跟我说需要实习到大四，嘴贱说初衷是打算实习3,4个月，然后换另一家公司。结果hr立马说那不行，收回入职邀请。她说我的心态已经变了，不会给考虑机会。555 用友科技（拿offer） 6.7没有笔试。面试：我看你简历上写了jvm,你说一下吧说一下java基础中，接口和抽象类的区别然后我自己在说知识点的时候，就提到自己的项目去了，最后两个项目都说了下业务逻辑，和一些基本的技术点。如果需要接触新知识，你是怎么学习的如果我要你在7天内完成任务，你完成不了，会怎么办你还要问我什么问题。（javaguide有提及，可以参考。）等了一周才发offer，公司大，环境好，但是工资较低，继续找 火烈鸟 6.11 凉透，难的发指。信心摧残。基础够格，就能过，无面经。]]></content>
      <categories>
        <category>面试汇总</category>
      </categories>
      <tags>
        <tag>面试总结</tag>
      </tags>
  </entry>
</search>
